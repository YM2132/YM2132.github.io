<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yusuf Mohammad">
<meta name="dcterms.date" content="2025-02-23">

<title>From Scratch - The Convolutional Layer</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="from_scratch_convolutional_layers_files/libs/clipboard/clipboard.min.js"></script>
<script src="from_scratch_convolutional_layers_files/libs/quarto-html/quarto.js"></script>
<script src="from_scratch_convolutional_layers_files/libs/quarto-html/popper.min.js"></script>
<script src="from_scratch_convolutional_layers_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="from_scratch_convolutional_layers_files/libs/quarto-html/anchor.min.js"></script>
<link href="from_scratch_convolutional_layers_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="from_scratch_convolutional_layers_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="from_scratch_convolutional_layers_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="from_scratch_convolutional_layers_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="from_scratch_convolutional_layers_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#moving-averages---john-the-programmer" id="toc-moving-averages---john-the-programmer" class="nav-link active" data-scroll-target="#moving-averages---john-the-programmer">Moving Averages - John the Programmer</a>
  <ul>
  <li><a href="#moving-averages" id="toc-moving-averages" class="nav-link" data-scroll-target="#moving-averages">Moving Averages</a>
  <ul class="collapse">
  <li><a href="#the-math-behind-mas" id="toc-the-math-behind-mas" class="nav-link" data-scroll-target="#the-math-behind-mas">The Math Behind MAs</a></li>
  <li><a href="#equation-time" id="toc-equation-time" class="nav-link" data-scroll-target="#equation-time">Equation Time!</a></li>
  <li><a href="#implementing-the-moving-average" id="toc-implementing-the-moving-average" class="nav-link" data-scroll-target="#implementing-the-moving-average">Implementing the Moving Average</a></li>
  <li><a href="#an-aside---the-low-pass-filter" id="toc-an-aside---the-low-pass-filter" class="nav-link" data-scroll-target="#an-aside---the-low-pass-filter">An aside - The Low-Pass Filter</a></li>
  </ul></li>
  <li><a href="#the-weighted-moving-average" id="toc-the-weighted-moving-average" class="nav-link" data-scroll-target="#the-weighted-moving-average">The Weighted Moving Average</a>
  <ul class="collapse">
  <li><a href="#the-math-behind-wmas" id="toc-the-math-behind-wmas" class="nav-link" data-scroll-target="#the-math-behind-wmas">The Math Behind WMAs</a></li>
  <li><a href="#implementing-the-wma" id="toc-implementing-the-wma" class="nav-link" data-scroll-target="#implementing-the-wma">Implementing the WMA</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#d-filters" id="toc-d-filters" class="nav-link" data-scroll-target="#d-filters">2D Filters</a>
  <ul>
  <li><a href="#box-blur" id="toc-box-blur" class="nav-link" data-scroll-target="#box-blur">Box Blur</a>
  <ul class="collapse">
  <li><a href="#the-math-behind-the-box-blur" id="toc-the-math-behind-the-box-blur" class="nav-link" data-scroll-target="#the-math-behind-the-box-blur">The Math Behind the Box Blur</a></li>
  <li><a href="#implementing-the-box-blur" id="toc-implementing-the-box-blur" class="nav-link" data-scroll-target="#implementing-the-box-blur">Implementing the Box Blur</a></li>
  </ul></li>
  <li><a href="#edge-detection-kernels" id="toc-edge-detection-kernels" class="nav-link" data-scroll-target="#edge-detection-kernels">Edge Detection Kernels</a>
  <ul class="collapse">
  <li><a href="#the-prewitt-operator" id="toc-the-prewitt-operator" class="nav-link" data-scroll-target="#the-prewitt-operator">The Prewitt Operator</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#cross-correlation-and-convolution" id="toc-cross-correlation-and-convolution" class="nav-link" data-scroll-target="#cross-correlation-and-convolution">Cross-correlation and Convolution</a>
  <ul>
  <li><a href="#cross-correlation-and-convolution---an-intuitive-approach" id="toc-cross-correlation-and-convolution---an-intuitive-approach" class="nav-link" data-scroll-target="#cross-correlation-and-convolution---an-intuitive-approach">Cross-Correlation and Convolution - An Intuitive Approach</a></li>
  <li><a href="#the-maths-of-convolution-and-cross-correlation" id="toc-the-maths-of-convolution-and-cross-correlation" class="nav-link" data-scroll-target="#the-maths-of-convolution-and-cross-correlation">The Maths of Convolution and Cross-correlation</a>
  <ul class="collapse">
  <li><a href="#an-aside---calculus" id="toc-an-aside---calculus" class="nav-link" data-scroll-target="#an-aside---calculus">An Aside - Calculus</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#convolutional-neural-networks-cnn" id="toc-convolutional-neural-networks-cnn" class="nav-link" data-scroll-target="#convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)</a>
  <ul>
  <li><a href="#adapting-cross-correlation-to-the-neural-network-setting" id="toc-adapting-cross-correlation-to-the-neural-network-setting" class="nav-link" data-scroll-target="#adapting-cross-correlation-to-the-neural-network-setting">Adapting Cross-correlation to the Neural Network Setting</a>
  <ul class="collapse">
  <li><a href="#sparse-interactions" id="toc-sparse-interactions" class="nav-link" data-scroll-target="#sparse-interactions">Sparse Interactions</a></li>
  <li><a href="#parameter-sharing" id="toc-parameter-sharing" class="nav-link" data-scroll-target="#parameter-sharing">Parameter Sharing</a></li>
  <li><a href="#equivariance-to-translation" id="toc-equivariance-to-translation" class="nav-link" data-scroll-target="#equivariance-to-translation">Equivariance to Translation</a></li>
  </ul></li>
  <li><a href="#implementing-the-conv-operation" id="toc-implementing-the-conv-operation" class="nav-link" data-scroll-target="#implementing-the-conv-operation">Implementing the Conv Operation</a>
  <ul class="collapse">
  <li><a href="#an-aside---generalised-matrix-multiplication-gemm" id="toc-an-aside---generalised-matrix-multiplication-gemm" class="nav-link" data-scroll-target="#an-aside---generalised-matrix-multiplication-gemm">An Aside - GEneralised Matrix Multiplication (GEMM)</a></li>
  <li><a href="#a-small-example---im2row-and-matmuls" id="toc-a-small-example---im2row-and-matmuls" class="nav-link" data-scroll-target="#a-small-example---im2row-and-matmuls">A Small Example - Im2row and Matmuls</a></li>
  </ul></li>
  <li><a href="#the-general-case---extension-to-rgb-3-channel-images" id="toc-the-general-case---extension-to-rgb-3-channel-images" class="nav-link" data-scroll-target="#the-general-case---extension-to-rgb-3-channel-images">The General Case - Extension to RGB (3 channel) Images</a></li>
  <li><a href="#implenting-im2row" id="toc-implenting-im2row" class="nav-link" data-scroll-target="#implenting-im2row">Implenting Im2row</a>
  <ul class="collapse">
  <li><a href="#flattening-the-patches" id="toc-flattening-the-patches" class="nav-link" data-scroll-target="#flattening-the-patches">1 - Flattening the patches</a></li>
  <li><a href="#the-matmul" id="toc-the-matmul" class="nav-link" data-scroll-target="#the-matmul">2 - The matmul</a></li>
  <li><a href="#reshaping-the-matmul-output" id="toc-reshaping-the-matmul-output" class="nav-link" data-scroll-target="#reshaping-the-matmul-output">3 - Reshaping The Matmul Output</a></li>
  </ul></li>
  <li><a href="#batched-im2row-operation" id="toc-batched-im2row-operation" class="nav-link" data-scroll-target="#batched-im2row-operation">Batched Im2row Operation</a></li>
  <li><a href="#a-convolutional-layer" id="toc-a-convolutional-layer" class="nav-link" data-scroll-target="#a-convolutional-layer">A Convolutional Layer</a>
  <ul class="collapse">
  <li><a href="#comparing-the-performance-of-the-customconv2d-with-the-pytorch-conv2d" id="toc-comparing-the-performance-of-the-customconv2d-with-the-pytorch-conv2d" class="nav-link" data-scroll-target="#comparing-the-performance-of-the-customconv2d-with-the-pytorch-conv2d">Comparing the Performance of the CustomConv2d with the PyTorch Conv2d</a></li>
  </ul></li>
  <li><a href="#a-fully-flegded-cnn" id="toc-a-fully-flegded-cnn" class="nav-link" data-scroll-target="#a-fully-flegded-cnn">A Fully Flegded CNN</a>
  <ul class="collapse">
  <li><a href="#pytorch-cnn-training-run" id="toc-pytorch-cnn-training-run" class="nav-link" data-scroll-target="#pytorch-cnn-training-run">PyTorch CNN Training Run</a></li>
  <li><a href="#custom-cnn-training-run" id="toc-custom-cnn-training-run" class="nav-link" data-scroll-target="#custom-cnn-training-run">Custom CNN Training Run</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="quarto-other-links"><h2>Other Links</h2><ul><li><a href="https://ym2132.github.io/"><i class="bi bi-link-45deg"></i>Yusuf's Deep Learning Blog</a></li><li><a href="https://github.com/YM2132?tab=repositories"><i class="bi bi-link-45deg"></i>Yusuf's GitHub</a></li></ul></div><div class="quarto-code-links"><h2>Code Links</h2><ul><li><a href="https://github.com/YM2132/YMPaperImplementations/blob/main/paper_implementations/ConvNeXt_series/from_scratch_convolutional_layers.ipynb"><i class="bi bi-file-code"></i>Convolution - GitHub will not render the whole thing, please download and run locally :)</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">From Scratch - The Convolutional Layer</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Yusuf Mohammad </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 23, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<hr>
<div id="b5b2df78-7b90-4d62-9e38-1775f5a0e67f" class="cell" data-execution_count="56">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Imports</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Set global random seeds for both numpy and Python's random - This is useless in ipynb we need to set randomstate before every</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># random operation</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.float_format'</span>, <span class="kw">lambda</span> x: <span class="st">'</span><span class="sc">%.5f</span><span class="st">'</span> <span class="op">%</span> x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Explore the resources that inspired this post
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><a href="https://betterexplained.com/articles/intuitive-convolution/">https://betterexplained.com/articles/intuitive-convolution/</a> - A great resource, some of the examples I give are adapted from this post.</p>
<p><a href="https://cs231n.github.io/convolutional-networks/#conv">https://cs231n.github.io/convolutional-networks/#conv</a> A great coverage of CNNs, check out the whole course too.</p>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a> The AlexNet paper, one of the most important milestones for CNNs.</p>
<p><a href="https://www.deeplearningbook.org/contents/convnets.html">https://www.deeplearningbook.org/contents/convnets.html</a> The age old classic book for deep learning.</p>
<p><a href="https://sgugger.github.io/convolution-in-depth.html">https://sgugger.github.io/convolution-in-depth.html</a> A brilliant blog showcasing the CNN.</p>
<p><a href="https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/">https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/</a> My introduction to GEMM and cuDNN</p>
<p><a href="https://leonardoaraujosantos.gitbook.io/artificial-inteligence/machine_learning/deep_learning/convolution_layer/making_faster">https://leonardoaraujosantos.gitbook.io/artificial-inteligence/machine_learning/deep_learning/convolution_layer/making_faster</a> Another great post about im2col</p>
<p><a href="https://arxiv.org/pdf/1410.0759">https://arxiv.org/pdf/1410.0759</a> The original cuDNN paper from Nvidia. A great read.</p>
<p><a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#">https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#</a></p>
<p><a href="https://uk.mathworks.com/help/images/ref/im2col.html">https://uk.mathworks.com/help/images/ref/im2col.html</a> Im2col was originally from matlab.</p>
<p><a href="https://fabianfuchsml.github.io/equivariance1of2/">https://fabianfuchsml.github.io/equivariance1of2/</a> An in depth explanation of equivariance to translation.</p>
<p><a href="https://en.wikipedia.org/wiki/Cross-correlation#Cross-correlation_function_2">https://en.wikipedia.org/wiki/Cross-correlation#Cross-correlation_function_2</a></p>
<p><a href="https://www.iro.umontreal.ca/~mignotte/IFT3205/Documents/UnderstandingDigitalSignalProcessing.pdf">https://www.iro.umontreal.ca/~mignotte/IFT3205/Documents/UnderstandingDigitalSignalProcessing.pdf</a></p>
<p><a href="https://setosa.io/ev/image-kernels/">https://setosa.io/ev/image-kernels/</a> A great intuitive animation on kernels.</p>
<p><a href="https://poloclub.github.io/cnn-explainer/">https://poloclub.github.io/cnn-explainer/</a> A beautiful visualisation of CNNs.</p>
</div>
</div>
</div>
<hr>
<p>Lately, I’ve used convolutional layers a lot and it occurred to me that I knew very little about the underlying convolution operation, except that it makes use of sliding windows and does some sort of filtering. In this post I chronicle my journey to understand the convolution from first principles. I hope you enjoy reading the post as much as I did writing it :)</p>
<p>The journey begins and ends with Convolutional Neural Networks (CNNs). By definition any neural network which makes use of the convolutional layer is a Convolutional Neural Network (CNN). The “convolution”, seems pretty important right? As it turns out it precedes the CNN, and is a pretty cool mathematical operation in its own right.</p>
<p>So, let’s explore the convolution, figure out how it works and its place within CNNs.</p>
<section id="moving-averages---john-the-programmer" class="level2">
<h2 class="anchored" data-anchor-id="moving-averages---john-the-programmer">Moving Averages - John the Programmer</h2>
<p>Let’s kick off things nice and easy, with one of the most ubiquitous filters around…</p>
<p>Here’s the scenario, John is a programmer and we measure his productivity by the number of Lines of Code (LoC) he adds or removes from the code base. Too bad we lost track of his actual data, so we will be a little mean and sample it from a normal distribution (pun intended). A positive number represents an addition of LoC to the codebase and a negative number represents a removal.</p>
<div id="75b2708a" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)  <span class="co"># any integer can be used as the seed</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's assume John's LoC per hour follows a Normal distribution with</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># mean=50 and standard deviaton=50, also John is a real workhorse in this session</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># he programmed non-stop for 50 hours!</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>lines_of_code <span class="op">=</span> np.random.normal(<span class="dv">50</span>, <span class="dv">50</span>, <span class="dv">50</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(lines_of_code)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[ 74.83570765  43.08678494  82.38442691 126.15149282  38.29233126
  38.29315215 128.96064078  88.37173646  26.5262807   77.12800218
  26.82911536  26.71351232  62.09811358 -45.66401223 -36.24589163
  21.88562354  -0.64155602  65.71236663   4.59879622 -20.61518507
 123.28243845  38.71118498  53.37641023 -21.23740931  22.78086377
  55.54612949  -7.54967887  68.78490092  19.9680655   35.41531251
  19.91466939 142.61390923  49.32513876  -2.88554645  91.12724561
 -11.0421825   60.44317975 -47.98350619 -16.40930244  59.84306179
  86.923329    58.56841406  44.21758588  34.94481522 -23.92609952
  14.00778958  26.96806145 102.85611131  67.18091448 -38.15200777]</code></pre>
</div>
</div>
<p>Let’s visualise his output.</p>
<div id="9be45b10" class="cell" data-execution_count="57">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>hours <span class="op">=</span> np.arange(<span class="bu">len</span>(lines_of_code))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>plt.plot(hours, lines_of_code, marker<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Figure 1 - Lines of Code by Hour'</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Hour'</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Value'</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="from_scratch_convolutional_layers_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="fdf4daf7" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>lines_of_code.mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>38.726304737193</code></pre>
</div>
</div>
<p>Looking into John’s LoC data, he produces around 39 LoC per hour. However, looking at hourly snapshots alone can be misleading. For example, take hour 16 we might say John was very unproductive producing nothing in that hour! However, this granular view fails to capture that John was very productive in the previous 2 hours (John doesn’t like hourly based reviews either).</p>
<p>John’s output varies significantly from hour-to-hour, making an assessment of him by observing any one point quite unfair. To get a more accurate assessment we should look at his output over a wider period. Let’s take 3 hours as our period, this could be done by summing his output over 3 hour periods, let’s look into that:</p>
<div id="64bbb5ac" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sum_n(n):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> n <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        y_n <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y_n</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    y_n <span class="op">=</span> lines_of_code[n<span class="op">-</span><span class="dv">2</span>] <span class="op">+</span> lines_of_code[n<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> lines_of_code[n]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y_n</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>sum_lines_code_past_3_hours <span class="op">=</span> []</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(lines_of_code)):</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    sum_lines_code_past_3_hours.append(sum_n(i))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>hours <span class="op">=</span> np.arange(<span class="bu">len</span>(lines_of_code))</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>plt.plot(hours, lines_of_code, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'Lines of code'</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>plt.plot(hours, sum_lines_code_past_3_hours, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'Sum of 3 hours'</span>)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Figure 2 - Lines of Code by Hour'</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Hour'</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Value'</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>plt.legend()  <span class="co"># Add the legend</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="from_scratch_convolutional_layers_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Wait a minute… this doesn’t look right. The new graph is confusing, it’s not clear where removals take place and the overall behaviour seems to be messed up. The problem is by simply summing we’ve changed the scale of the data completely - we’re now showing the output over 3 hours. To rectify this we can divide each summation by 3 to get the <b>Moving Average (MA)</b> of the data. The MA tells us John’s average hourly output over 3 hours, which is exactly what we want. Then we can directly compare this MA with the original data!</p>
<p>Let’s perform this operation and take a look at the output.</p>
<section id="moving-averages" class="level3">
<h3 class="anchored" data-anchor-id="moving-averages">Moving Averages</h3>
<p>First a look at the math behind the MA, I promise this’ll be fun!</p>
<hr>
<section id="the-math-behind-mas" class="level4">
<h4 class="anchored" data-anchor-id="the-math-behind-mas">The Math Behind MAs</h4>
<p>The moving average is a type of filter, specifically a linear filter (from the signal processing field). A linear filter removes certain frequencies and allows others to pass through. There are many types of linear filter but the MA is a low-pass linear filter. Such a filter allows low frequencies to pass through and reduces the effect of (attenuates) high frequencies. In essence, we remove the peaks and troughs and smooth the data.</p>
<p>What do we mean by frequencies? In signal processing frequency typically refers to the speed at which a signal changes. Here we’re working with a time series, so we can take frequency to mean the rate of change in our series (namely the rate of change of John’s output). A low frequency occurs when the rate of change between points is low and vice versa for a high frequency. This gets us to why MA is a linear low-pass filter, its effect is to reduce the influence of any one point, by averaging we pull values into a tighter range.</p>
</section>
<section id="equation-time" class="level4">
<h4 class="anchored" data-anchor-id="equation-time">Equation Time!</h4>
<p>For John we want to average over 3 hours of input data, namely the previous 3 hours.</p>
<p>So, let’s take the 3rd datapoint in our lines of code series (to keep things simple I refer to this as <span class="math inline">\(x\)</span> below and the MA data is represented by the array <span class="math inline">\(y\)</span>).</p>
<p><span class="math display">\[
y[2] = \frac{1}{3}x[0] + \frac{1}{3}x[1] + \frac{1}{3}x[2] \tag{1}
\]</span></p>
<p>We can then generalise this to calculate the MA for any data point in our series:</p>
<p><span class="math display">\[
y[n] = \frac{1}{3}(x[n{-}2] + x[n{-}1] + x[n]) \tag{2} \\
\]</span> <span class="math display">\[
y[n] = \frac{1}{3}\sum\limits_{k=n{-}2}^{n}x[k] \tag{3}
\]</span></p>
<p>We can use this to calculate the 3 period MA at any point in the data. You could change this to work with any period, just edit the index range in the sum and the division before the sum. Note, this may seem a bit foreign at the moment but it will come into play shortly when we take a look at cross-correlation and convolution!</p>
<p>Lastly, the choice of the kernel values in an MA is not up to us, we choose the time period to filter but that forces the filter to have values 1/t with t items, where t is the period we average over, this ensures the filter sums to 1. We have t=3 and filter = [1/3, 1/3, 1/3], if we wanted t=5 the filter would be [1/5, 1/5, 1/5, 1/5, 1/5]. Doing this retains the scale of our data, preventing what happened earlier when we only summed the terms.</p>
<hr>
<p>I just introduced a new term “kernel”, throughout the post I will use filter or kernel interchangeably. The filter and the kernel refer to the same thing - the set of weights applied to the data (here that being [1/3, 1/3, 1/3]).</p>
</section>
<section id="implementing-the-moving-average" class="level4">
<h4 class="anchored" data-anchor-id="implementing-the-moving-average">Implementing the Moving Average</h4>
<p>Now we understand the math, we can implement the MA. The most important idea here is the sliding window, this idea will crop up many times in this post. The idea in question being: that we slide a kernel across our data and calculate the average at each point. This operation will become clearer when we look at kernels which have different values in them, whereas now we have just <span class="math inline">\(\frac{1}{3}\)</span> so bear with me. Nevertheless let’s look at a quick example.</p>
<p>Take the first 5 points of data. (run lines_of_code[0:5] to get your first 5 lines)</p>
<pre><code>lines_of_code[0:5] = [74.84,  43.09,  82.38, 126.15, 38.30]
kernel = 1/3

y(0) = 0
y(1) = 0
y(2) = 1/3(74.84 + 43.09 + 82.38)
y(3) = 1/3(43.09 + 82.38 + 126.15)
y(4) = 1/3(82.38 + 126.15 + 38.30)</code></pre>
<p>y(0) and y(1) are 0 because they are undefined, at these points we don’t have n-2 values before them. This is a problem in MA’s. The first data points are always 0 until we have in our case n-2 and in the general case it would be n - m, where m = window_size - 1. We can deal with this by either setting them to 0 or you can copy the first valid MA number into the undefined values.</p>
<p>y(2), y(3) and y(4) are where things get interesting. These, y(2), y(3), …, y(n) are the outputs of our windows. Take the last value in the MA calculation for y(2) 82.38, notice how it shifts to the left one position at a time in the subsequent calculations. This is what we mean by sliding window. Now, let’s take a look at how to implement it!</p>
<div id="84bdb2e8" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;We have a function to perform the multiplication and summation</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># It takes a parameter n, the index of the current number</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> avg_loc(n):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;The average is undefined for the first 2 points as we do not yet have 3 numbers to average</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> n <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        y_n <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y_n</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    y_n <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)<span class="op">*</span>(lines_of_code[n<span class="op">-</span><span class="dv">2</span>] <span class="op">+</span> lines_of_code[n<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> lines_of_code[n])</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y_n</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>avg_lines_code_past_3_hours <span class="op">=</span> []</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co"># This for loop implements the summation from equation (3)</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(lines_of_code)):</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    avg_lines_code_past_3_hours.append(avg_loc(i))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s take a look at the MA plot.</p>
<div id="5849b8b9" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>hours <span class="op">=</span> np.arange(<span class="bu">len</span>(lines_of_code))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>plt.plot(hours, lines_of_code, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'Lines of code'</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>plt.plot(hours, avg_lines_code_past_3_hours, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'3-Hour Moving Average'</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Figure 3 - Lines of Code by Hour'</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Hour'</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Value'</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="from_scratch_convolutional_layers_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can observe the MA’s low pass filtering at play. The MA (orange line) is smoother than the original (blue line). The MA has removed the large spikes in John’s output, which causes the smoothing, indicating we have removed some of the noise and made the overall trend clearer to see. So, on a 3 hour scale John’s output is more stable and may be a fairer way to assess him (John sure is happy right now!).</p>
<p>Let’s take a deeper look at the first 6 hours of John’s output to understand the effect of smoothing.</p>
<div id="9372f51e" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>lines_of_code[<span class="dv">0</span>:<span class="dv">6</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>array([ 74.83570765,  43.08678494,  82.38442691, 126.15149282,
        38.29233126,  38.29315215])</code></pre>
</div>
</div>
<p>The MA’s effect on each point then is as follows:</p>
<div id="b0366ac8" class="cell" data-scrolled="true" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'0 -&gt; 3 : </span><span class="sc">{</span>lines_of_code[<span class="dv">0</span>:<span class="dv">3</span>]<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span><span class="bu">sum</span>(lines_of_code[<span class="dv">0</span>:<span class="dv">3</span>])<span class="op">/</span><span class="dv">3</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'1 -&gt; 4 : </span><span class="sc">{</span>lines_of_code[<span class="dv">1</span>:<span class="dv">4</span>]<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span><span class="bu">sum</span>(lines_of_code[<span class="dv">1</span>:<span class="dv">4</span>])<span class="op">/</span><span class="dv">3</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'2 -&gt; 5 : </span><span class="sc">{</span>lines_of_code[<span class="dv">2</span>:<span class="dv">5</span>]<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span><span class="bu">sum</span>(lines_of_code[<span class="dv">2</span>:<span class="dv">5</span>])<span class="op">/</span><span class="dv">3</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'3 -&gt; 6 : </span><span class="sc">{</span>lines_of_code[<span class="dv">3</span>:<span class="dv">6</span>]<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span><span class="bu">sum</span>(lines_of_code[<span class="dv">3</span>:<span class="dv">6</span>])<span class="op">/</span><span class="dv">3</span><span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 -&gt; 3 : [74.83570765 43.08678494 82.38442691] -&gt; 66.76897316567901
1 -&gt; 4 : [ 43.08678494  82.38442691 126.15149282] -&gt; 83.87423488895888
2 -&gt; 5 : [ 82.38442691 126.15149282  38.29233126] -&gt; 82.2760836630897
3 -&gt; 6 : [126.15149282  38.29233126  38.29315215] -&gt; 67.57899207892514</code></pre>
</div>
</div>
<p>You can plainly see the smoothing taking place here. Take indices 2 -&gt; 5 the greatest difference between the lowest and highest value in the window is 87.86 (between 126.15 and 38.29). After taking the MA we get 82.28, now the distance between 38.29 (lowest value) and MA is 43.99 and the distance from 126.15 (highest point) to the MA is 43.87. In essence the MA balanced out the extremes in this window! Rather than focusing on small fluctuations we can look at the overall trend (i.e.&nbsp;despite John being distracted by Alex in hour 5 we can assess his output over the hours where he was locked in).</p>
<p>This is what makes the MA so powerful, it allows us to disregard the moment to moment fluctuations and observe long-term trends.</p>
<hr>
</section>
<section id="an-aside---the-low-pass-filter" class="level4">
<h4 class="anchored" data-anchor-id="an-aside---the-low-pass-filter">An aside - The Low-Pass Filter</h4>
<p>Attenuation is a reduction in signal strength, for us it is the smoothing effect where we sort of chop off the highs and lows of our data. The chopping off is determined by the cutoff frequency, it constitutes what is a low signal which we let pass and the high signals which get attenuated.</p>
<p>To decide which signals get attenuated, in an MA, we use the following equation:</p>
<p><span class="math display">\[
\text{fc} = \frac{0.443}{N*T}
\]</span></p>
<p>Where N = 3 (number of periods we want to average) and T = 1 (sampling period).</p>
<p>So, <span class="math inline">\(\text{fc} = \frac{0.443}{3*1} = 0.148\)</span> is our cut off frequency. It means that our cutoff is 0.148 cycles per hour and a cycle repeats every <span class="math inline">\(1/0.148=6.75\)</span> hour. So a pattern which repeats faster than every 6.75 hours will get attenuated. The amount of attenuation is a convention in signal processing, at the cutoff frequency we will reduce a signal to 70.7% of its strength (reducing the rate of change of our data). If it repeats faster than every 6.75 hours it will get attenuated more and less if it repeats slower than every 6.75 hours. This reduction of signal strength is what causes the smoothing while allowing long term trends to persist.</p>
<p>From the example we used above take hours 4 -&gt; 5, the change between them is <span class="math inline">\(126.15 - 38.29 = 87.86\)</span>. Then from the MA points 4 -&gt; 5 the change is <span class="math inline">\(83.87 - 82.28 = 1.59\)</span>. Then, the percentage of reduction is given by: <span class="math inline">\((87.86-1.59)/87.86 * 100 = 98.19%\)</span>. Meaning the signal was reduced to 1.81% of its signal strength, indicating the signal here repeats faster than every 6.75 hours. Put simply, the change of 87.86 represents a very rapid change in one hour, which is much faster than the 6.75 hour cutoff, causing a large reduction in its rate of change after the MA.</p>
<p>Disclaimer: I am not well versed in this field, if you know more about cutoff frequencies and how they work please let me know.</p>
<hr>
<p>And that’s the moving average a simple yet powerful filter.</p>
<p>Oh shucks, John’s company have implemented a new review process! Any change made by John must now be reviewed before being pushed to the code base. In total the review takes 2 hours, changes made by John are tested and checked by his team. After this the code is stable, tests passed, bugs squashed and code pushed to the main branch. Therefore, we may assume that changes from 2 hours ago are a more meaningful estimation of John’s output, rather than recent unvetted changes.</p>
<p>Can we modify our MA to apply more weight to the tested changes?</p>
<hr>
</section>
</section>
<section id="the-weighted-moving-average" class="level3">
<h3 class="anchored" data-anchor-id="the-weighted-moving-average">The Weighted Moving Average</h3>
<p><b>Weighted Moving Averages (WMA)</b> are your answer! Unlike the uniform filter [1/3, 1/3, 1/3] we used before, WMAs assign different weights to each value, giving greater importance to specific points in our data. We want code which passed the review process to be emphasized, so we design a new weighted filter: [1/2, 1/3, 1/6]. With this the first (t-2) value in any three hour window will have a larger impact on the output of the WMA. Let’s see it in action!</p>
<section id="the-math-behind-wmas" class="level4">
<h4 class="anchored" data-anchor-id="the-math-behind-wmas">The Math Behind WMAs</h4>
<p>Like the MA, the central part of WMA is the kernel. The MA has a uniform kernel: <span class="math inline">\([\frac{1}{3}, \frac{1}{3}, \frac{1}{3}]\)</span>. Whereas the WMA has a non-uniform kernel <span class="math inline">\([\frac{1}{2}, \frac{1}{3}, \frac{1}{6}]\)</span>.</p>
<p>Previously, for each window we sum the 3 numbers and then multiply by the 1 over the length of that window (<span class="math inline">\(1/3\)</span> or <span class="math inline">\(1/n\)</span> where n = length of kernel). In a WMA we can’t do that as the kernel is not uniform. We need a method to multiply two vectors and sum the result, luckily for us this operation exists and it’s called the dot product: <span class="math inline">\(a\cdot b = \sum\limits_{i=1}^{n}a[i]b[i]\)</span>. The plan is to take our kernel, slide it over the data and compute the dot product for each of the points. Note, for a dot product both arrays must have the same length which for us would be 3 for the window and 3 for the filter (it works in the general case too). Then to get <span class="math inline">\(y[n]\)</span>:</p>
<p><span class="math display">\[
y[n] = \sum\limits_{i=0}^{2}x[n-(2-i)]f[i], \text{where x is the window and f is the kernel}
\]</span></p>
<p>And we do so for each n point in our LoC series. The operation is called a sliding window dot product. The <span class="math inline">\(n-(2-i)\)</span> indexing of x ensures we look back in time at our data.</p>
</section>
<section id="implementing-the-wma" class="level4">
<h4 class="anchored" data-anchor-id="implementing-the-wma">Implementing the WMA</h4>
<div id="39f33c45" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We pass in a kernel this time, this function will work for any kernel</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> weighted_avg_loc(n, kernel):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;The average is undefined for the first n-1 points as we do not yet have n numbers to average</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n <span class="op">&lt;</span> <span class="bu">len</span>(kernel)<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        y_n <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y_n</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    y_n <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;The dot product is essentially a series of multiplications followed by a sum</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(kernel)):</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        y_n <span class="op">+=</span> (kernel[i]<span class="op">*</span>lines_of_code[n<span class="op">-</span>(<span class="dv">2</span><span class="op">-</span>i)])</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y_n.astype(<span class="bu">int</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="24b28478" class="cell" data-scrolled="true" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>weighted_avg_lines_code_past_3_hours <span class="op">=</span> []</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(lines_of_code)):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    weighted_avg_lines_code_past_3_hours.append(weighted_avg_loc(i, kernel<span class="op">=</span>[<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">6</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1ccdfe9f" class="cell" data-execution_count="58">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>hours <span class="op">=</span> np.arange(<span class="bu">len</span>(lines_of_code))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plt.plot(hours, lines_of_code, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'Lines of Code'</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>plt.plot(hours, avg_lines_code_past_3_hours, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'3-Hour Moving Average'</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>plt.plot(hours, weighted_avg_lines_code_past_3_hours, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'3-Hour Weighted Moving Average'</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> highlight_point(hours, data, point_idx, marker<span class="op">=</span><span class="st">'o'</span>, size<span class="op">=</span><span class="dv">15</span>, color<span class="op">=</span><span class="st">'red'</span>):</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>   <span class="co">"""</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">   Highlight a specific point on the plot with a marker.</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">   </span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">   Args:</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">       hours: x-axis data (time points)</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">       data: y-axis data (values to plot)</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">       point_idx: index of point to highlight</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co">       marker: marker style (default: 'o' for circle)</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co">       size: size of marker (default: 15)</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co">       color: color of marker (default: 'red')</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="co">   """</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>   plt.plot(hours[point_idx], </span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>           data[point_idx],</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>           marker,</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>           markersize<span class="op">=</span>size,</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>           fillstyle<span class="op">=</span><span class="st">'none'</span>,</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>           color<span class="op">=</span>color)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage example:</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>highlight_point(hours, weighted_avg_lines_code_past_3_hours, <span class="dv">25</span>, color<span class="op">=</span><span class="st">'red'</span>, size<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>highlight_point(hours, lines_of_code, <span class="dv">23</span>, color<span class="op">=</span><span class="st">'red'</span>, size<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Figure 4 - Lines of Code by Hour'</span>)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Hour'</span>)</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Value'</span>)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>plt.legend()  <span class="co"># Add the legend</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="from_scratch_convolutional_layers_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Looking at Figure 4, we can see that the Weighted Moving Average (WMA) is smoothing our data, but does so differently than the regular MA (orange line). Our custom kernel gives greater weight to the first element in each window, which changes how peaks and valleys are represented. To understand the effect of the WMA, focus on the highlighted points at positions n=25 and n-2=23. For the window at position 25, the input values are [-21.24, 22.78, 55.55]. Since our kernel weights the first value (n-2) most heavily, and this value (-21.24) is significantly negative and far from the window’s average, it pulls the WMA result downward. This is why the WMA point at position 25 is lower than the corresponding MA point, which would treat all three values equally.</p>
<p>What is the point of this? As discussed earlier, we wanted to apply a greater weighting to lines of code which have passed the review process. In other words, certain data points are more important than others across the window. Below, you’ll see some other WMA kernels in action alongside the ones we’ve already examined.</p>
<p>Note, that the smoothing we see here is similar to the MA. The WMA does have a cutoff frequency, but calculating it is a little more involved and we will skip it for now.</p>
<p>And that concludes the Weighted Moving Average!</p>
<div id="ab97e702" class="cell" data-execution_count="59">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>weighted_avg1 <span class="op">=</span> []</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>weighted_avg2 <span class="op">=</span> []</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>weighted_avg3 <span class="op">=</span> []</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(lines_of_code)):</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    weighted_avg1.append(weighted_avg_loc(i, kernel<span class="op">=</span>[<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">6</span>]))</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    weighted_avg2.append(weighted_avg_loc(i, kernel<span class="op">=</span>[<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">6</span>]))</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    weighted_avg3.append(weighted_avg_loc(i, kernel<span class="op">=</span>[<span class="dv">1</span><span class="op">/</span><span class="dv">6</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>]))</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>hours <span class="op">=</span> np.arange(<span class="bu">len</span>(lines_of_code))</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>plt.plot(hours, lines_of_code, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'Lines of Code'</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>plt.plot(hours, avg_lines_code_past_3_hours, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'3-Hour Moving Average'</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>plt.plot(hours, weighted_avg1, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'kernel=[1/2, 1/3, 1/6]'</span>)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>plt.plot(hours, weighted_avg2, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'kernel=[1/3, 1/2, 1/6]'</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>plt.plot(hours, weighted_avg3, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'kernel=[1/6, 1/3, 1/2]'</span>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Figure 5 - A Few Different WMAs'</span>)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Hour'</span>)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Value'</span>)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>plt.legend()  <span class="co"># Add the legend</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="from_scratch_convolutional_layers_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>I leave you with some other kernel examples, ponder how they smooth the data before moving on :)</p>
<hr>
</section>
</section>
</section>
<section id="d-filters" class="level2">
<h2 class="anchored" data-anchor-id="d-filters">2D Filters</h2>
<p>So far the kernels/filters we have used operated on 1d data. It’s time to move up a dimension and work with 2d data. The data type shifts from time series to images, a very natural representation of 2d data. As such, we move from signal processing to image processing.</p>
<p>The following grayscale image, or Jim the Cat, is our subject for this section:</p>
<div class="quarto-figure quarto-figure-center" style="text-align: center;">
<figure class="figure">
<p><img src="https://www.cs.montana.edu/courses/spring2004/430/lectures/02/cat256.jpg" class="img-fluid figure-img"></p>
<figcaption>Figure 6 - Jim the Cat</figcaption>
</figure>
</div>
<p>Kernels are useful for many things in image processing (<a href="https://homepages.inf.ed.ac.uk/rbf/HIPR2/filtops.htm">https://homepages.inf.ed.ac.uk/rbf/HIPR2/filtops.htm</a>). We’ll start with the MA, and apply it to an image. Instead of a kernel which averages over previous data, our kernel will be a 2d grid that averages pixel intensities in a local area. This kernel is called a <b>Box Blur</b>, as with the MA the purpose is to reduce noise. When applied to images the kernel creates a blurring effect by smoothing out sharp transitions between pixels in images, which gives the kernel its name!</p>
<section id="box-blur" class="level3">
<h3 class="anchored" data-anchor-id="box-blur">Box Blur</h3>
<p>The math here is the same as in the 1d MA so we can push ahead without going through it again, the code will be our bridge.</p>
<p>The image in Fig 6 is a 2d grid of numbers, the information within it can be presented in a 2d matrix:</p>
<div id="34c1d9f1" class="cell" data-scrolled="true" data-execution_count="60">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> io <span class="im">import</span> BytesIO</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Read image from URL - saves us from downloading it :</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://www.cs.montana.edu/courses/spring2004/430/lectures/02/cat256.jpg"</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># We add a header so that our reqeust looks a little real and does not return an error 403.</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>headers <span class="op">=</span> {<span class="st">'User-Agent'</span>: <span class="st">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'</span>}</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> requests.get(url, headers<span class="op">=</span>headers)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(BytesIO(response.content))</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to numpy array </span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co"># We select one of the three channels, the image is stored in RGB even though it is greyscale</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;This is probably just to maintain a standard. All three channels are equal and you can select any of</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;channel 0, 1 or 2 and get the same resulting array.</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>img_array <span class="op">=</span> np.array(img)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print shape and data</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image shape:"</span>, img_array.shape)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image data:"</span>)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(img_array)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Image shape: (256, 256)
Image data:
[[176 177 174 ... 131 129 126]
 [177 178 176 ... 131 129 126]
 [177 179 178 ... 131 127 124]
 ...
 [150 160 163 ...  91  91  93]
 [170 171 174 ...  97  96  95]
 [195 189 187 ... 102  99  97]]</code></pre>
</div>
</div>
<p>So, our data is in a 2d matrix (or a list of lists in Python) and the kernel is 2d too! The operation is again a multiplication followed by a sum (dot product). In 2d we also do element wise multiplication and then a summation over all the points. But, it doesn’t strictly constitute a dot product anymore as the dot product is defined as an operation between two vectors. However, the implementation is similar and we can still think of it as the dot product.</p>
<p>We again use n=3 for our kernel. The average will then be across a 3x3 area and the filter is as follows:</p>
<p><span class="math display">\[
\text{kernel} = \begin{bmatrix}
\frac{1}{9} &amp; \frac{1}{9} &amp; \frac{1}{9}\\
\frac{1}{9} &amp; \frac{1}{9} &amp; \frac{1}{9}\\
\frac{1}{9} &amp; \frac{1}{9} &amp; \frac{1}{9}
\end{bmatrix}
\]</span></p>
<p>Generally a kernel of size N would be an NxN matrix of the form:</p>
<p><span class="math display">\[
\text{kernel} = \begin{bmatrix}
\frac{1}{N^2} &amp; \frac{1}{N^2} &amp; \frac{1}{N^2}\\
\frac{1}{N^2} &amp; ... &amp; ... \\
\frac{1}{N^2} &amp; ... &amp;  \frac{1}{N^2}
\end{bmatrix}
\]</span></p>
<p>Our sliding window will operate on the data surrounding each pixel, we centre the kernel around the pixel. However, at the edges of images we can’t do this because we don’t have the values around the centre (Similar to MA where the first n-1 values were 0/undefined). The easiest solution is to not calculate for those points and set them to 0. In an image this leads to a reduction in the dimensions of the image<a href="#1"><sup>1</sup></a>.</p>
<p>The sliding window operation works as before, except we slide across the vertical axis too. Here’s a small example with some 5x5 to which we will apply our 3x3 (<span class="math inline">\(\frac{1}{9})\)</span> kernel:</p>
<p>Sample data: <span class="math display">\[
\begin{bmatrix}
2 &amp; 5 &amp; 1 &amp; 4 &amp; 3 \\
6 &amp; 3 &amp; 8 &amp; 2 &amp; 7 \\
1 &amp; 4 &amp; 2 &amp; 5 &amp; 1 \\
3 &amp; 7 &amp; 9 &amp; 2 &amp; 4 \\
2 &amp; 1 &amp; 5 &amp; 6 &amp; 8
\end{bmatrix}
\]</span></p>
<p>The kernel starts at position [1,1] highlighted in red, because that is the first position where with centered data our 3x3 kernel fits. Observe how the shift takes place.</p>
<p><span class="math display">\[
\left[
\begin{matrix}
\colorbox{cyan}{2} &amp; \colorbox{cyan}{5} &amp; \colorbox{cyan}{1} &amp; 4 &amp; 3 \\
\colorbox{cyan}{6} &amp; \colorbox{red}{3} &amp; \colorbox{cyan}{8} &amp; 2 &amp; 7 \\
\colorbox{cyan}{1} &amp; \colorbox{cyan}{4} &amp; \colorbox{cyan}{2} &amp; 5 &amp; 1 \\
3 &amp; 7 &amp; 9 &amp; 2 &amp; 4 \\
2 &amp; 1 &amp; 5 &amp; 6 &amp; 8
\end{matrix}
\right]
\quad
\left[
\begin{matrix}
2 &amp; \colorbox{cyan}{5} &amp; \colorbox{cyan}{1} &amp; \colorbox{cyan}{4} &amp; 3 \\
6 &amp; \colorbox{cyan}{3} &amp; \colorbox{red}{8} &amp; \colorbox{cyan}{2} &amp; 7 \\
1 &amp; \colorbox{cyan}{4} &amp; \colorbox{cyan}{2} &amp; \colorbox{cyan}{5} &amp; 1 \\
3 &amp; 7 &amp; 9 &amp; 2 &amp; 4 \\
2 &amp; 1 &amp; 5 &amp; 6 &amp; 8
\end{matrix}
\right]
\quad
\left[
\begin{matrix}
2 &amp; 5 &amp; \colorbox{cyan}{1} &amp; \colorbox{cyan}{4} &amp; \colorbox{cyan}{3} \\
6 &amp; 3 &amp; \colorbox{cyan}{8} &amp; \colorbox{red}{2} &amp; \colorbox{cyan}{7} \\
1 &amp; 4 &amp; \colorbox{cyan}{2} &amp; \colorbox{cyan}{5} &amp; \colorbox{cyan}{1} \\
3 &amp; 7 &amp; 9 &amp; 2 &amp; 4 \\
2 &amp; 1 &amp; 5 &amp; 6 &amp; 8
\end{matrix}
\right]
\quad
\left[
\begin{matrix}
6 &amp; 3 &amp; 8 &amp; 2 &amp; 7 \\
\colorbox{cyan}{1} &amp; \colorbox{cyan}{4} &amp; \colorbox{cyan}{2} &amp; 5 &amp; 1 \\
\colorbox{cyan}{3} &amp; \colorbox{red}{7} &amp; \colorbox{cyan}{9} &amp; 2 &amp; 4 \\
\colorbox{cyan}{2} &amp; \colorbox{cyan}{1} &amp; \colorbox{cyan}{5} &amp; 6 &amp; 8 \\
2 &amp; 1 &amp; 5 &amp; 6 &amp; 8
\end{matrix}
\right]
\]</span></p>
<p>First window computation (center at [1,1]): <span class="math display">\[
\frac{1}{9}(2 + 5 + 1 + 6 + 3 + 8 + 1 + 4 + 2) =  3.56
\]</span></p>
<p>Second window computation (center at [1,2]): <span class="math display">\[
\frac{1}{9}(5 + 1 + 4 + 3 + 8 + 2 + 4 + 2 + 5) =  3.78
\]</span></p>
<p>Third window computation (center at [1,3]): <span class="math display">\[
\frac{1}{9}(1 + 4 + 3 + 8 + 2 + 7 + 2 + 5 + 1) =  3.67
\]</span></p>
<p>Fourth window computation (center at [2,1]): <span class="math display">\[
\frac{1}{9}(1 + 4 + 2 + 3 + 7 + 9 + 2 + 1 + 5) =  3.78
\]</span></p>
<p>Notice, in the summation and multiplication part we have flattened the data to be a dot product. In the implementation to follow we won’t do this, it just helps our understanding. Also, we can simply multiply all datapoints in each operation by 1/9 as our kernel is uniform.</p>
<p>In general for an NxN kernel we can say that: <span class="math inline">\(\text{start}=\frac{(N-1)}{2}\)</span>.</p>
<p>For N=3, then <span class="math inline">\(\text{start}=\frac{(3-1)}{2} = 1\)</span>, N=5, then <span class="math inline">\(\text{start}=\frac{(5-1)}{2} = 2\)</span> etc<a href="#2"><sup>2</sup></a>. The sliding operation remains the same regardless of N size.</p>
<section id="the-math-behind-the-box-blur" class="level4">
<h4 class="anchored" data-anchor-id="the-math-behind-the-box-blur">The Math Behind the Box Blur</h4>
<p>Let’s generalise what we see above with a formula :)</p>
<p><span class="math display">\[
y[i,j] = \sum_{m=-\lfloor N/2 \rfloor}^{\lfloor N/2 \rfloor}\sum_{n=-\lfloor N/2 \rfloor}^{\lfloor N/2 \rfloor} x[i+m,j+n]k[m+\lfloor N/2 \rfloor,n+\lfloor N/2 \rfloor]
\]</span></p>
<p>Where <span class="math inline">\(\lfloor \rfloor\)</span> is the floor operator, same as the // in Python. We remove the remainder of any division and round down. The indexing of <span class="math inline">\(k\)</span> works to reverse the range of the summation, <span class="math inline">\([-\lfloor \frac{N}{2} \rfloor, \lfloor \frac{N}{2} \rfloor]\)</span>, which in our case would be from [-1, 1]. Say we are at -1, we have <span class="math inline">\(m = -1\)</span> and then <span class="math inline">\(-1 + 1 = 0\)</span>, similarly at <span class="math inline">\(m = 0\)</span> we get <span class="math inline">\(0 + 1\)</span> giving us the needed indices to correctly index our kernel.</p>
</section>
<section id="implementing-the-box-blur" class="level4">
<h4 class="anchored" data-anchor-id="implementing-the-box-blur">Implementing the Box Blur</h4>
<div id="6f15056b-0bdb-4fe7-ab35-c7a737bdc37d" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;We can generate the range of values which m and n will take. This saves us some time later.</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> itertools <span class="im">import</span> product</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_kernel_range(N):</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> N <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> calc_kernel_range(N)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>ranges <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="op">-</span>index, index<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># product creates all combinations of two iterable datatypes</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>ranges <span class="op">=</span> <span class="bu">list</span>(product(ranges, ranges))</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>ranges</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>[(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 0), (0, 1), (1, -1), (1, 0), (1, 1)]</code></pre>
</div>
</div>
<div id="aef12f5b-7ddc-413d-8cf5-09d6106ccf32" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We implement the general formula using three for loops.</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> box_blur(data, kernel):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> []</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> <span class="bu">len</span>(kernel[<span class="dv">0</span>])</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    edge_limit <span class="op">=</span> (N<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data)):</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;Must ensure we are in valid range such that our kernel fits</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">&lt;</span> edge_limit <span class="kw">or</span> i <span class="op">&gt;=</span> <span class="bu">len</span>(data) <span class="op">-</span> edge_limit:</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        row <span class="op">=</span> []</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data[i])):</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>            <span class="co">#&nbsp;temp_sum holds the </span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>            temp_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>            <span class="co">#&nbsp;Must ensure we are in valid range such that our kernel fits</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> j <span class="op">&lt;</span> edge_limit <span class="kw">or</span> j <span class="op">&gt;=</span> <span class="bu">len</span>(data[i]) <span class="op">-</span> edge_limit:</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> m, n <span class="kw">in</span> ranges:</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>                idx_i <span class="op">=</span> i <span class="op">+</span> m</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>                idx_j <span class="op">=</span> j <span class="op">+</span> n</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>                temp_sum <span class="op">+=</span> data[idx_i, idx_j] <span class="op">*</span> kernel[m<span class="op">+</span>(N<span class="op">//</span><span class="dv">2</span>), n<span class="op">+</span>(N<span class="op">//</span><span class="dv">2</span>)]</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>            row.append(temp_sum)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>        result.append(row)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(result)</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>                </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s run a sample matrix through our function. To make sure it works I suggest you try and calculate the box blurred matrix by hand. The numbers highlighted in green are the ones which will form the centre of our kernels. Slide across each one calculate the output and then check if the matrix you create matches the one we see here.</p>
<div id="e7f31df0-0290-4454-b0e8-b2384bf6ccb3" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>) </span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">10</span>, (<span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>kernel <span class="op">=</span> np.full((<span class="dv">3</span>, <span class="dv">3</span>), <span class="dv">1</span><span class="op">/</span><span class="dv">9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><span class="math display">\[
\left[
\begin{matrix}
6 &amp; 3 &amp; 7 &amp; 4 &amp; 6 \\
9 &amp; \colorbox{green}{2} &amp; \colorbox{green}{6} &amp; \colorbox{green}{7} &amp; 4 \\
3 &amp; \colorbox{green}{7} &amp; \colorbox{green}{7} &amp; \colorbox{green}{2} &amp; 5 \\
4 &amp; \colorbox{green}{1} &amp; \colorbox{green}{7} &amp; \colorbox{green}{5} &amp; 1 \\
4 &amp; 0 &amp; 9 &amp; 5 &amp; 8
\end{matrix}
\right]
\]</span></p>
<div id="5907490e-7151-4095-aa36-c031658230ed" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>box_blur(data, kernel)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>array([[5.55555556, 5.        , 5.33333333],
       [5.11111111, 4.88888889, 4.88888889],
       [4.66666667, 4.77777778, 5.44444444]])</code></pre>
</div>
</div>
<p>In the example above we see the dimensionality reduction. In a 3x3 kernel we need to start at position [1,1], we cannot (without special measures) start in row 0 or column 0 for any kernel operation. Likewise, on the other side we cannot start a kernel operation in row/column 4. Hence, we lose 2 dimensions from this matrix. Generally, the dimensionality reduction is due to the constraint of making the kernel fit.</p>
<p>Now, let’s go back to our grayscale image and see what effect the box blur has.</p>
<div id="d739b192-a1cb-44f6-9be7-0b8dfd9b67ed" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>img_blurred <span class="op">=</span> box_blur(img_array, kernel)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>img_array.shape, img_blurred.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>((256, 256), (254, 254))</code></pre>
</div>
</div>
<div id="75562e70-de06-4811-968b-1a79f6b61939" class="cell" data-execution_count="65">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a figure with two subplots side by side</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))  <span class="co"># Optional: adjust figure size</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Original image (left)</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(img_array, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Figure 7a - Original Jim'</span>)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Blurred image (right)</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)  <span class="co"># (rows, columns, position)</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>plt.imshow(img_blurred, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Figure 7b - Blurred Jim'</span>)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust layout to prevent overlap</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the figure</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="from_scratch_convolutional_layers_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The image has been blurred and does appear to be smoother. A good example is the checkerboard above the cat. Once again this operation acts in the same way as the MA we studied before. In the cat image, by using the box blur we average across a 3x3 area, reducing the effect of any one pixel and as before reducing the strength of the highest and lowest intensity pixels (analogous to the signal strength).</p>
<p>Let’s move on to take a look at a specially designed kernel which unlocks more ideas on the road to the convolutional neural network.</p>
</section>
</section>
<section id="edge-detection-kernels" class="level3">
<h3 class="anchored" data-anchor-id="edge-detection-kernels">Edge Detection Kernels</h3>
<p>Until this point the kernels used have been very basic, they were just averaging kernels (the WMA was a slight departure) and their purpose has been the same. Edge detection is a task in image processing where we try and highlight the edges of objects in an image. This discussion will form the foundation for understanding what a CNN does.</p>
<section id="the-prewitt-operator" class="level4">
<h4 class="anchored" data-anchor-id="the-prewitt-operator">The Prewitt Operator</h4>
<p>The <b>Prewitt Operator</b> is a specialised kernel to detect edges. It contains two kernels, one to detect horizontal edges and another to detect vertical edges.</p>
<p><span class="math display">\[
\text{horizontal} = \left[
\begin{matrix}
-1 &amp; -1 &amp; -1 \\
0 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 1 \\
\end{matrix}
\right]
\]</span></p>
<p><span class="math display">\[
\text{vertical} = \left[
\begin{matrix}
-1 &amp; 0 &amp; 1 \\
-1 &amp; 0 &amp; 1 \\
-1 &amp; 0 &amp; 1 \\
\end{matrix}
\right]
\]</span></p>
<p>An edge in an image is characterized by an abrupt change in pixel intensity, which directly informs the design of Prewitt kernels. These kernels are specifically crafted to detect intensity changes in different directions. Consider the horizontal operator: it contains -1 values in the top row, 0s in the middle row, and 1s in the bottom row. This pattern effectively negates the top row of pixels, ignores the middle row, and preserves the bottom row before summing. The intuition is straightforward - where an edge exists, pixels on opposite sides will have significantly different intensities. The greater this intensity difference, the larger the absolute value produced by the kernel. When the output approaches zero, it indicates similar pixel intensities on both sides, suggesting no edge is present. Let’s look at an example:</p>
<p><span class="math display">\[
\text{window} = \left[
\begin{matrix}
200 &amp; 255 &amp; 193 &amp; 201 \\
150 &amp; 172 &amp; 169 &amp; 132\\
70 &amp; 80 &amp; 83 &amp; 54\\
\end{matrix}
\right]
\]</span></p>
<p>If we use the horizontal filter on <span class="math inline">\(\text{window}\)</span> we get:</p>
<p><span class="math display">\[
\text{horizontal\_edges} = \left[
\begin{matrix}
-415 &amp; -432
\end{matrix}
\right]
\]</span></p>
<p>And these are quite large (absolute) numbers (given pixel intensity only goes up to 255) so the filter tells us an edge exists here. The numbers exceeding 255 are a result of not normalising our data, but we don’t need to here. In an actual output image, these would be scaled to fit in the range 0-255 and you’d see them as bright white. Let’s look at another example where an edge would not be present:</p>
<p><span class="math display">\[
\text{window} = \left[
\begin{matrix}
195 &amp; 182 &amp; 193 &amp; 179 \\
150 &amp; 172 &amp; 169 &amp; 132\\
176 &amp; 201 &amp; 195 &amp; 187\\
\end{matrix}
\right]
\]</span></p>
<p><span class="math display">\[
\text{horizontal\_edges} = \left[
\begin{matrix}
2 &amp; 29
\end{matrix}
\right]
\]</span></p>
<p>This time the filter outputs are close to 0 indicating a lack of edges.</p>
<p>In general, how will the edges show in our output? If the filter detects an edge it will output large numbers, these translate to high pixel intensities (negative values will be flipped) and vice versa for when the filter outputs numbers close to 0. A high pixel value represents a white colour and a low will be black, the differences we see will highlight the edges. Knowing this, we can apply the Prewitt Operator to our cat image.</p>
<div id="5e659530-487b-4de0-a106-cfd58af53422" class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>horizontal_edge_kernel <span class="op">=</span> np.array([</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>vertical_edge_kernel <span class="op">=</span> np.array([</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>horizontal_edges <span class="op">=</span> box_blur(img_array, horizontal_edge_kernel)</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>vertical_edges <span class="op">=</span> box_blur(img_array, vertical_edge_kernel)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="0a9bca2e-ea03-40c2-a393-02898779c9ac" class="cell" data-execution_count="68">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a figure with two subplots side by side</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))  <span class="co"># Optional: adjust figure size</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Original image (left)</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(horizontal_edges, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Horizontal Edges'</span>)  <span class="co"># Optional: add title</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Blurred image (right)</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)  <span class="co"># (rows, columns, position)</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>plt.imshow(vertical_edges, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Vertical Edges'</span>)  <span class="co"># Optional: add title</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust layout to prevent overlap</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the figure</span></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="from_scratch_convolutional_layers_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The eyes are the best example here, in the horizontal edge image we see the clear difference in pixel values. If you look back at Fig 6, the part around the eyes is black, creating a large abrupt change in pixel density at the border which our filter picks up on.</p>
<p>In this edge detection case we see the true power of what kernels can do, we are not just doing a simple operation like averaging anymore rather we are detecting underlying patterns in our data! The Prewitt kernel has been specifically designed to capture patterns in the data whereby pixel intensity differs, the greater the difference the greater the output of the filtering operation.</p>
<p>The approach we have been using to apply our kernels is <b>cross-correlation</b>. Cross-correlation is a pattern matching operation between two pieces of data whereby the greater the similarity between the kernel and data the greater the value of the output, as we saw with the Prewitt operator. This is also where the idea for the sliding dot product is from. It’s time to embark on a deep dive of cross-correlation and its closely related operation <b>convolution</b>.</p>
<hr>
</section>
</section>
</section>
<section id="cross-correlation-and-convolution" class="level2">
<h2 class="anchored" data-anchor-id="cross-correlation-and-convolution">Cross-correlation and Convolution</h2>
<p>So far, we’ve seen how the sliding window dot product can help us to process data whether it be time series or images. The operations we have been using apply the cross-correlation technique. Cross-correlation has a closely related operation named convolution, which gives the CNN its name. Convolution uses the same sliding window dot product method, but differs in that either the kernel or the data is flipped. This flipping has some interesting effects, especially when dealing with time dependent data. To better understand the implications, let’s turn to our friend John.</p>
<section id="cross-correlation-and-convolution---an-intuitive-approach" class="level3">
<h3 class="anchored" data-anchor-id="cross-correlation-and-convolution---an-intuitive-approach">Cross-Correlation and Convolution - An Intuitive Approach</h3>
<p>John’s company have implemented another new review process where instead of reviews taking 2 hours there is a multi-step 3 day process. On day 1 there is 1 reviewer, on day 2 there are 3 reviewers and 5 reviewers on day 3. The reviewers are very thorough and whatever changes we give them, they will review in one day ready for review by the next set of reviewers the next day. Our goal is to track the total number of changes reviewed at the end of any one day. Let’s get some dummy data to explore this problem:</p>
<p>Lines of Code: [5, 10, 9, 21, 5, 7] Reviewers: [1, 3, 5]</p>
<p>So, the task is to keep track of how many lines of code all reviewers have read. Here’s a simple way to do it (we will use the <span class="math inline">\(y[x]\)</span> to keep it clean and pad the lines of code with zeros such that we can calculate <span class="math inline">\(y[0]\)</span> and <span class="math inline">\(y[1]\)</span>):</p>
<pre><code>- Day 0 - y[0] = 5*1 = 5
- Day 1 - y[1] = 5*3 + 10*1 = 25
- Day 2 - y[2] = 5*5 + 10*3 + 9*1 = 64
- And so on and so forth...</code></pre>
<p>This manual method is tedious. A thought arises, we have two lists so let’s try to apply cross-correlation on this task!</p>
<pre><code>- y[0] = 1*0 + 3*0 + 5*5 = 25
- y[1] = 1*0 + 3*5 + 5*10 = 65
- y[2] = 1*5 + 3*10 + 5*9 = 75
- ...</code></pre>
<p>Woah, something seems off here. Take a closer look at the order of operation in the cross correlation approach. Take y[0], on day 0 we have only 1 reviewer but here we act as if we have 5. Cross-correlation is applying our kernel backwards!</p>
<p>Let’s flip it then, by doing so we move into a convolution operation. By flipping the kernel becomes: [5, 3, 1].</p>
<pre><code>- y[0] = 5*0 + 3*0 + 1*5 = 5
- y[1] = 5*0 + 3*5 + 1*10 = 25
- y[2] = 5*5 + 3*10 + 1*9 = 64</code></pre>
<p>Convolution matches our manual calculation! The key here was flipping the kernel, when flipped and convolved the indices line up with the correct time order. The most recent lines of code passed to reviewers will always be reviewed by one reviewer as intended by the company, this is reflected in our kernel too!</p>
</section>
<section id="the-maths-of-convolution-and-cross-correlation" class="level3">
<h3 class="anchored" data-anchor-id="the-maths-of-convolution-and-cross-correlation">The Maths of Convolution and Cross-correlation</h3>
<p>The final piece of the puzzle is a coverage of the general mathematics behind both convolution and cross-correlation.</p>
<p>Take our convolution example, we have two lists of data, we flipped one of them and then multiply and sum. We can write this generally as:</p>
<p><span class="math display">\[
y[n] = \sum_{i=0}^{M-1}x[i]k[n-i]
\]</span></p>
<p><span class="math inline">\(y[n]\)</span> is any output and <span class="math inline">\(M=\text{len(k)}\)</span> where <span class="math inline">\(k\)</span> is the kernel. Likewise, cross-correlation can be seen as:</p>
<p><span class="math display">\[
y[n] = \sum_{i=0}^{M-1}x[i]k[n+i]
\]</span></p>
<p>Now you see how simple the difference is, it almost makes you wonder why the different operations? Firstly, they can result in wildly different outputs because convolution looks back in a time aware fashion but cross-correlation doesn’t. Convolution is better suited in cases where causality is paramount, in signal processing the current value is gotten from previous ones and in time series the current value is usually somewhat dependent on past values, more importantly in both cases in a live system you have no idea what is coming next. Whereas, cross-correlation is more concerned with pattern matching as in the Prewitt case.</p>
<hr>
<section id="an-aside---calculus" class="level4">
<h4 class="anchored" data-anchor-id="an-aside---calculus">An Aside - Calculus</h4>
<p>Both cross-correlation and convolution have a continuous formula, currently we have observed the discrete version which works with arrays that have a defined set of values. However, through calculus we can define convolution and cross-correlation for continuous data or functions. Given the data we have previously for the review system, take [1, 3, 5] to be the function k(x) where k(0)=1, k(1)=3 and k(2)=5 and likewise for the data list we make it f(x). So f(x) = data function and k(x) = kernel function.</p>
<p>By this, the definition of our current convolution becomes:</p>
<p><span class="math display">\[
y[n] = \sum_{i=0}^{M-1}x(i)k(n-i)
\]</span></p>
<p>and cross-correlation becomes:</p>
<p><span class="math display">\[
y[n] = \sum_{i=0}^{M-1}x(i)k(n+i)
\]</span></p>
<p>See if you can spot the difference, it’s very subtle… The square brackets are now normal brackets! Because we are no longer indexing an array but passing parameters to a function which returns an output and for the values we defined happens to return exactly the values we need… hmmm very convenient (note that what the actual function is doesn’t matter just that it returns the values we need at the specific range, this is sort of confusing but imagine it’s just a perfect function whose form is of little importance). We can extend this idea, a summation is still discrete, the values can be made smaller but not infinitely small. Enter calculus, the integral can be used to sum continuous data! The integral adds up infinitesimally small parts over ranges, so we can sum over every possible point for where x(x) and k(x) are defined. So we get:</p>
<p>Convolution:</p>
<p><span class="math display">\[
\int^{\infty}_{-\infty} x(\tau)k(t-\tau) d\tau
\]</span></p>
<p>Cross-correlation:</p>
<p><span class="math display">\[
\int^{\infty}_{-\infty} x(\tau)k(t+\tau) d\tau
\]</span></p>
<p>Here are the major changes: the summation becomes an integral and we switch out i and n for <span class="math inline">\(\tau\)</span> and t respectively, moving us from the discrete to continuous space. For all intents and purposes, to us these are the same and in theory they work the same too. In most machine learning cases you will have discrete data so the the previous formulation will serve us well enough.</p>
<p>So, if any ever asks you to explain cross-correlation you can tell them: it’s fancy multiplication where we take a kernel, slide it over our data, take the product and sum all points doing this many times over and over. Cross-correlation gives us a method to pattern match. If they ask for convolution you tell them: it’s cross-correlation with the kernel flipped, the flipping adds a sense of causality to our operation.</p>
<hr>
</section>
</section>
</section>
<section id="convolutional-neural-networks-cnn" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)</h2>
<p>So far, all kernels were hand selected or defined by the task. While this approach works for well-defined operations like edge detection or blurring, it falls short for complex challenges. Consider this: if you wanted to create a system to distinguish between cats and dogs, what kernel values would you choose? With potentially an infinite set of kernel values, it’s impossible to choose the right one. This is where the neural network steps in, combines with filters and out steps the CNN!</p>
<p>The CNN represents one of deep learning’s most powerful ideas: instead of hand-crafting kernels, let the network learn the optimal values for a given task. It consists of multiple layers, with the convolutional layer at its heart - where many learnable filters work together to identify different patterns in input images. While other types of layers<a href="#3"><sup>3</sup></a> support the network, the convolutional layer is the star of the show.</p>
<p>Interestingly, despite the name, CNNs use cross-correlation rather than convolution when sliding filters over inputs (don’t just take it from me, look at the <a href="https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/conv.py#L495">PyTorch implementation</a>). More on this to come.</p>
<p>To avoid ambiguity, we’ll refer to this operation simply as the ‘conv’ operation - it’s the beautiful combination of cross-correlation’s pattern-matching ability with the neural network’s capacity to learn optimal parameters.</p>
<section id="adapting-cross-correlation-to-the-neural-network-setting" class="level3">
<h3 class="anchored" data-anchor-id="adapting-cross-correlation-to-the-neural-network-setting">Adapting Cross-correlation to the Neural Network Setting</h3>
<p>So, previously all kernels were either defined by the task (MA) or hand designed specifically to achieve some goal (Prewitt Operator, blurring and WMA). This is counter to the neural network approach, where we let the algorithm decide the parameters (here parameters mean the values in our kernel) of our operations. Naturally you wonder, what if we can enable a neural network to learn the parameters of a kernel? This wonderful idea is what gives us the CNN, rather than selecting parameters by hand we let the model learn the best parameters to complete our task, best meaning that which minimises the loss the most. Honestly that’s it, instead of defining kernels we let the model do it! From this arises the lack of an actual convolution operation in the CNN, the model doesn’t care if we use cross-correlation or convolution! The model decides which orientation the kernel should be - if it is best to learn the flipped kernel in cross-correlation (the convolution kernel) then it does and if we were to use convolution it may end up learning the flipped kernel (so the cross-correlation kernel). By allowing the model to learn the kernel, we alleviate the decision of whether to use cross-correlation or convolution.</p>
<p>The implementation of conv in CNNs doesn’t stray from what we’ve built before, we continue to use a sliding window dot product with a kernel and a summation or simple fancy multiplication. However there are two big changes, firstly we need to make the operation work with RGB data and second add the framework which lets our model learn the parameters. On the first change, an RGB image has 3 channels or 3 2d maps of information so our kernel needs to work across channels. The data seems to be 3d now, however it is not. Each pixel has 3 values (the channels store this info) at the same location, whereas in true 3d the 3 channels represent different spatial locations. To deal with this, we apply three separate kernels on each 2d map in any given image and then sum the outputs to give us one position (this differs from typical cross-correlation or convolution scenario, there with an RGB image we get RGB channels out too, as we don’t sum the results across channels)</p>
<p>Now, this is remarkable but no accident. The conv operation (op) itself has many attractive properties, which make it viable to use in the CNN. The 3 key properties are: sparse interactions, parameter sharing and equivariance to translation.</p>
<section id="sparse-interactions" class="level4">
<h4 class="anchored" data-anchor-id="sparse-interactions">Sparse Interactions</h4>
<p>In traditional fully connected neural networks, images present a problem. Namely, any given image has a lot of data points. Take an image of size 128x128x3, to create a fully connected layer you would need 49152 weights. Then imagine many of these layers connected together, your parameter count would grow very quickly. In comes the kernel, each value in the output of our conv operation (we call the output a filter map) is dependent upon the parameters of our kernel. However, because the kernel is usually much smaller than the input data each output point is only affected by a small subset of input values, contrast this with the fully connected layer where each point in the filter map would be affected by each pixel in the input. By doing this we increase the efficiency of our network.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="from_scratch_convolutional_layers_files/figure-html/eaa020b6-dd2e-45ce-9f96-1e381e8580b3-1-9aa8653c-6064-405a-a466-bcf6cd0191da.png" class="img-fluid figure-img"></p>
<figcaption>Figure 8 - Sparse Interactions - <a href="https://www.deeplearningbook.org/contents/convnets.html">Source</a></figcaption>
</figure>
</div>
</section>
<section id="parameter-sharing" class="level4">
<h4 class="anchored" data-anchor-id="parameter-sharing">Parameter Sharing</h4>
<p>Look back at any of the examples we studied, we set the parameters for the kernel and used them across the data without changing the values. This is a key idea in cross-correlation/convolution and subsequently in the conv operation. The assumption is that if we can detect a useful feature in one part of the data it will appear in other parts of the data. So, with the conv op, where we learn the parameters, instead of learning different parameters for each input pixel, we can just learn the <span class="math inline">\(k \times k\)</span> kernel and apply it across the image.</p>
</section>
<section id="equivariance-to-translation" class="level4">
<h4 class="anchored" data-anchor-id="equivariance-to-translation">Equivariance to Translation</h4>
<p>Translation means a shift of the data. Take images, a translation could be moving the image up, down, left or right. Being equivariant means that as input changes the output changes in the same way. Put both together, if our image is moved then our output won’t change materially it’ll just move too. This property, is the one which allows parameter sharing as our kernels can detect the same feature if they are just moved around the image.</p>
<p>This property increases the generality of our models from train time to test time. Imagine in our training data we have cats, but they only appear in the left side of images. Then the model will create filters good at detecting cats, now during test time we have an image of a cat on the right side, what happens? Well, due to equivariance to translation the exact position doesn’t matter, as our feature maps are robust to this!</p>
<p>Look at Figure 9, we go back to the edge detection Prewitt operator to showcase equivariance to translation. Taking Jim the cat, one image is normal and the other has been shifted by 128 pixels. The output of this shift is the image split and put back together down the middle. Now, look at the edge detection output, it’s shifted in the same way! This behaviour applies in CNN kernels too.</p>
<div id="093b2f2d" class="cell" data-execution_count="78">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>horizontal_edges <span class="op">=</span> box_blur(img_array, horizontal_edge_kernel)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>shifted_array <span class="op">=</span> np.roll(img_array, shift<span class="op">=</span><span class="dv">128</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>shifted_horizontal_edges <span class="op">=</span> box_blur(shifted_array, horizontal_edge_kernel)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a figure with a 2x2 grid</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">12</span>))</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Original image (top left)</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>plt.imshow(img_array, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Figure 9 - Original Image'</span>)</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Shifted image (top right)</span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>plt.imshow(shifted_array, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Figure 9 - Shifted Image'</span>)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Original edges (bottom left)</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>plt.imshow(horizontal_edges, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Figure 9 - Horizontal Edges'</span>)</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Shifted edges (bottom right)</span></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>plt.imshow(shifted_horizontal_edges, cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Figure 9 - Shifted Horizontal Edges'</span>)</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust layout to prevent overlap</span></span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the figure</span></span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="from_scratch_convolutional_layers_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="implementing-the-conv-operation" class="level3">
<h3 class="anchored" data-anchor-id="implementing-the-conv-operation">Implementing the Conv Operation</h3>
<p>We have already implemented the conv operation, earlier we wrote the box_blur function which implements the behaviour required. The only bit we need now is the differentiable part (which allows us to learn parameters). However the box_blur function is very slow. We used 3 for loops giving an approximate runtime of <span class="math inline">\(O(N^3)\)</span>, which is pretty abysmal if we were to add a batch dimension (processing more than one image at a time) we’d end up with <span class="math inline">\(O(N^4)\)</span>. Let’s see if we can speed this up.</p>
<p>There is a beautiful piece of equipment called a “GPU” which excels at doing fast matrix multiplications, luckily for us we don’t have to make one nor do we have to implement the low-level code to use one, what is required of us is to transform our slow conv operation into something which can be executed with matrix multiplications. This is a common theme in deep learning, in the back of your mind think “How can we best use the parallel processing capability of a GPU to speed this up” (most of the time someone will have already done it for you!).</p>
<hr>
<section id="an-aside---generalised-matrix-multiplication-gemm" class="level4">
<h4 class="anchored" data-anchor-id="an-aside---generalised-matrix-multiplication-gemm">An Aside - GEneralised Matrix Multiplication (GEMM)</h4>
<p>In the days of old (1979) a library was created, for the Fortran language, by the wizards we look back upon in awe. The library was called Basic Linear Algebra Subprograms (BLAS). The purpose was to speed up and have standard optimised implementations of common linear algebra operations such as vector addition, dot product, scalar multiplication, linear combinations and most importantly for us matrix multiplication<a href="#4"><sup>4</sup></a> The idea was to allow developers to focus on the higher level details of making useful applications with the operations, rather than spend huge amounts of time on low-level optimisations. It’s a classic division of labour. Another benefit, is by having a central team which focuses on the low-level details they can make the most advanced optimisations due to their expertise and create operations which can be used regardless of the hardware they are used on. The second point is important, without such libraries we would all have to spend lots of time programming the same things, then if a new piece of hardware comes out there’s no guarantee our code will be portable, not easily anyway.</p>
<p>GEneralised Matrix Multiplication (GEMM), arose from BLAS in 1990. It’s an extremely optimised matrix multiplication which we want to utilise. As mentioned, the goal of these libraries is to be portable for any hardware, unfortunately BLAS was created for the CPU. CPUs are great, but their ability to process larger parallel processes is outmatched by the GPU. GPUs see a lot of use in the deep learning space, the use of the GPU is what kicked off the deep learning revolution in 2012<a href="#5"><sup>5</sup></a>. GPUs are great at matrix multiplications (which is the most common operation in deep learning). Many people knew this and so did Nvidia, in 2014 Nvidia created a library for their GPUs to speed up deep learning operations in a similar manner to BLAS but they targeted GPUs. It was aptly named: cuDNN (Cuda Deep Neural Network), a series of primitive operations used in deep learning which are optimised for GPUs (its creation was inspired by the goal of BLAS, cuDNN fulfilling the same role for Nvidia GPUs). Funnily enough, CNNs are what sparked the creation of cuDNN.</p>
<hr>
<p>So, to get a faster conv we need to take our sliding window approach and get a matrix multiplication(mat mul). Think first about what our conv operation looks like at the moment: we have a kernel, which slides over the input image. Each <span class="math inline">\(k{\times}k\)</span> area which the kernel passes over is called a patch. We have many patches, and with these is where the conversion to a mat mul takes place. There’s another method named im2col (from matlab), which turns the sliding window patches into columns of a matrix. We flip this slightly, to take our patches and flatten them into rows and the kernel itself becomes a column (I found this to be more intuitive for me, it doesn’t matter much which way we flatten the patches to), let’s call my operation <b>im2row</b>.</p>
<p>Im2row flattens our image into a matrix with each patch in a row and the kernel as a column vector. If we were to have multiple kernels we would stack the flattened column vectors such that we get a matrix where each column is one flattened kernel. Then a matmul will get us the same result that our old conv would get! Bear with me here, the code to follow will add the meat to the bone, I think this process is wonderful albeit not easy to understand at first.</p>
</section>
<section id="a-small-example---im2row-and-matmuls" class="level4">
<h4 class="anchored" data-anchor-id="a-small-example---im2row-and-matmuls">A Small Example - Im2row and Matmuls</h4>
<p>We’ll go through a small example by hand to get our heads around the concept. Let’s generate a some data, our input being a 5x5 “image” (just some random integers from 0-255) and a 3x3 kernel.</p>
<p>A last bit of theory before we start, I want to outline how im2row transforms our conv to matrices. We know that we flatten patches into rows, but what will be the dimension of our image_matrix? Well, the number of rows will be the number of patches and the length of the column will be the length of any one given flattened patch. Therefore, <b>m = number of patches and n = length of a flattened patch</b>. For the kernel then, <b>n=length of a flattened patch and p = number of kernels</b>. We see then post a matmul the output shape given by <span class="math inline">\((m \times n)(n \times p) = (m \times p)\)</span>, will be the number of patches for rows and number of kernels for columns. These outputs will form matrices, with each column vector being one output of the feature maps.</p>
<p>With that in mind, lets get going!</p>
<div id="2f351920-d25c-4b72-a515-f120441aee57" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Lets make an example image (5x5) and a kernel (3x3) - no channels to keep it simple</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>image_no_channels <span class="op">=</span> np.random.randint(<span class="dv">0</span>,<span class="dv">256</span>, (<span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>kernel <span class="op">=</span> np.random.randint(<span class="dv">0</span>,<span class="dv">10</span>, (<span class="dv">3</span>,<span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="70b942fc-85d1-4b7a-a110-accde046215f" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>image_no_channels, kernel</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>(array([[102, 179,  92,  14, 106],
        [ 71, 188,  20, 102, 121],
        [210, 214,  74, 202,  87],
        [116,  99, 103, 151, 130],
        [149,  52,   1,  87, 235]]),
 array([[6, 3, 7],
        [4, 6, 9],
        [2, 6, 7]]))</code></pre>
</div>
</div>
<p>The patches for this data are <span class="math inline">\(3{\times}3\)</span>, or <span class="math inline">\(k \times k\)</span> with k being the dimension of our kernel (<span class="math inline">\(k=3\)</span> here). We can manually define an image_matrix by taking each patch and flattening it out into a row. I encourage you to get a pen and paper (or use your notebook) and do this part yourself. Remember a patch is the area the kernel slides over.</p>
<p>I’ll define the first two and leave the rest to you (note, we’ll use 0-based indexing):</p>
<p>Row 0 of image matrix, starting at 188 (position 1,1 in image_no_channels matrix, where this is the centre of the kernel) the patch would be:</p>
<p><span class="math display">\[
\begin{gather*}
\text{Patch 0}\\
\begin{bmatrix}
102 &amp; 179 &amp; 92 \\
71 &amp; 188 &amp; 20 \\
210 &amp; 214 &amp; 74\\
\end{bmatrix}
\xrightarrow{\text{flatten}}
\begin{bmatrix}
102 &amp; 179 &amp; 92 &amp; 71 &amp; 188 &amp; 20 &amp; 210 &amp; 214 &amp; 74
\end{bmatrix}
\end{gather*}
\]</span></p>
<p>Row 1 of image matrix, starting at 20 (position 1,2 in image_no_channels matrix) the patch would be:</p>
<p><span class="math display">\[
\begin{gather*}
\text{Patch 1}\\
\begin{bmatrix}
179 &amp; 92 &amp; 14\\
188 &amp; 20 &amp; 102\\
214 &amp; 74 &amp; 202\\
\end{bmatrix}
\xrightarrow{\text{flatten}}
\begin{bmatrix}
179 &amp; 92 &amp; 14 &amp; 188 &amp; 20 &amp; 102 &amp; 214 &amp; 74 &amp; 202
\end{bmatrix}
\end{gather*}
\]</span></p>
<p>And so on for the image_matrix. On the other hand, the kernel is flattened into a column vector (stored in a matrix even though there is only one for generality):</p>
<p><span class="math display">\[
\begin{gather*}
\text{Kernel}\\
\begin{bmatrix}
6 &amp; 3 &amp; 7\\
4 &amp; 6 &amp; 9\\
2 &amp; 6 &amp; 7\\
\end{bmatrix}
\xrightarrow{\text{flatten}}
\begin{bmatrix}
6\\
3\\
7\\
4\\
6\\
9\\
2\\
6\\
7\\
\end{bmatrix}
\end{gather*}
\]</span></p>
<p>That’s it, overall quite a simple process, but one with astonishing results.</p>
<p>Expand the following code block to see the full image_matrix.</p>
<div id="d161b2e8-2ec2-4be9-846c-bf0244e8cf69" class="cell" data-execution_count="84">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>image_matrix <span class="op">=</span> np.array([</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">102</span>, <span class="dv">179</span>, <span class="dv">92</span>, <span class="dv">71</span>, <span class="dv">188</span>, <span class="dv">20</span>, <span class="dv">210</span>, <span class="dv">214</span>, <span class="dv">74</span>],</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">179</span>, <span class="dv">92</span>, <span class="dv">14</span>, <span class="dv">188</span>, <span class="dv">20</span>, <span class="dv">102</span>, <span class="dv">214</span>, <span class="dv">74</span>, <span class="dv">202</span>],</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">92</span>, <span class="dv">14</span>, <span class="dv">106</span>, <span class="dv">20</span>, <span class="dv">102</span>, <span class="dv">121</span>, <span class="dv">74</span>, <span class="dv">202</span>, <span class="dv">87</span>],</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">71</span>,<span class="dv">188</span>, <span class="dv">20</span>, <span class="dv">210</span>, <span class="dv">214</span>, <span class="dv">74</span>, <span class="dv">116</span>, <span class="dv">99</span>, <span class="dv">103</span>],</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">188</span>, <span class="dv">20</span>, <span class="dv">102</span>, <span class="dv">214</span>, <span class="dv">74</span>, <span class="dv">202</span>, <span class="dv">99</span>, <span class="dv">103</span>, <span class="dv">151</span>],</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">20</span>, <span class="dv">102</span>, <span class="dv">121</span>, <span class="dv">74</span>, <span class="dv">202</span>, <span class="dv">87</span>, <span class="dv">102</span>, <span class="dv">151</span>, <span class="dv">130</span>],</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">210</span>, <span class="dv">214</span>, <span class="dv">74</span>, <span class="dv">116</span>, <span class="dv">99</span>, <span class="dv">103</span>, <span class="dv">149</span>, <span class="dv">52</span>, <span class="dv">1</span>],</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">214</span>, <span class="dv">74</span>, <span class="dv">202</span>, <span class="dv">99</span>, <span class="dv">103</span>, <span class="dv">151</span>, <span class="dv">52</span>, <span class="dv">1</span>, <span class="dv">87</span>],</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">74</span>, <span class="dv">202</span>, <span class="dv">87</span>, <span class="dv">103</span>, <span class="dv">151</span>, <span class="dv">130</span>, <span class="dv">1</span>, <span class="dv">87</span>, <span class="dv">235</span>],</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>kernel_matrix <span class="op">=</span> np.array([</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">6</span>],</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">3</span>],</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">7</span>],</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">4</span>],</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">6</span>],</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">9</span>],</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">2</span>],</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">6</span>],</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">7</span>],</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>image_matrix.shape, kernel_matrix.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="84">
<pre><code>((9, 9), (9, 1))</code></pre>
</div>
</div>
<p>The current shapes match the theory we discussed above, in a 5x5 image a 3x3 kernel fits 9 times and a 3x3 patch flattened into a row is 9 numbers. Also, we have 1 feature map which is 3x3, thus the 1x9 column vector.</p>
<section id="matrix-multiplication---a-recap" class="level5">
<h5 class="anchored" data-anchor-id="matrix-multiplication---a-recap">Matrix Multiplication - A Recap</h5>
<p>Now it’s time for the fun stuff, matmuls. Let’s refresh our memory of how matmuls work. This will tell us why the im2row works and why we can perform such an optimisation. The <a href="https://en.wikipedia.org/wiki/Matrix_multiplication">equation</a> for matmul is:</p>
<p><span class="math display">\[
c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + ... + a_{in}b_{nj} = \sum_{k=0}^{n}a_{ik}b_{kj}
\]</span></p>
<p>For us, <span class="math inline">\(\mathit{\mathbf{C}}\)</span> would be the output feature map/s, <span class="math inline">\(\mathit{\mathbf{A}}\)</span> is the input image and <span class="math inline">\(\mathit{\mathbf{B}}\)</span> the kernel. The matmul looks familiar to the dot product, because it is a dot product. Namely <span class="math inline">\(c_{ij}\)</span> is the dot product between <span class="math inline">\(i^{th}\)</span> row of a and <span class="math inline">\(j^{th}\)</span> column of b. Think about how we orientated the image_matrix and kernel_matrix, with our patches as the rows of <span class="math inline">\(\mathit{\mathbf{A}}\)</span> and kernel as the column of <span class="math inline">\(\mathit{\mathbf{B}}\)</span> we are doing the exact dot product we need, we have reduced our conv to a much more efficient matmul!</p>
<p>Using the formula above we can take our matrices to get the following (a useful way to picture this is we take the column of the kernel matrix and compute the dot product between each row, for multiple columns you’d just do this one column at a time):</p>
<div style="overflow-x: auto;">
<p><span class="math display">\[
\begin{align*}
c_{1,1} &amp;= (102 \cdot 6) + (179 \cdot 3) + (92 \cdot 7) + (71 \cdot 4) + (188 \cdot 6) + (20 \cdot 9) + (210 \cdot 2) + (214 \cdot 6) + (74 \cdot 7) = 5607\\
c_{2,1} &amp;= (179 \cdot 6) + (92 \cdot  3) + (14 \cdot 7) + (188 \cdot 4) + (20 \cdot 6) + (102 \cdot 9) + (214 \cdot 2) + (74 \cdot 6) + (202 \cdot 7) = 5524\\
c_{3,1} &amp;= (92 \cdot 6) + (14 \cdot  3) + (106 \cdot 7) + (20 \cdot 4) + (102 \cdot 6) + (121 \cdot 9) + (74 \cdot 2) + (202 \cdot 6) + (87 \cdot 7) = 5086\\
\vdots \\
c_{9,1} &amp;= (74 \cdot 6) + (202 \cdot 3) + (87 \cdot 7) + (103 \cdot 4) + (151 \cdot 6) + (130 \cdot 9) + (1 \cdot 2) + (87 \cdot 6) + (235 \cdot 7) = 6316
\end{align*}
\]</span></p>
</div>
<p>I’ll let you fill in the blanks. For <span class="math inline">\(c_{4,1} \xrightarrow{\;} c_{8,1}\)</span>. Remember, here we do exactly the same dot products we would have done with the kernel and its related patches.</p>
<p>After all is said and done we get the following matrix.</p>
<p><span class="math display">\[
\begin{bmatrix}
5607 \\
5542 \\
5086 \\
5467 \\
6893 \\
5586 \\
5022 \\
6012 \\
6316
\end{bmatrix}
\]</span></p>
<p>The last step of this process is to reshape the feature map:</p>
<p><span class="math display">\[
\begin{bmatrix}
5607 &amp; 5542 &amp; 5086 \\
5467 &amp; 6893 &amp; 5586 \\
5022 &amp; 6012 &amp; 6316
\end{bmatrix}
\]</span></p>
<div id="18c41418-5a91-44fe-ac37-47b8ea7cd30b" class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Here's the numpy matmul, just to check we did it right.</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>np.matmul(image_matrix, kernel_matrix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>array([[5607],
       [5524],
       [5086],
       [5467],
       [6893],
       [5584],
       [5022],
       [6012],
       [6316]])</code></pre>
</div>
</div>
<p>There we have it! The im2row algorithm in action, it’s truly wonderful! By transforming our input data and kernel just a little bit we get huge time savings. All that’s left to do is implement this in an algorithm which we can be used in a neural network forward pass.</p>
</section>
</section>
</section>
<section id="the-general-case---extension-to-rgb-3-channel-images" class="level3">
<h3 class="anchored" data-anchor-id="the-general-case---extension-to-rgb-3-channel-images">The General Case - Extension to RGB (3 channel) Images</h3>
<p>Typically in a CNN we work with 3 channel (RGB) images, not 1 channel (grayscale). In a 3 channel image instead of having one value at each point, we have 3. This is represented by a 3 layer matrix of <span class="math inline">\({H}\times{W}\)</span> of the image, so we now have <span class="math inline">\(H \times W \times D, \text{ where } D=3\)</span>. As mentioned, we can treat this in the same manner we handled the conv, because it isn’t true 3d. The depth of 3 represents RGB, each one R,G,B is called a channel. Essentially at each point in the image we have 3 pixel values and in the entire image we have 3 2d matrices. Our kernel will have the same depth as the input_data. There are two equivalent ways to apply the sliding window, one: apply the kernel individually to each 2d matrix and then sum the outputs, two: sum all 2d matrices across the depth and then apply one kernel. The idea is that regardless of the depth, the conv produces one value at each position.</p>
<p>For im2row, the only major change is how we flatten the input data. We flatten each 2d matrix and then append them to each other. So for an RGB image, we’d flatten R, G, B 2d matrices as before by taking our <span class="math inline">\(k \times k\)</span> and then we append first the G to the end of R and then B to the end of R and G. Look at Figure 10 to see this in action. Likewise for the kernel, which is now <span class="math inline">\(k \times k \times D\)</span>, flatten each <span class="math inline">\(k \times k\)</span> matrix and then append the vectors together. Then, the rest is the same as we do the matmul, providing us <span class="math inline">\(n\)</span> number of columns of output, which we can reshape into our feature maps. Note, we do not reshape them into having a depth <span class="math inline">\(D\)</span>, because the kernel operation produces one value at every point each output filter map has shape <span class="math inline">\(H \times W\)</span> without the <span class="math inline">\(D\)</span>. The depth of the output is decided by the number of kernels we apply.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="from_scratch_convolutional_layers_files/figure-html/326a3124-f18c-4bee-b2cd-904950558e1a-1-image-3.png" class="img-fluid figure-img"></p>
<figcaption>Figure 10 - A depiction of im2row. Source: Drawn by me, adapted from the Nvidia cuDNN paper.</figcaption>
</figure>
</div>
<p>Picking up our old example, now with a <span class="math inline">\(5 \times 5 \times 3\)</span> input image and a <span class="math inline">\(3 \times 3 \times 3\)</span> kernel. Recall we want the shapes <span class="math inline">\(m \times n\)</span> and <span class="math inline">\(n \times p\)</span>.</p>
<p>Where:</p>
<p><span class="math display">\[m=\text{number of patches which fit in the input image}\]</span></p>
<p><span class="math display">\[n=\text{length of a flattened patch}\]</span></p>
<p><span class="math display">\[p=\text{the number of kernels we want to apply}\]</span></p>
<p>The number of patches doesn’t change, in a <span class="math inline">\(5 \times 5 \times 3\)</span> image we can apply a <span class="math inline">\(3 \times 3 \times 3\)</span> kernel, 9 times. To calculate <span class="math inline">\(m\)</span> I prefer to look at the <span class="math inline">\(H \times W\)</span> not the <span class="math inline">\(D\)</span> value, the <span class="math inline">\(D\)</span> doesn’t play too big of a part here. So: <span class="math display">\[
m=9, n=27, p=1
\]</span> The size of the input matrix will be <span class="math inline">\(9 \times 27\)</span> and the kernel matrix will be <span class="math inline">\(27 \times 1\)</span>. The only difference here is <span class="math inline">\(n\)</span>, we calculate this by flattening the <span class="math inline">\(H \times W\)</span> and then multiplying by <span class="math inline">\(D\)</span>. Essentially, we flatten each <span class="math inline">\(H \times W\)</span> in all channels, ending up with <span class="math inline">\(D\)</span> of them. Then post matmul we just need to reshape, before reshaping we’d have a <span class="math inline">\(9 \times 1\)</span> matrix. After reshaping we get our <span class="math inline">\(3 \times 3 \times 1\)</span> feature map output.</p>
<p>Up to now, we’ve worked with 1 kernel at a time in return giving only 1 output feature map. This isn’t common in CNNs, in CNNs we usually map to a larger number of feature maps, say 16. To do this, we would just create 16 individual kernels and apply them. Doing so with the above example would give the following: <span class="math display">\[
m=9, n=27, p=16
\]</span> The output on im2row has the shape <span class="math inline">\(9 \times 16\)</span>, <span class="math inline">\(16\)</span> columns which each represent 1 feature map. When we reshape, we reshape each column into a <span class="math inline">\(3 \times 3\)</span> matrix, finally we’d have a 16 channel matrix <span class="math inline">\(3 \times 3 \times 16\)</span> ready for the next layer!</p>
</section>
<section id="implenting-im2row" class="level3">
<h3 class="anchored" data-anchor-id="implenting-im2row">Implenting Im2row</h3>
<p>All that remains is to implement im2row! This function will work for any general 2d image with any number of channels.</p>
<p>There are 3 pieces of the im2row puzzle:</p>
<pre><code>1 - Flattening the input patches
2 - Doing the matmul
3 - Reshaping matmul output</code></pre>
<p>Let’s get to it, right after a small aside.</p>
<hr>
<section id="an-aside---how-3-channel-images-are-represented-in-data" class="level5">
<h5 class="anchored" data-anchor-id="an-aside---how-3-channel-images-are-represented-in-data">An Aside - How 3 Channel Images are Represented in Data</h5>
<p>Below you see the structure of a 3 channel image and a 3 channel kernel in numpy, it’ll be the same in PyTorch. Each matrix represents one channel, the first 5x5 is red, the second is green and the last is blue. It’s essentially 3 stacked matrices one representing each channel.</p>
<div id="c372cea8-8def-4c82-9abd-5d553d3e7a14" class="cell" data-execution_count="34">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="co"># We do not add a batch dimension here. </span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>img_test <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">10</span>, (<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>kernel <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">5</span>, (<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>C, H, W <span class="op">=</span> img_test.shape</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"5x5 grid where each position shows [R,G,B] values:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(C):</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(img_test[c])</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>kernel_c, k, k <span class="op">=</span> kernel.shape</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"3x3 grid where each position shows [R,G,B] values:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k_c <span class="kw">in</span> <span class="bu">range</span>(kernel_c):</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(kernel[k_c])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>5x5 grid where each position shows [R,G,B] values:

[[6 3 7 4 6]
 [9 2 6 7 4]
 [3 7 7 2 5]
 [4 1 7 5 1]
 [4 0 9 5 8]]
[[0 9 2 6 3]
 [8 2 4 2 6]
 [4 8 6 1 3]
 [8 1 9 8 9]
 [4 1 3 6 7]]
[[2 0 3 1 7]
 [3 1 5 5 9]
 [3 5 1 9 1]
 [9 3 7 6 8]
 [7 4 1 4 7]]
3x3 grid where each position shows [R,G,B] values:

[[3 4 2]
 [4 4 1]
 [2 2 2]]
[[4 3 2]
 [4 1 3]
 [1 3 4]]
[[0 3 1]
 [4 3 0]
 [0 2 2]]</code></pre>
</div>
</div>
<p>At each point in the 5x5 matrix we have an array of length 3. This array is the colour channels RGB, so instead of having 3 stacked matrices we have a 5x5 matrix where each element of the matrix is the channel! The kernel is the same, which ensures our RGB matmul is aligned, for both image and kernel the output matrix from im2row will be of the format: [<span style="color:red">R</span><span style="color:red">R</span><span style="color:red">R</span><span style="color:red">R</span><em><span style="color:green">G</span><span style="color:green">G</span><span style="color:green">G</span><span style="color:green">G</span></em><span style="color:blue">B</span><span style="color:blue">B</span><span style="color:blue">B</span><span style="color:blue">B</span>…]. This keeps the matmul aligned nicely.</p>
<hr>
</section>
<section id="flattening-the-patches" class="level4">
<h4 class="anchored" data-anchor-id="flattening-the-patches">1 - Flattening the patches</h4>
<p>Flattening the image requires calculating the number of patches which will fit into our image. To do so we can use the following two equations:</p>
<p><span class="math display">\[
\begin{align*}
\text{columns: }\frac{(W-K+2P)}{S}+1,\\[1em]
\text{rows: }\frac{(H-K+2P)}{S}+1
\end{align*}
\]</span> Where:</p>
<pre><code>- W = Width
- H = Height
- K = Kernel dimension
- P = Padding (We assume 0 padding in all our examples/code for simplicity)
- S = Stride (We set stride = 1, meaning we move the kernel across one 1 pixel at a time. This is sometimes set to higher numbers)</code></pre>
<p>Then multiply <span class="math inline">\(H\)</span> and <span class="math inline">\(W\)</span> to get the total number of patches.</p>
<div id="0f81883b-67a9-4f30-8d97-51d52048fbbd" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> im2row(images, kernels, padding<span class="op">=</span><span class="dv">0</span>, stride<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1 - Flattening the image and kernels</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the formula to calculate number of patches</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    C_img, H, W <span class="op">=</span> images.shape</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    N, C_kernel, k, k <span class="op">=</span> kernels.shape</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> C_img <span class="op">!=</span> C_kernel:</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">Exception</span>(<span class="st">"Your kernel and image channels are mismatched"</span>)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>        C <span class="op">=</span> C_img</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># does int casting have any unintended consequences?</span></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>    height_kernel_fit <span class="op">=</span> <span class="bu">int</span>(((H<span class="op">-</span>k <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>padding)<span class="op">/</span>stride)<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>    width_kernel_fit <span class="op">=</span> <span class="bu">int</span>(((W<span class="op">-</span>k <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>padding)<span class="op">/</span>stride)<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>    n_patches <span class="op">=</span> height_kernel_fit <span class="op">*</span> width_kernel_fit</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>    patch_size <span class="op">=</span> k <span class="op">*</span> k <span class="op">*</span> C</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pre-allocate array which will contain our patches.</span></span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>    img_mat <span class="op">=</span> np.zeros((n_patches, patch_size))</span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the start position of our operation, from there we pull out the kernel</span></span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We use numpy .flatten() to return the flattened image - try implement the flatten function yourself :)</span></span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(height_kernel_fit):</span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(width_kernel_fit):</span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a>            patch <span class="op">=</span> images[:, i:i<span class="op">+</span>k, j:j<span class="op">+</span>k]</span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a>            patch <span class="op">=</span> patch.flatten()</span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a>            img_mat[idx] <span class="op">=</span> patch.flatten()</span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a>            idx <span class="op">+=</span> stride</span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;Flip the dimensions such that the flatten just goes into it, then transpose at end of everything</span></span>
<span id="cb47-34"><a href="#cb47-34" aria-hidden="true" tabindex="-1"></a>    kernel_mat <span class="op">=</span> np.zeros((N, patch_size))</span>
<span id="cb47-35"><a href="#cb47-35" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb47-36"><a href="#cb47-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb47-37"><a href="#cb47-37" aria-hidden="true" tabindex="-1"></a>        flattened_kernel <span class="op">=</span> kernels[i].flatten()</span>
<span id="cb47-38"><a href="#cb47-38" aria-hidden="true" tabindex="-1"></a>        kernel_mat[idx] <span class="op">=</span> flattened_kernel.T</span>
<span id="cb47-39"><a href="#cb47-39" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb47-40"><a href="#cb47-40" aria-hidden="true" tabindex="-1"></a>    kernel_mat <span class="op">=</span> kernel_mat.T</span>
<span id="cb47-41"><a href="#cb47-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-42"><a href="#cb47-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Added for testing</span></span>
<span id="cb47-43"><a href="#cb47-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> img_mat, kernel_mat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="40516759-4f09-4609-8aa1-73cb5cd61dd6" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>img_test <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">10</span>, (<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>kernel_test <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">5</span>, (<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>img_matrix, kernel_matrix <span class="op">=</span> im2row(img_test, kernel_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="6405e888-445b-4ef2-a0cc-bd6302a673c4" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>img_matrix<span class="op">@</span>kernel_matrix</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>array([[287.],
       [267.],
       [287.],
       [308.],
       [322.],
       [341.],
       [306.],
       [296.],
       [381.]])</code></pre>
</div>
</div>
</section>
<section id="the-matmul" class="level4">
<h4 class="anchored" data-anchor-id="the-matmul">2 - The matmul</h4>
<p>This one is simple, in numpy the “@” symbol represents a matrix multiplication. We do the matmul between our image matrix and the kernels matrix as described before.</p>
<div id="0c075cb6-ab7d-4e26-8102-77b8459e90ed" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> im2row(images, kernels, padding<span class="op">=</span><span class="dv">0</span>, stride<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1 - Flattening the image and kernels</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the formula to calculate number of patches</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    C_img, H, W <span class="op">=</span> images.shape</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    N, C_kernel, k, k <span class="op">=</span> kernels.shape</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> C_img <span class="op">!=</span> C_kernel:</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">Exception</span>(<span class="st">"Your kernel and image channels are mismatched"</span>)</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>        C <span class="op">=</span> C_img</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Does int casting have any unintended consequences?</span></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>    height_kernel_fit <span class="op">=</span> <span class="bu">int</span>(((H<span class="op">-</span>k <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>padding)<span class="op">/</span>stride)<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>    width_kernel_fit <span class="op">=</span> <span class="bu">int</span>(((W<span class="op">-</span>k <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>padding)<span class="op">/</span>stride)<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>    n_patches <span class="op">=</span> height_kernel_fit <span class="op">*</span> width_kernel_fit</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>    patch_size <span class="op">=</span> k <span class="op">*</span> k <span class="op">*</span> C</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pre-allocate array</span></span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>    img_mat <span class="op">=</span> np.zeros((n_patches, patch_size))</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the start position of our operation, from there we pull out the kernel</span></span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(height_kernel_fit):</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(width_kernel_fit):</span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a>            patch <span class="op">=</span> images[:, i:i<span class="op">+</span>k, j:j<span class="op">+</span>k]</span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a>            patch <span class="op">=</span> patch.flatten()</span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a>            img_mat[idx] <span class="op">=</span> patch.flatten()</span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a>            idx <span class="op">+=</span> stride</span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;Flip the dimensions such that the flatten just goes into it, then transpose at end of everything</span></span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a>    kernel_mat <span class="op">=</span> np.zeros((N, patch_size))</span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a>        flattened_kernel <span class="op">=</span> kernels[i].flatten()</span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a>        kernel_mat[idx] <span class="op">=</span> flattened_kernel.T</span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true" tabindex="-1"></a>    kernel_mat <span class="op">=</span> kernel_mat.T</span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2 - The matmul!</span></span>
<span id="cb51-42"><a href="#cb51-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Part 2 is very simple, a single line change.</span></span>
<span id="cb51-43"><a href="#cb51-43" aria-hidden="true" tabindex="-1"></a>    flattened_feature_map <span class="op">=</span> img_mat<span class="op">@</span>kernel_mat</span>
<span id="cb51-44"><a href="#cb51-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-45"><a href="#cb51-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> flattened_feature_map</span>
<span id="cb51-46"><a href="#cb51-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-47"><a href="#cb51-47" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb51-48"><a href="#cb51-48" aria-hidden="true" tabindex="-1"></a>img_test <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">10</span>, (<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb51-49"><a href="#cb51-49" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb51-50"><a href="#cb51-50" aria-hidden="true" tabindex="-1"></a>kernel_test <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">5</span>, (<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb51-51"><a href="#cb51-51" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> im2row(img_test, kernel_test)</span>
<span id="cb51-52"><a href="#cb51-52" aria-hidden="true" tabindex="-1"></a>output.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>(9, 1)</code></pre>
</div>
</div>
<p>In that output, the row values are the points in our feature map and 1 is the number of feature maps. 1 feature map as we have 1 input kernel. To finish up let’s add the feature map reshaping.</p>
</section>
<section id="reshaping-the-matmul-output" class="level4">
<h4 class="anchored" data-anchor-id="reshaping-the-matmul-output">3 - Reshaping The Matmul Output</h4>
<p>Another simple addition, we can make use of the numpy reshape() to wrap up our im2row implementation.</p>
<div id="f5569864-89a6-44f0-9171-25b610a33603" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> im2row(images, kernels, padding<span class="op">=</span><span class="dv">0</span>, stride<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1 - Flattening the image and kernels</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the formula to calculate number of patches</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    C_img, H, W <span class="op">=</span> images.shape</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    N, C_kernel, k, k <span class="op">=</span> kernels.shape</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> C_img <span class="op">!=</span> C_kernel:</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">Exception</span>(<span class="st">"Your kernel and image channels are mismatched"</span>)</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>        C <span class="op">=</span> C_img</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># does int casting have any unintended consequences?</span></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>    height_kernel_fit <span class="op">=</span> <span class="bu">int</span>(((H<span class="op">-</span>k <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>padding)<span class="op">/</span>stride)<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>    width_kernel_fit <span class="op">=</span> <span class="bu">int</span>(((W<span class="op">-</span>k <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>padding)<span class="op">/</span>stride)<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>    n_patches <span class="op">=</span> height_kernel_fit <span class="op">*</span> width_kernel_fit</span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>    patch_size <span class="op">=</span> k <span class="op">*</span> k <span class="op">*</span> C</span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pre-allocate array</span></span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a>    img_mat <span class="op">=</span> np.zeros((n_patches, patch_size))</span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the start position of our operation, from there we pull out the kernel</span></span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(height_kernel_fit):</span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(width_kernel_fit):</span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>            patch <span class="op">=</span> images[:, i:i<span class="op">+</span>k, j:j<span class="op">+</span>k]</span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>            patch <span class="op">=</span> patch.flatten()</span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>            img_mat[idx] <span class="op">=</span> patch.flatten()</span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>            idx <span class="op">+=</span> stride</span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;Flip the dimensions such that the flatten just goes into it, then transpose at end of everything</span></span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>    kernel_mat <span class="op">=</span> np.zeros((N, patch_size))</span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a>        flattened_kernel <span class="op">=</span> kernels[i].flatten()</span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a>        kernel_mat[idx] <span class="op">=</span> flattened_kernel.T</span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb53-39"><a href="#cb53-39" aria-hidden="true" tabindex="-1"></a>    kernel_mat <span class="op">=</span> kernel_mat.T</span>
<span id="cb53-40"><a href="#cb53-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-41"><a href="#cb53-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2 - The matmul!</span></span>
<span id="cb53-42"><a href="#cb53-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Part 2 is very simple, a single line change.</span></span>
<span id="cb53-43"><a href="#cb53-43" aria-hidden="true" tabindex="-1"></a>    flattened_feature_map <span class="op">=</span> img_mat<span class="op">@</span>kernel_mat</span>
<span id="cb53-44"><a href="#cb53-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-45"><a href="#cb53-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(flattened_feature_map.shape)</span>
<span id="cb53-46"><a href="#cb53-46" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(flattened_feature_map)</span>
<span id="cb53-47"><a href="#cb53-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb53-48"><a href="#cb53-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3 - The Reshape</span></span>
<span id="cb53-49"><a href="#cb53-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We need to reshape each feature map</span></span>
<span id="cb53-50"><a href="#cb53-50" aria-hidden="true" tabindex="-1"></a>    feature_map <span class="op">=</span> flattened_feature_map.T</span>
<span id="cb53-51"><a href="#cb53-51" aria-hidden="true" tabindex="-1"></a>    feature_map <span class="op">=</span> feature_map.reshape(N,k,k)</span>
<span id="cb53-52"><a href="#cb53-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-53"><a href="#cb53-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> feature_map</span>
<span id="cb53-54"><a href="#cb53-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-55"><a href="#cb53-55" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb53-56"><a href="#cb53-56" aria-hidden="true" tabindex="-1"></a>img_test <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">10</span>, (<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb53-57"><a href="#cb53-57" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb53-58"><a href="#cb53-58" aria-hidden="true" tabindex="-1"></a>kernel_test <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">5</span>, (<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb53-59"><a href="#cb53-59" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> im2row(img_test, kernel_test)</span>
<span id="cb53-60"><a href="#cb53-60" aria-hidden="true" tabindex="-1"></a>output.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(9, 1)
[[287.]
 [267.]
 [287.]
 [308.]
 [322.]
 [341.]
 [306.]
 [296.]
 [381.]]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>(1, 3, 3)</code></pre>
</div>
</div>
<div id="27d5fe71-4cf4-4692-8ded-88236ac93e08" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>array([[[287., 267., 287.],
        [308., 322., 341.],
        [306., 296., 381.]]])</code></pre>
</div>
</div>
<p>There we have it! The full im2row function for one processing one image at a time.</p>
</section>
</section>
<section id="batched-im2row-operation" class="level3">
<h3 class="anchored" data-anchor-id="batched-im2row-operation">Batched Im2row Operation</h3>
<p>Now we have an im2row function, we want to take this and use it to implement the forward pass in a convolutional layer. However, typically the input to a neural network has the following shape (B,C,H,W), where B = batch_size. To speed up processing we often pass many images (batches) through the network at the same time, our current im2row won’t work with batched input, let’s fix that.</p>
<p>The changes are quite small but have a big impact. Check the code comments in the im2row_batched function to give you a walkthrough.</p>
<div id="7af0997b-f719-4f78-836b-782b1df914ac" class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> im2row_batched(images, kernels, padding<span class="op">=</span><span class="dv">0</span>, stride<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1 - Flattening the image and kernels</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the formula to calculate number of patches</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract B from images, B is the batch size</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    B, C_img, H, W <span class="op">=</span> images.shape</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    N, C_kernel, k, k <span class="op">=</span> kernels.shape</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> C_img <span class="op">!=</span> C_kernel:</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">Exception</span>(<span class="st">"Your kernel and image channels are mismatched"</span>)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>        C <span class="op">=</span> C_img</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># does int casting have any unintended consequences?</span></span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>    height_kernel_fit <span class="op">=</span> <span class="bu">int</span>(((H<span class="op">-</span>k <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>padding)<span class="op">/</span>stride)<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>    width_kernel_fit <span class="op">=</span> <span class="bu">int</span>(((W<span class="op">-</span>k <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>padding)<span class="op">/</span>stride)<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a>    n_patches <span class="op">=</span> height_kernel_fit <span class="op">*</span> width_kernel_fit</span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a>    patch_size <span class="op">=</span> k <span class="op">*</span> k <span class="op">*</span> C</span>
<span id="cb58-20"><a href="#cb58-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-21"><a href="#cb58-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pre-allocate array</span></span>
<span id="cb58-22"><a href="#cb58-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;Each individual image has n_patches and we have B images, thus the multiplication</span></span>
<span id="cb58-23"><a href="#cb58-23" aria-hidden="true" tabindex="-1"></a>    img_mat <span class="op">=</span> np.zeros((B <span class="op">*</span> n_patches, patch_size))</span>
<span id="cb58-24"><a href="#cb58-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(img_mat.shape)</span>
<span id="cb58-25"><a href="#cb58-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-26"><a href="#cb58-26" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb58-27"><a href="#cb58-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;Add another loop to handle batches. </span></span>
<span id="cb58-28"><a href="#cb58-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb58-29"><a href="#cb58-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(height_kernel_fit):</span>
<span id="cb58-30"><a href="#cb58-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(width_kernel_fit):</span>
<span id="cb58-31"><a href="#cb58-31" aria-hidden="true" tabindex="-1"></a>                <span class="co"># We now index each image in the batch. This ensures our kernel doesn't interact at boundaries across two images.</span></span>
<span id="cb58-32"><a href="#cb58-32" aria-hidden="true" tabindex="-1"></a>                patch <span class="op">=</span> images[b, :, i:i<span class="op">+</span>k, j:j<span class="op">+</span>k]</span>
<span id="cb58-33"><a href="#cb58-33" aria-hidden="true" tabindex="-1"></a>                patch <span class="op">=</span> patch.flatten()</span>
<span id="cb58-34"><a href="#cb58-34" aria-hidden="true" tabindex="-1"></a>                img_mat[idx] <span class="op">=</span> patch.flatten()</span>
<span id="cb58-35"><a href="#cb58-35" aria-hidden="true" tabindex="-1"></a>                idx <span class="op">+=</span> stride</span>
<span id="cb58-36"><a href="#cb58-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-37"><a href="#cb58-37" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;Flip the dimensions such that the flatten just goes into it, then transpose at end of everything</span></span>
<span id="cb58-38"><a href="#cb58-38" aria-hidden="true" tabindex="-1"></a>    kernel_mat <span class="op">=</span> np.zeros((N, patch_size))</span>
<span id="cb58-39"><a href="#cb58-39" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb58-40"><a href="#cb58-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb58-41"><a href="#cb58-41" aria-hidden="true" tabindex="-1"></a>        flattened_kernel <span class="op">=</span> kernels[i].flatten()</span>
<span id="cb58-42"><a href="#cb58-42" aria-hidden="true" tabindex="-1"></a>        kernel_mat[idx] <span class="op">=</span> flattened_kernel.T</span>
<span id="cb58-43"><a href="#cb58-43" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb58-44"><a href="#cb58-44" aria-hidden="true" tabindex="-1"></a>    kernel_mat <span class="op">=</span> kernel_mat.T</span>
<span id="cb58-45"><a href="#cb58-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-46"><a href="#cb58-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2 - The matmul!</span></span>
<span id="cb58-47"><a href="#cb58-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Part 2 is very simple, a single line change.</span></span>
<span id="cb58-48"><a href="#cb58-48" aria-hidden="true" tabindex="-1"></a>    flattened_feature_map <span class="op">=</span> img_mat<span class="op">@</span>kernel_mat</span>
<span id="cb58-49"><a href="#cb58-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb58-50"><a href="#cb58-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3 - The Reshape</span></span>
<span id="cb58-51"><a href="#cb58-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We need to reshape each feature map</span></span>
<span id="cb58-52"><a href="#cb58-52" aria-hidden="true" tabindex="-1"></a>    feature_map <span class="op">=</span> flattened_feature_map.T</span>
<span id="cb58-53"><a href="#cb58-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape our images back to the same format they came in in.</span></span>
<span id="cb58-54"><a href="#cb58-54" aria-hidden="true" tabindex="-1"></a>    feature_map <span class="op">=</span> feature_map.reshape(B, N, k, k)</span>
<span id="cb58-55"><a href="#cb58-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-56"><a href="#cb58-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> feature_map</span>
<span id="cb58-57"><a href="#cb58-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-58"><a href="#cb58-58" aria-hidden="true" tabindex="-1"></a><span class="co"># We pass in a batch of 2 sample images. Hence the output will have 18 rows</span></span>
<span id="cb58-59"><a href="#cb58-59" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;9 for image 1 and 9 for image 2</span></span>
<span id="cb58-60"><a href="#cb58-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-61"><a href="#cb58-61" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb58-62"><a href="#cb58-62" aria-hidden="true" tabindex="-1"></a>img_test_batch <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">10</span>, (<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb58-63"><a href="#cb58-63" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb58-64"><a href="#cb58-64" aria-hidden="true" tabindex="-1"></a>kernel_test_batch <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">5</span>, (<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb58-65"><a href="#cb58-65" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> im2row_batched(img_test_batch, kernel_test_batch)</span>
<span id="cb58-66"><a href="#cb58-66" aria-hidden="true" tabindex="-1"></a>output.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(18, 27)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="89">
<pre><code>(2, 1, 3, 3)</code></pre>
</div>
</div>
<div id="efcb7777-1fb5-43fa-9c87-726cd1ea31f9" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>array([[[[287., 267., 287.],
         [308., 322., 341.],
         [306., 296., 381.]]],


       [[[300., 282., 261.],
         [325., 284., 283.],
         [302., 299., 318.]]]])</code></pre>
</div>
</div>
</section>
<section id="a-convolutional-layer" class="level3">
<h3 class="anchored" data-anchor-id="a-convolutional-layer">A Convolutional Layer</h3>
<p>It’s time for the crescendo of our journey - utilising our im2row_batched operation to implement a full convolutional layer! By leveraging PyTorch’s framework, we can focus solely on defining the forward pass with our custom function while PyTorch handles the backward pass<sup>6</sup> calculations.</p>
<p>The PyTorch nn.Module is the foundation of nearly all deep learning models in this framework. This powerful base class allows us to define our custom forward pass using the im2row_batched function we’ve built, while automagically managing gradient calculations during training. Let’s see the fruits of labour come to life!</p>
<div id="07a5675a-db2f-43d8-871f-865432bf0806" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomConv2d(nn.Module):</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, kernel_size, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store the parameters of our conv</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.in_channels <span class="op">=</span> in_channels</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_channels <span class="op">=</span> out_channels</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.kernel_size <span class="op">=</span> kernel_size <span class="cf">if</span> <span class="bu">isinstance</span>(kernel_size, <span class="bu">tuple</span>) <span class="cf">else</span> (kernel_size, kernel_size)</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stride <span class="op">=</span> stride</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.padding <span class="op">=</span> padding</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The kernel is the main parameter of this conv. We need to manage them inside the layer rather</span></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># than providing them seperately as we did before.</span></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights (kernels) using PyTorch's parameter system</span></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape: (out_channels, in_channels, kernel_height, kernel_width)</span></span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Seems that in ipynb we need the manual_seed after every random code piece to maintain the </span></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># determinism</span></span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">#torch.manual_seed(42)</span></span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(</span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>            torch.randn(out_channels, in_channels, <span class="op">*</span><span class="va">self</span>.kernel_size)</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;x is the input to our conv layer</span></span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x shape: (batch_size, in_channels, height, width)</span></span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a>        B, C, H, W <span class="op">=</span> x.shape</span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a>        k_h, k_w <span class="op">=</span> <span class="va">self</span>.kernel_size</span>
<span id="cb63-29"><a href="#cb63-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-30"><a href="#cb63-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate output dimensions</span></span>
<span id="cb63-31"><a href="#cb63-31" aria-hidden="true" tabindex="-1"></a>        height_kernel_fit <span class="op">=</span> <span class="bu">int</span>(((H <span class="op">-</span> k_h <span class="op">+</span> <span class="dv">2</span><span class="op">*</span><span class="va">self</span>.padding)<span class="op">/</span><span class="va">self</span>.stride) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb63-32"><a href="#cb63-32" aria-hidden="true" tabindex="-1"></a>        width_kernel_fit <span class="op">=</span> <span class="bu">int</span>(((W <span class="op">-</span> k_w <span class="op">+</span> <span class="dv">2</span><span class="op">*</span><span class="va">self</span>.padding)<span class="op">/</span><span class="va">self</span>.stride) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb63-33"><a href="#cb63-33" aria-hidden="true" tabindex="-1"></a>        n_patches <span class="op">=</span> height_kernel_fit <span class="op">*</span> width_kernel_fit</span>
<span id="cb63-34"><a href="#cb63-34" aria-hidden="true" tabindex="-1"></a>        patch_size <span class="op">=</span> k_h <span class="op">*</span> k_w <span class="op">*</span> C</span>
<span id="cb63-35"><a href="#cb63-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-36"><a href="#cb63-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre-allocate array</span></span>
<span id="cb63-37"><a href="#cb63-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;Each individual image has n_patches and we have B images, thus the multiplication</span></span>
<span id="cb63-38"><a href="#cb63-38" aria-hidden="true" tabindex="-1"></a>        img_mat <span class="op">=</span> torch.zeros((B <span class="op">*</span> n_patches, patch_size), device<span class="op">=</span>x.device)</span>
<span id="cb63-39"><a href="#cb63-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-40"><a href="#cb63-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. </span><span class="al">TODO</span><span class="co">: Add padding - I'll leave this one up to you</span></span>
<span id="cb63-41"><a href="#cb63-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-42"><a href="#cb63-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Extract and flatten patches</span></span>
<span id="cb63-43"><a href="#cb63-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Earlier we noted that the im2row is more efficient, here we have 3 loops instead of 4 which is better but hardly efficient.</span></span>
<span id="cb63-44"><a href="#cb63-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We can optimise the following loop by making use of vectorised operations, however we'll skip it for this blog.</span></span>
<span id="cb63-45"><a href="#cb63-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># I encourage you to try and optimise this operation, as you'll see later in it's current state it is dreadfully slow.</span></span>
<span id="cb63-46"><a href="#cb63-46" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb63-47"><a href="#cb63-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb63-48"><a href="#cb63-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(height_kernel_fit):</span>
<span id="cb63-49"><a href="#cb63-49" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(width_kernel_fit):</span>
<span id="cb63-50"><a href="#cb63-50" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># We now index each image in the batch. This ensures our kernel doesn't interact at boundaries across two images.</span></span>
<span id="cb63-51"><a href="#cb63-51" aria-hidden="true" tabindex="-1"></a>                    patch <span class="op">=</span> x[b, :, i:i<span class="op">+</span>k_h, j:j<span class="op">+</span>k_w]</span>
<span id="cb63-52"><a href="#cb63-52" aria-hidden="true" tabindex="-1"></a>                    patch <span class="op">=</span> patch.flatten()</span>
<span id="cb63-53"><a href="#cb63-53" aria-hidden="true" tabindex="-1"></a>                    img_mat[idx] <span class="op">=</span> patch.flatten()</span>
<span id="cb63-54"><a href="#cb63-54" aria-hidden="true" tabindex="-1"></a>                    idx <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb63-55"><a href="#cb63-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-56"><a href="#cb63-56" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;Flip the dimensions such that the flatten just goes into it, then transpose at end of everything</span></span>
<span id="cb63-57"><a href="#cb63-57" aria-hidden="true" tabindex="-1"></a>        kernel_mat <span class="op">=</span> torch.zeros((<span class="va">self</span>.out_channels, patch_size), device<span class="op">=</span>x.device)</span>
<span id="cb63-58"><a href="#cb63-58" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb63-59"><a href="#cb63-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.out_channels):</span>
<span id="cb63-60"><a href="#cb63-60" aria-hidden="true" tabindex="-1"></a>            flattened_kernel <span class="op">=</span> <span class="va">self</span>.weight[i].flatten()</span>
<span id="cb63-61"><a href="#cb63-61" aria-hidden="true" tabindex="-1"></a>            kernel_mat[idx] <span class="op">=</span> flattened_kernel.T</span>
<span id="cb63-62"><a href="#cb63-62" aria-hidden="true" tabindex="-1"></a>            idx <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb63-63"><a href="#cb63-63" aria-hidden="true" tabindex="-1"></a>        kernel_mat <span class="op">=</span> kernel_mat.T</span>
<span id="cb63-64"><a href="#cb63-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-65"><a href="#cb63-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Perform matrix multiplication</span></span>
<span id="cb63-66"><a href="#cb63-66" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-67"><a href="#cb63-67" aria-hidden="true" tabindex="-1"></a>        flattened_feature_map <span class="op">=</span> img_mat<span class="op">@</span>kernel_mat</span>
<span id="cb63-68"><a href="#cb63-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-69"><a href="#cb63-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Reshape to proper output format</span></span>
<span id="cb63-70"><a href="#cb63-70" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-71"><a href="#cb63-71" aria-hidden="true" tabindex="-1"></a>        feature_map <span class="op">=</span> flattened_feature_map.T        </span>
<span id="cb63-72"><a href="#cb63-72" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> feature_map.reshape(<span class="va">self</span>.out_channels, B, height_kernel_fit, width_kernel_fit)</span>
<span id="cb63-73"><a href="#cb63-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Permute to get (batch_size, out_channels, height, width)</span></span>
<span id="cb63-74"><a href="#cb63-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># permute reorders the dimensions based on the order we provide it</span></span>
<span id="cb63-75"><a href="#cb63-75" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.permute(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb63-76"><a href="#cb63-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb63-77"><a href="#cb63-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="comparing-the-performance-of-the-customconv2d-with-the-pytorch-conv2d" class="level4">
<h4 class="anchored" data-anchor-id="comparing-the-performance-of-the-customconv2d-with-the-pytorch-conv2d">Comparing the Performance of the CustomConv2d with the PyTorch Conv2d</h4>
<p>Expand the code blocks to check out the comparison code. Essentially we are checking if given both layers having the same weights are the outputs similar. They won’t be equal due to floating point operations, so they just have to be close enough (roughly a difference less than 0.0001ish).</p>
<div id="d9713acc" class="cell" data-execution_count="50">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>in_channels <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>out_channels <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>height <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>kernel_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Input image</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(batch_size, in_channels, height, width)</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize both convolution layers</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>pytorch_conv <span class="op">=</span> nn.Conv2d(in_channels, out_channels, kernel_size, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>custom_conv <span class="op">=</span> CustomConv2d(in_channels, out_channels, kernel_size, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Copy weights from PyTorch conv to custom conv to ensure fair comparison</span></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>custom_conv.weight.data <span class="op">=</span> pytorch_conv.weight.data.clone()</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass</span></span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>pytorch_output <span class="op">=</span> pytorch_conv(x)</span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>custom_output <span class="op">=</span> custom_conv(x)</span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare outputs</span></span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> torch.<span class="bu">abs</span>(pytorch_output <span class="op">-</span> custom_output)</span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>max_diff <span class="op">=</span> torch.<span class="bu">max</span>(diff).item()</span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a>mean_diff <span class="op">=</span> torch.mean(diff).item()</span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shapes:"</span>)</span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch Conv2d output shape: </span><span class="sc">{</span>pytorch_output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Custom Conv2d output shape: </span><span class="sc">{</span>custom_output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Differences:"</span>)</span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Maximum absolute difference: </span><span class="sc">{</span>max_diff<span class="sc">:.8f}</span><span class="ss">"</span>)</span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean absolute difference: </span><span class="sc">{</span>mean_diff<span class="sc">:.8f}</span><span class="ss">"</span>)</span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Test backward pass</span></span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> pytorch_output.<span class="bu">sum</span>()</span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a>pytorch_grad <span class="op">=</span> pytorch_conv.weight.grad.clone()</span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a>pytorch_conv.weight.grad <span class="op">=</span> <span class="va">None</span>  <span class="co"># Reset grad</span></span>
<span id="cb64-42"><a href="#cb64-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-43"><a href="#cb64-43" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> custom_output.<span class="bu">sum</span>()</span>
<span id="cb64-44"><a href="#cb64-44" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb64-45"><a href="#cb64-45" aria-hidden="true" tabindex="-1"></a>custom_grad <span class="op">=</span> custom_conv.weight.grad.clone()</span>
<span id="cb64-46"><a href="#cb64-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-47"><a href="#cb64-47" aria-hidden="true" tabindex="-1"></a>grad_diff <span class="op">=</span> torch.<span class="bu">abs</span>(pytorch_grad <span class="op">-</span> custom_grad)</span>
<span id="cb64-48"><a href="#cb64-48" aria-hidden="true" tabindex="-1"></a>max_grad_diff <span class="op">=</span> torch.<span class="bu">max</span>(grad_diff).item()</span>
<span id="cb64-49"><a href="#cb64-49" aria-hidden="true" tabindex="-1"></a>mean_grad_diff <span class="op">=</span> torch.mean(grad_diff).item()</span>
<span id="cb64-50"><a href="#cb64-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-51"><a href="#cb64-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Gradient differences:"</span>)</span>
<span id="cb64-52"><a href="#cb64-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Maximum gradient difference: </span><span class="sc">{</span>max_grad_diff<span class="sc">:.8f}</span><span class="ss">"</span>)</span>
<span id="cb64-53"><a href="#cb64-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean gradient difference: </span><span class="sc">{</span>mean_grad_diff<span class="sc">:.8f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Output shapes:
PyTorch Conv2d output shape: torch.Size([4, 5, 3, 3])
Custom Conv2d output shape: torch.Size([4, 5, 3, 3])

Differences:
Maximum absolute difference: 0.00000024
Mean absolute difference: 0.00000004

Gradient differences:
Maximum gradient difference: 0.00000191
Mean gradient difference: 0.00000057</code></pre>
</div>
</div>
<p>We got roughly the same values on a forward pass and backward pass, exactly what we want to see!</p>
<p>How about efficiency?</p>
<div id="fcbc866c" class="cell" data-execution_count="84">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> timeit</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of iterations for timing</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>n_iterations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Time PyTorch implementation</span></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>pytorch_time <span class="op">=</span> timeit.timeit(</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span>: pytorch_conv(x), </span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>    number<span class="op">=</span>n_iterations</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Time custom implementation</span></span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>custom_time <span class="op">=</span> timeit.timeit(</span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span>: custom_conv(x), </span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>    number<span class="op">=</span>n_iterations</span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch Conv2d average time: </span><span class="sc">{</span>pytorch_time<span class="op">/</span>n_iterations<span class="op">*</span><span class="dv">1000</span><span class="sc">:.3f}</span><span class="ss"> ms"</span>)</span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Custom Conv2d average time: </span><span class="sc">{</span>custom_time<span class="op">/</span>n_iterations<span class="op">*</span><span class="dv">1000</span><span class="sc">:.3f}</span><span class="ss"> ms"</span>)</span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Custom implementation is </span><span class="sc">{</span>custom_time<span class="op">/</span>pytorch_time<span class="sc">:.1f}</span><span class="ss">x slower than PyTorch"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>PyTorch Conv2d average time: 0.072 ms
Custom Conv2d average time: 0.616 ms
Custom implementation is 8.5x slower than PyTorch</code></pre>
</div>
</div>
<p>Boooo! Our kernel as expected is much slower than the PyTorch implementation. Now, remember we purposefully did not optimise our kernel to retain its educational ability ;) Perhaps another day we can study the optimisation of the conv.</p>
</section>
</section>
<section id="a-fully-flegded-cnn" class="level3">
<h3 class="anchored" data-anchor-id="a-fully-flegded-cnn">A Fully Flegded CNN</h3>
<p>As a sanity check of our custom implementation, we can use it within a CNN proper. I created two small 3 layer CNNs, one of which uses the PyTorch Conv2d and the other which uses our own CustomConv2d. We’ll take the MNIST dataset, with the goal of getting roughly the same accuracy using the PyTorch Conv2d and our own CustomConv2d.</p>
<p>Expand the code block below to see the full model + data loading code, we can ignore the details as we only really care about the convolutional layer and the output today.</p>
<div id="5ef334b5" class="cell" data-execution_count="52">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;Make a dirty CNN to classify CIFAR10</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PytorchCNN(nn.Module):</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels):  </span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()  </span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;B: Batch size</span></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;C: Channel</span></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;H: Height</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;W: Width</span></span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># At any point in our network x has the shape (B, C, H, W)</span></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B doesn't change</span></span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;C, H, W can change</span></span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>         </span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># At this point our input x has the shape (B, 1, 28, 28)</span></span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span>in_channels, out_channels<span class="op">=</span><span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># After this conv we get out (B, 16, 26, 26)</span></span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;C = 16 now, we have 16 output feature maps</span></span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;H, W decreased by 2 as we have a 3x3 kernel</span></span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batchnorm1 <span class="op">=</span> nn.BatchNorm2d(<span class="dv">16</span>)</span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu1 <span class="op">=</span> nn.ReLU()</span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-22"><a href="#cb68-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">3</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb68-23"><a href="#cb68-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;Now we have (B, 32, 24, 24)</span></span>
<span id="cb68-24"><a href="#cb68-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batchnorm2 <span class="op">=</span> nn.BatchNorm2d(<span class="dv">32</span>)</span>
<span id="cb68-25"><a href="#cb68-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu2 <span class="op">=</span> nn.ReLU()</span>
<span id="cb68-26"><a href="#cb68-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-27"><a href="#cb68-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Final conv to flatten the network and get our predictions</span></span>
<span id="cb68-28"><a href="#cb68-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(<span class="dv">32</span>, <span class="dv">10</span>, <span class="dv">3</span>, bias<span class="op">=</span><span class="va">False</span>)  <span class="co"># 10 classes for CIFAR10</span></span>
<span id="cb68-29"><a href="#cb68-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Now we have (B, 10, 22, 22)</span></span>
<span id="cb68-30"><a href="#cb68-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-31"><a href="#cb68-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb68-32"><a href="#cb68-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb68-33"><a href="#cb68-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.batchnorm1(x)</span>
<span id="cb68-34"><a href="#cb68-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu1(x)</span>
<span id="cb68-35"><a href="#cb68-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-36"><a href="#cb68-36" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb68-37"><a href="#cb68-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.batchnorm2(x)</span>
<span id="cb68-38"><a href="#cb68-38" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu2(x)</span>
<span id="cb68-39"><a href="#cb68-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-40"><a href="#cb68-40" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv3(x)</span>
<span id="cb68-41"><a href="#cb68-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Average over spatial dimensions and flatten</span></span>
<span id="cb68-42"><a href="#cb68-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.mean([<span class="dv">2</span>, <span class="dv">3</span>])  <span class="co"># This replaces need for adaptive pooling</span></span>
<span id="cb68-43"><a href="#cb68-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The parameter is a list in the mean method. This list represents indices, and means we squash the </span></span>
<span id="cb68-44"><a href="#cb68-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># input x on indices 2 and 3. These are the H, W indices. We end up with a (B, 10) shape tensor.</span></span>
<span id="cb68-45"><a href="#cb68-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each element in this tensor represents the probability that the input x belongs to one of the</span></span>
<span id="cb68-46"><a href="#cb68-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># categories.</span></span>
<span id="cb68-47"><a href="#cb68-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-48"><a href="#cb68-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb68-49"><a href="#cb68-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-50"><a href="#cb68-50" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomCNN(nn.Module):</span>
<span id="cb68-51"><a href="#cb68-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels):  </span>
<span id="cb68-52"><a href="#cb68-52" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()  </span>
<span id="cb68-53"><a href="#cb68-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-54"><a href="#cb68-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> CustomConv2d(in_channels<span class="op">=</span>in_channels, out_channels<span class="op">=</span><span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb68-55"><a href="#cb68-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batchnorm1 <span class="op">=</span> nn.BatchNorm2d(<span class="dv">16</span>)</span>
<span id="cb68-56"><a href="#cb68-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu1 <span class="op">=</span> nn.ReLU()</span>
<span id="cb68-57"><a href="#cb68-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-58"><a href="#cb68-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> CustomConv2d(<span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">3</span>)</span>
<span id="cb68-59"><a href="#cb68-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batchnorm2 <span class="op">=</span> nn.BatchNorm2d(<span class="dv">32</span>)</span>
<span id="cb68-60"><a href="#cb68-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu2 <span class="op">=</span> nn.ReLU()</span>
<span id="cb68-61"><a href="#cb68-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-62"><a href="#cb68-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Final conv to flatten the network and get our predictions</span></span>
<span id="cb68-63"><a href="#cb68-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> CustomConv2d(<span class="dv">32</span>, <span class="dv">10</span>, <span class="dv">3</span>)  <span class="co"># 10 classes for CIFAR10</span></span>
<span id="cb68-64"><a href="#cb68-64" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-65"><a href="#cb68-65" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb68-66"><a href="#cb68-66" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb68-67"><a href="#cb68-67" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.batchnorm1(x)</span>
<span id="cb68-68"><a href="#cb68-68" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu1(x)</span>
<span id="cb68-69"><a href="#cb68-69" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-70"><a href="#cb68-70" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb68-71"><a href="#cb68-71" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.batchnorm2(x)</span>
<span id="cb68-72"><a href="#cb68-72" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu2(x)</span>
<span id="cb68-73"><a href="#cb68-73" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-74"><a href="#cb68-74" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv3(x)</span>
<span id="cb68-75"><a href="#cb68-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Average over spatial dimensions and flatten</span></span>
<span id="cb68-76"><a href="#cb68-76" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.mean([<span class="dv">2</span>, <span class="dv">3</span>])  <span class="co"># This replaces need for adaptive pooling</span></span>
<span id="cb68-77"><a href="#cb68-77" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb68-78"><a href="#cb68-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb68-79"><a href="#cb68-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-80"><a href="#cb68-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluation function</span></span>
<span id="cb68-81"><a href="#cb68-81" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate(model, dataloader, criterion, device):</span>
<span id="cb68-82"><a href="#cb68-82" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()  <span class="co"># Set to evaluation mode</span></span>
<span id="cb68-83"><a href="#cb68-83" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb68-84"><a href="#cb68-84" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb68-85"><a href="#cb68-85" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb68-86"><a href="#cb68-86" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-87"><a href="#cb68-87" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():  <span class="co"># No need to track gradients</span></span>
<span id="cb68-88"><a href="#cb68-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> data <span class="kw">in</span> dataloader:</span>
<span id="cb68-89"><a href="#cb68-89" aria-hidden="true" tabindex="-1"></a>            inputs, labels <span class="op">=</span> data</span>
<span id="cb68-90"><a href="#cb68-90" aria-hidden="true" tabindex="-1"></a>            inputs, labels <span class="op">=</span> inputs.to(device), labels.to(device)</span>
<span id="cb68-91"><a href="#cb68-91" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb68-92"><a href="#cb68-92" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(inputs)</span>
<span id="cb68-93"><a href="#cb68-93" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb68-94"><a href="#cb68-94" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb68-95"><a href="#cb68-95" aria-hidden="true" tabindex="-1"></a>            total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb68-96"><a href="#cb68-96" aria-hidden="true" tabindex="-1"></a>            _, predicted <span class="op">=</span> outputs.<span class="bu">max</span>(<span class="dv">1</span>)</span>
<span id="cb68-97"><a href="#cb68-97" aria-hidden="true" tabindex="-1"></a>            total <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb68-98"><a href="#cb68-98" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> predicted.eq(labels).<span class="bu">sum</span>().item()</span>
<span id="cb68-99"><a href="#cb68-99" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-100"><a href="#cb68-100" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_loss <span class="op">/</span> <span class="bu">len</span>(dataloader), <span class="fl">100.</span> <span class="op">*</span> correct <span class="op">/</span> total</span>
<span id="cb68-101"><a href="#cb68-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-102"><a href="#cb68-102" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataset download and dataloader init</span></span>
<span id="cb68-103"><a href="#cb68-103" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb68-104"><a href="#cb68-104" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb68-105"><a href="#cb68-105" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), (<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>))</span>
<span id="cb68-106"><a href="#cb68-106" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb68-107"><a href="#cb68-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-108"><a href="#cb68-108" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb68-109"><a href="#cb68-109" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb68-110"><a href="#cb68-110" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>(<span class="fl">0.5</span>,), std<span class="op">=</span>(<span class="fl">0.5</span>,))  <span class="co"># Single value for grayscale</span></span>
<span id="cb68-111"><a href="#cb68-111" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb68-112"><a href="#cb68-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-113"><a href="#cb68-113" aria-hidden="true" tabindex="-1"></a>trainset <span class="op">=</span> torchvision.datasets.MNIST(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb68-114"><a href="#cb68-114" aria-hidden="true" tabindex="-1"></a>                                       download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb68-115"><a href="#cb68-115" aria-hidden="true" tabindex="-1"></a>testset <span class="op">=</span> torchvision.datasets.MNIST(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb68-116"><a href="#cb68-116" aria-hidden="true" tabindex="-1"></a>                                     download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb68-117"><a href="#cb68-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-118"><a href="#cb68-118" aria-hidden="true" tabindex="-1"></a><span class="co"># Create data loaders</span></span>
<span id="cb68-119"><a href="#cb68-119" aria-hidden="true" tabindex="-1"></a>trainloader <span class="op">=</span> torch.utils.data.DataLoader(trainset, batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb68-120"><a href="#cb68-120" aria-hidden="true" tabindex="-1"></a>                                        shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb68-121"><a href="#cb68-121" aria-hidden="true" tabindex="-1"></a>testloader <span class="op">=</span> torch.utils.data.DataLoader(testset, batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb68-122"><a href="#cb68-122" aria-hidden="true" tabindex="-1"></a>                                       shuffle<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The following two code blocks contain the training code, expand them to check it out.</p>
<section id="pytorch-cnn-training-run" class="level4">
<h4 class="anchored" data-anchor-id="pytorch-cnn-training-run">PyTorch CNN Training Run</h4>
<div id="0f4bca21" class="cell" data-scrolled="true" data-execution_count="54">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training the PyTorch model</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define pytorch model</span></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>pytorch_model <span class="op">=</span> PytorchCNN(in_channels<span class="op">=</span><span class="dv">1</span>).to(device)</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>pytorch_criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>pytorch_optimizer <span class="op">=</span> torch.optim.Adam(pytorch_model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop with evaluation</span></span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set to training mode</span></span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>    pytorch_model.train() </span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a>    pytorch_running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, data <span class="kw">in</span> <span class="bu">enumerate</span>(trainloader, <span class="dv">0</span>):</span>
<span id="cb69-20"><a href="#cb69-20" aria-hidden="true" tabindex="-1"></a>        inputs, labels <span class="op">=</span> data</span>
<span id="cb69-21"><a href="#cb69-21" aria-hidden="true" tabindex="-1"></a>        inputs, labels <span class="op">=</span> inputs.to(device), labels.to(device)</span>
<span id="cb69-22"><a href="#cb69-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb69-23"><a href="#cb69-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train the pytorch CNN model</span></span>
<span id="cb69-24"><a href="#cb69-24" aria-hidden="true" tabindex="-1"></a>        pytorch_optimizer.zero_grad()</span>
<span id="cb69-25"><a href="#cb69-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb69-26"><a href="#cb69-26" aria-hidden="true" tabindex="-1"></a>        pytorch_outputs <span class="op">=</span> pytorch_model(inputs)</span>
<span id="cb69-27"><a href="#cb69-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb69-28"><a href="#cb69-28" aria-hidden="true" tabindex="-1"></a>        pytorch_loss <span class="op">=</span> pytorch_criterion(pytorch_outputs, labels)</span>
<span id="cb69-29"><a href="#cb69-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb69-30"><a href="#cb69-30" aria-hidden="true" tabindex="-1"></a>        pytorch_loss.backward()</span>
<span id="cb69-31"><a href="#cb69-31" aria-hidden="true" tabindex="-1"></a>        pytorch_optimizer.step()</span>
<span id="cb69-32"><a href="#cb69-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb69-33"><a href="#cb69-33" aria-hidden="true" tabindex="-1"></a>        pytorch_running_loss <span class="op">+=</span> pytorch_loss.item()</span>
<span id="cb69-34"><a href="#cb69-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb69-35"><a href="#cb69-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">250</span> <span class="op">==</span> <span class="dv">249</span>:    <span class="co"># Print every 100 mini-batches</span></span>
<span id="cb69-36"><a href="#cb69-36" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'[Epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, Batch </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">]'</span>)</span>
<span id="cb69-37"><a href="#cb69-37" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"PYTORCH CNN LOSS: </span><span class="sc">{</span>pytorch_running_loss <span class="op">/</span> <span class="dv">250</span><span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb69-38"><a href="#cb69-38" aria-hidden="true" tabindex="-1"></a>            pytorch_running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb69-39"><a href="#cb69-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-40"><a href="#cb69-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate after each epoch</span></span>
<span id="cb69-41"><a href="#cb69-41" aria-hidden="true" tabindex="-1"></a>    pytorch_train_loss, pytorch_train_acc <span class="op">=</span> evaluate(pytorch_model, trainloader, pytorch_criterion, device)</span>
<span id="cb69-42"><a href="#cb69-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-43"><a href="#cb69-43" aria-hidden="true" tabindex="-1"></a>    pytorch_test_loss, pytorch_test_acc <span class="op">=</span> evaluate(pytorch_model, testloader, pytorch_criterion, device)</span>
<span id="cb69-44"><a href="#cb69-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-45"><a href="#cb69-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">:'</span>)</span>
<span id="cb69-46"><a href="#cb69-46" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Pytorch Train Loss: </span><span class="sc">{</span>pytorch_train_loss<span class="sc">:.3f}</span><span class="ss">, Pytorch Train Acc: </span><span class="sc">{</span>pytorch_train_acc<span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb69-47"><a href="#cb69-47" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Pytorch Test Loss: </span><span class="sc">{</span>pytorch_test_loss<span class="sc">:.3f}</span><span class="ss">, Pytorch Test Acc: </span><span class="sc">{</span>pytorch_test_acc<span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb69-48"><a href="#cb69-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'--------------------'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[Epoch 1, Batch 250]
PYTORCH CNN LOSS: 1.544
[Epoch 1, Batch 500]
PYTORCH CNN LOSS: 0.972
[Epoch 1, Batch 750]
PYTORCH CNN LOSS: 0.737
[Epoch 1, Batch 1000]
PYTORCH CNN LOSS: 0.638
[Epoch 1, Batch 1250]
PYTORCH CNN LOSS: 0.520
[Epoch 1, Batch 1500]
PYTORCH CNN LOSS: 0.443
[Epoch 1, Batch 1750]
PYTORCH CNN LOSS: 0.404
[Epoch 1, Batch 2000]
PYTORCH CNN LOSS: 0.386
[Epoch 1, Batch 2250]
PYTORCH CNN LOSS: 0.357
[Epoch 1, Batch 2500]
PYTORCH CNN LOSS: 0.310
[Epoch 1, Batch 2750]
PYTORCH CNN LOSS: 0.330
[Epoch 1, Batch 3000]
PYTORCH CNN LOSS: 0.301
[Epoch 1, Batch 3250]
PYTORCH CNN LOSS: 0.313
[Epoch 1, Batch 3500]
PYTORCH CNN LOSS: 0.274
[Epoch 1, Batch 3750]
PYTORCH CNN LOSS: 0.245
Epoch 1:
Pytorch Train Loss: 0.501, Pytorch Train Acc: 81.99%
Pytorch Test Loss: 0.497, Pytorch Test Acc: 81.92%
--------------------</code></pre>
</div>
</div>
</section>
<section id="custom-cnn-training-run" class="level4">
<h4 class="anchored" data-anchor-id="custom-cnn-training-run">Custom CNN Training Run</h4>
<div id="4c0be31d" class="cell" data-scrolled="true" data-execution_count="55">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;Training the custom model</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define custom model</span></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>custom_model <span class="op">=</span> CustomCNN(in_channels<span class="op">=</span><span class="dv">1</span>).to(device)</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>custom_criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>custom_optimizer <span class="op">=</span> torch.optim.Adam(custom_model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop with evaluation</span></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set to training mode</span></span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>    custom_model.train()</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>    custom_running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, data <span class="kw">in</span> <span class="bu">enumerate</span>(trainloader, <span class="dv">0</span>):</span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>        inputs, labels <span class="op">=</span> data</span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a>        inputs, labels <span class="op">=</span> inputs.to(device), labels.to(device)</span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train the custom CNN model</span></span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a>        custom_optimizer.zero_grad()</span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb71-26"><a href="#cb71-26" aria-hidden="true" tabindex="-1"></a>        custom_outputs <span class="op">=</span> custom_model(inputs)</span>
<span id="cb71-27"><a href="#cb71-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb71-28"><a href="#cb71-28" aria-hidden="true" tabindex="-1"></a>        custom_loss <span class="op">=</span> custom_criterion(custom_outputs, labels)</span>
<span id="cb71-29"><a href="#cb71-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb71-30"><a href="#cb71-30" aria-hidden="true" tabindex="-1"></a>        custom_loss.backward()</span>
<span id="cb71-31"><a href="#cb71-31" aria-hidden="true" tabindex="-1"></a>        custom_optimizer.step()</span>
<span id="cb71-32"><a href="#cb71-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb71-33"><a href="#cb71-33" aria-hidden="true" tabindex="-1"></a>        custom_running_loss <span class="op">+=</span> custom_loss.item()</span>
<span id="cb71-34"><a href="#cb71-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb71-35"><a href="#cb71-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">250</span> <span class="op">==</span> <span class="dv">249</span>:    <span class="co"># Print every 100 mini-batches</span></span>
<span id="cb71-36"><a href="#cb71-36" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'[Epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, Batch </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">]'</span>)</span>
<span id="cb71-37"><a href="#cb71-37" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"CUSTOM CNN LOSS: </span><span class="sc">{</span>custom_running_loss <span class="op">/</span> <span class="dv">250</span><span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb71-38"><a href="#cb71-38" aria-hidden="true" tabindex="-1"></a>            custom_running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb71-39"><a href="#cb71-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-40"><a href="#cb71-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate after each epoch</span></span>
<span id="cb71-41"><a href="#cb71-41" aria-hidden="true" tabindex="-1"></a>    custom_train_loss, custom_train_acc <span class="op">=</span> evaluate(custom_model, trainloader, custom_criterion, device)</span>
<span id="cb71-42"><a href="#cb71-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-43"><a href="#cb71-43" aria-hidden="true" tabindex="-1"></a>    custom_test_loss, custom_test_acc <span class="op">=</span> evaluate(custom_model, testloader, custom_criterion, device)</span>
<span id="cb71-44"><a href="#cb71-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-45"><a href="#cb71-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">:'</span>)</span>
<span id="cb71-46"><a href="#cb71-46" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Custom Train Loss: </span><span class="sc">{</span>custom_train_loss<span class="sc">:.3f}</span><span class="ss">, Custom Train Acc: </span><span class="sc">{</span>custom_train_acc<span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb71-47"><a href="#cb71-47" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Custom Test Loss: </span><span class="sc">{</span>custom_test_loss<span class="sc">:.3f}</span><span class="ss">, Custom Test Acc: </span><span class="sc">{</span>custom_test_acc<span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb71-48"><a href="#cb71-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'--------------------'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[Epoch 1, Batch 250]
CUSTOM CNN LOSS: 1.865
[Epoch 1, Batch 500]
CUSTOM CNN LOSS: 1.171
[Epoch 1, Batch 750]
CUSTOM CNN LOSS: 0.919
[Epoch 1, Batch 1000]
CUSTOM CNN LOSS: 0.815
[Epoch 1, Batch 1250]
CUSTOM CNN LOSS: 0.691
[Epoch 1, Batch 1500]
CUSTOM CNN LOSS: 0.629
[Epoch 1, Batch 1750]
CUSTOM CNN LOSS: 0.583
[Epoch 1, Batch 2000]
CUSTOM CNN LOSS: 0.531
[Epoch 1, Batch 2250]
CUSTOM CNN LOSS: 0.476
[Epoch 1, Batch 2500]
CUSTOM CNN LOSS: 0.436
[Epoch 1, Batch 2750]
CUSTOM CNN LOSS: 0.406
[Epoch 1, Batch 3000]
CUSTOM CNN LOSS: 0.383
[Epoch 1, Batch 3250]
CUSTOM CNN LOSS: 0.402
[Epoch 1, Batch 3500]
CUSTOM CNN LOSS: 0.345
[Epoch 1, Batch 3750]
CUSTOM CNN LOSS: 0.359
Epoch 1:
Custom Train Loss: 0.639, Custom Train Acc: 77.95%
Custom Test Loss: 0.608, Custom Test Acc: 78.95%
--------------------</code></pre>
</div>
</div>
<p>Look at that! Our model’s accuracy is pretty close together. It will vary slightly but that is to be expected, if you run either model again I’m sure the numbers will change.<a href="#7"><sup>7</sup></a></p>
<p>You probably noticed how slow the custom CNN is compared to the PyTorch CNN (if you didn’t run the code, it took me a few hours on a GPU to get the output you see with the custom CNN). The triple nested for loop is the culprit. As the input size of images grows the number of iterations we take will cubically increase, which is unacceptable. The second thing you’ll notice is that the loss and the accuracy of the two networks are pretty similar, success! We’ve completed our journey, building up a convolutional layer and our foundational knowledge of this brilliant piece of history!</p>
<p>I hope you enjoyed the post, I think the depth of math behind such a seemingly simple idea is truly wonderful.</p>
<hr>
<p><a id="1" style="text-decoration: none; color: inherit;" href=""><sup>1</sup></a> A common solution to this problem is padding. We add a boundary of 0 valued pixels all across the image. This has a few uses, another being to help retain the dimensions of the image.</p>
<p><a id="2" style="text-decoration: none; color: inherit;" href=""><sup>2</sup></a> Kernels are usually odd-value shaped. This is because odd numbers allow us to easily determine the centre of the kernel.</p>
<p><a id="3" style="text-decoration: none; color: inherit;" href=""><sup>3</sup></a> The other typical layers found in CNNs: pooling, non-linear activation functions (ReLu etc), fully connected, batch norm, dropout among others. If you want some more background check out: <a href="https://poloclub.github.io/cnn-explainer/">https://poloclub.github.io/cnn-explainer/</a> - This is a great resource to learn more about CNNs.</p>
<p><a id="4" style="text-decoration: none; color: inherit;" href=""><sup>4</sup></a> <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms"> BLAS </a></p>
<p><a id="5" style="text-decoration: none; color: inherit;" href=""><sup>5</sup></a> <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">The AlexNet Paper</a></p>
<p><a id="6" style="text-decoration: none; color: inherit;" href=""><sup>6</sup></a> I decided to implement only the forward pass for simplicity. I invite anyone to go ahead and implement the backward pass too, if you do please let me know how it goes @ <a href="mailto:y%75sufmohamma%64@l%69ve.com">yusufmohammad@live.com</a></p><a href="mailto:y%75sufmohamma%64@l%69ve.com">
</a><p><a href="mailto:y%75sufmohamma%64@l%69ve.com"></a><a id="7" style="text-decoration: none; color: inherit;" href=""><sup>7</sup></a> The variability in accuracy comes from multiple sources: random initialization of weights, the stochastic nature of optimization, and the relatively simple architecture we’re using. MNIST is quite sensitive to these variations, given how simple of a dataset it is. More stable results could be achieved through techniques like proper weight initialization (e.g., He initialization), learning rate scheduling, or a better model (add more layers xD).</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>