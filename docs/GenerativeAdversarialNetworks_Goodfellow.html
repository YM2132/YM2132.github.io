<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yusuf Mohammad">

<title>From Scratch - Generative Adversarial Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="GenerativeAdversarialNetworks_Goodfellow_files/libs/clipboard/clipboard.min.js"></script>
<script src="GenerativeAdversarialNetworks_Goodfellow_files/libs/quarto-html/quarto.js"></script>
<script src="GenerativeAdversarialNetworks_Goodfellow_files/libs/quarto-html/popper.min.js"></script>
<script src="GenerativeAdversarialNetworks_Goodfellow_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="GenerativeAdversarialNetworks_Goodfellow_files/libs/quarto-html/anchor.min.js"></script>
<link href="GenerativeAdversarialNetworks_Goodfellow_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="GenerativeAdversarialNetworks_Goodfellow_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="GenerativeAdversarialNetworks_Goodfellow_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="GenerativeAdversarialNetworks_Goodfellow_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="GenerativeAdversarialNetworks_Goodfellow_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
<div class="quarto-other-links"><h2>Other Links</h2><ul><li><a href="https://ym2132.github.io/"><i class="bi bi-link-45deg"></i>Yusuf's Deep Learning Blog</a></li><li><a href="https://github.com/YM2132?tab=repositories"><i class="bi bi-link-45deg"></i>Yusuf's GitHub</a></li></ul></div><div class="quarto-code-links"><h2>Code Links</h2><ul><li><a href="https://github.com/YM2132/YMPaperImplementations/blob/main/paper_implementations/python_implementations/GAN_Goodfellow.py"><i class="bi bi-file-code"></i>GANs.py</a></li></ul></div></div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">From Scratch - Generative Adversarial Networks</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Yusuf Mohammad </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<hr>
<p>Generative Adversarial Networks (GANs), discovered by Ian Goodfellow in 2014, were an early method in the area of generative AI. I will focus on image generation as set out in the paper Generative Adversarial Nets<a href="#1"><sup>1</sup></a>. This paper is the focal point of this blog post, and I will guide you through the implementation of the paper. After reading this I hope you understand GANs better and how to build them.</p>
<p>A small note, this blog post will attempt to show you my full process for understanding and implementing the paper. I try to include my entire thought process which leads up to the final code for the model, I hope you find this helpful. I will assume some prior background knowledge of deep learning techniques, e.g.&nbsp;what is an MLP<a href="#2"><sup>2</sup></a>, the basics of PyTorch<a href="#3"><sup>3</sup></a>.</p>
<p>The name Generative Adversarial Network tells us some of the story of this framework. In a GAN we have two models, which we pit against each other, the Generator (G) model and the Discriminator (D) model. The goal of G is to capture the distribution of the training data and then use this to generate samples (images in our case) from that distribution. Now what does it mean for a dataset of images to have a distribution, simply the images themselves have stastical properties. In an image typically neighbouring pixels have high correlation and distant have low correlation, in MNIST specifically there is a lot of straight lines and curves (as you’d expect in images). The G model is learning these statistical properties, hence learning the distribution and samples from this to generate new images.</p>
<p>On the other hand, the goal of D is to determine whether an image is from the training set distribution or the G distribution i.e.&nbsp;to detect if an image is real (a real image is one from the training set), or fake (an image from the generated set).</p>
<p>The training regime is as follows: the D model is trained to maximise the probability of assigning the correct label to both generated and training examples, the G model is trained to maximise the probabilty of D making a mistake (more on this later). Let’s now explore the journey I took to arrive at my final implementation!</p>
<section id="the-path-to-implementation" class="level2">
<h2 class="anchored" data-anchor-id="the-path-to-implementation">The Path to Implementation</h2>
<p>A major point of initial confusion for me was the idea of training two neural networks simultaneously and the mechanics of this. So, first I trained just one, namely the Discriminator model. I figured that if I can get a D model which could classify between generated random noise images and real images, it would build my underlying understanding of the GAN and how to create the training loop. To get started with this lets first define the D and G models. We will only train D we will not update the parameters of G.</p>
<p>This idea came from examining Algorithm 1 provided in the paper:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GenerativeAdversarialNetworks_Goodfellow_files/figure-html/767ddf10-1-image.png" class="img-fluid figure-img"></p>
<figcaption>Algorithm 1</figcaption>
</figure>
</div>
<p>We see in Algorithm 1 we have two gradient updates, initially to get our heads around the problem lets simply update the generator only.</p>
<hr style="border:2px solid black">
<p><b>Heads up</b> The following is an insight into my process of understanding the paper. My hope is to provide some insight into my process, feel free to skip ahead to the <a href="#skip">actual implemenation</a>.</p>
<hr style="border:2px solid black">
<section id="simple-generator" class="level3">
<h3 class="anchored" data-anchor-id="simple-generator">Simple Generator</h3>
<p>First, let’s get all the admin stuff out the way ;)</p>
<div id="8bf5f2b1" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># All the imports required for this implementation</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> TensorDataset, ConcatDataset, random_split, DataLoader, Dataset</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># We can make use of a GPU if you have one on your computer. This works for Nvidia and M series GPU's</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># These 2 lines assign some data on the memory of the device and output it. The output confirms</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if we have set the intended device</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.ones(<span class="dv">1</span>, device<span class="op">=</span>device)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (x)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> torch.backends.cuda.is_built():</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.ones(<span class="dv">1</span>, device<span class="op">=</span>device)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (x)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> (<span class="st">"cpu"</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.ones(<span class="dv">1</span>, device<span class="op">=</span>device)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([1.], device='mps:0')</code></pre>
</div>
</div>
<p>I define a simple G which takes an input of size 1 and returns an image which is just random noise. In the paper it is stated that the input to G is random noise, here I choose a number from a Normal distribution as my noise and for this instance of G I set the input size to 1. Also, the ReLU layer’s in this model come from the paper, despite the actual model architecture not being specified, they state it was a a Multi-Layer Perceptron (MLP) for both D and G. For G I use a simple two layer MLP with ReLU between the layers, I also employ a tanh for the output layer. The tanh ensures the output values are between [-1, 1] this keeps our pixel values in the same range as the actual mnist data. Note, in the paper the G uses ReLU and Sigmoid but I opt for Tanh as it works better.</p>
<hr>
<p>This is a common theme when implementing papers, you have to use your intuition when deciding the architecture and piece together the puzzle the best you can from the hints given in the paper. The papers are often incomplete in their description of the techniques used, the best way to build your intuition is doing it repeatedly and not being afraid to try different things.</p>
<hr>
<p>To finish off, the actual output of the model must be converted to a matrix. I chose to do this inside the forward function and I include my own implementation as well as the PyTorch way. Uncomment my code to play around with it, it currently only works when the input has dimensions [1] (I leave it up to you to try and implement this to work with inputs which have more than 1 image). The reason being is that I do not handle the batch dimension, to do so you’d need another for loop.</p>
<div id="262ddde1" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The following code block is a simple way to define neural networks in PyTorch.</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;We init the layers and then pass x through these layers in the forward pass.</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Generator(nn.Module): </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">256</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu1 <span class="op">=</span> nn.ReLU()</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(<span class="dv">256</span>, <span class="dv">784</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tanh <span class="op">=</span> nn.Tanh()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear1(x)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu1(x)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear2(x)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.tanh(x)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Need to convert the output vector x to a matrix</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note this is my way of doing the conversion, there are much better ways to do this</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but, implementing it by hand may give you some insights into what is happening on line 40</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">'''</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="co">        g_out_mat = torch.zeros(1, 28, 28)</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="co">        m = 0</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="co">        n = 0</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="co">        for i in range(len(x)):</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="co">            if i % 28 == 0 and i != 0:</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="co">                m += 1</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="co">                n = 0</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="co">            g_out_mat[0, m, n] = x[i]</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="co">            n += 1</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="co">        '''</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A simpler way to reshape the output to a 28x28 matrix</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We use -1 as the first dim as it tells PyTorch to automatically calculate the correct size for x</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># i.e. the batch size. Try out a different value and see what happens. Functionally it is equivalent to</span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># putting x.size(0)</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> Generator()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now lets generate a random noise sample and show what this model outputs!</p>
<div id="d9f73b8c" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;Set mu and sigma for our Normal distrubiton and sample one value from the distribution</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>mu, sigma <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>noise_value <span class="op">=</span> np.random.normal(mu, sigma, <span class="dv">1</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># The input to our network has to be a tensor datatype, in this case it just has one value</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>g_in <span class="op">=</span> torch.tensor(noise_value, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># We do the forward pass on the input</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>g_out <span class="op">=</span> generator(g_in)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># This is a small function to display the output</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> imshow(img):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> img <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="fl">0.5</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    npimg <span class="op">=</span> img.numpy()</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    plt.imshow(np.transpose(npimg, (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>imshow(g_out.detach().cpu()), torch.Tensor([<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="GenerativeAdversarialNetworks_Goodfellow_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We see that the image is completely random and has no patterns, which is exactly what we wanted. Now, when training the D model, we must train on both generated and real images. I was unsure on how to do this in practice, but we can use a hack in this case. Lets supplement the MNIST dataset with 60k generated images, i.e.&nbsp;create a 50/50 split on generated/real images.</p>
<p>This method is not what we use to train the actual network, as in the actual training loop we must provide newly generated samples at each epoch (the generator is improving so we want the new samples to be better at fooling the discriminator). But, for now lets stick with it!</p>
<div id="d9355075" class="cell" data-scrolled="false" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Lets first generate 70k noise numbers from the normal dist</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>noise_tensor <span class="op">=</span> torch.randn(<span class="dv">70000</span>, <span class="dv">1</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Will pass each of these to the model to give us 70k noisy images</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    gen_images <span class="op">=</span> generator(noise_tensor)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    gen_images <span class="op">=</span> gen_images.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>gen_labels <span class="op">=</span> torch.zeros((<span class="dv">70000</span>, <span class="dv">1</span>))  <span class="co">#&nbsp;We init a list of 70k labels which are all 0. 0 means generated image</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>gen_labels <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> <span class="dv">70000</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Lets show an example of what we just generated</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>imshow(gen_images[<span class="dv">0</span>].detach()), gen_labels[<span class="dv">0</span>]</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dimension of generated images Tensor: </span><span class="sc">{</span>gen_images<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="GenerativeAdversarialNetworks_Goodfellow_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Dimension of generated images Tensor: torch.Size([70000, 1, 28, 28])</code></pre>
</div>
</div>
<p>As we wanted we get a random image just as before, only now we have 70000 of them. The next step is to add these to the original MNIST dataset. We do this as follows: create a PyTorch dataset of the generated images and their labels, create a train/test split (matching MNIST train/test split size) of the generated dataset and then finally combine the MNIST and Generated dataset together. Take a look at how this is done!</p>
<div id="7d390f23" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;First we need to load in the MNIST dataset. The following code is a standard way to download PyTorch</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># datasets</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># We normalise the images and convert them to tensors.</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.5</span>,), (<span class="fl">0.5</span>,)),</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;Load both MNIST test and train sets</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>mnist_train <span class="op">=</span> torchvision.datasets.MNIST(</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">'./Data'</span>,</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transform,</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>mnist_test <span class="op">=</span> torchvision.datasets.MNIST(</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">'./Data'</span>,</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transform</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;For our example we are classifying if an image is from MNIST or the generated set, so we assign all examples</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="co"># from MNIST with the label 1</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>mnist_train.targets <span class="op">=</span> torch.ones_like(mnist_train.targets, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>mnist_test.targets <span class="op">=</span> torch.ones_like(mnist_train.targets, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="co"># In PyTorch we can use DataLoader class to instantiate an iterator which will efficiently pass data to the </span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="co"># network</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>    mnist_train, </span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size,</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>    mnist_test,</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size,</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co">: We never use the train/test split why not just train with all data?</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Above we load the actual MNIST dataset and now we combine the real MNIST images and the generated images.</p>
<div id="08e88924" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a custom dataset class which allows us to keep the labels as integers to match the MNIST data</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># The datatype for MNIST labels is integers, if we do not define a custom dataset class the label types</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># will not match up so this is necessary for the code to work</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomTensorDataset(Dataset):</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Dataset wrapping tensors and integer labels.</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">        tensors (Tensor): contains sample data.</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">        labels (list of int): contains sample labels.</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tensors, labels):</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> tensors.size(<span class="dv">0</span>) <span class="op">==</span> <span class="bu">len</span>(labels)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tensors <span class="op">=</span> tensors</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> labels</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.tensors[index], <span class="va">self</span>.labels[index]</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.tensors.size(<span class="dv">0</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>gen_dataset <span class="op">=</span> CustomTensorDataset(gen_images, gen_labels)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the train/test split of the generated dataset</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="dv">60000</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>test_size <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>gen_train_dataset, gen_test_dataset <span class="op">=</span> random_split(gen_dataset, [train_size, test_size])</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;Combine MNIST and the generated dataset</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>comb_train_dataset <span class="op">=</span> ConcatDataset([mnist_train, gen_train_dataset])</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>comb_test_dataset <span class="op">=</span> ConcatDataset([mnist_test, gen_test_dataset])</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataLoaders for the combined datasets</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>comb_train_loader <span class="op">=</span> DataLoader(comb_train_dataset, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>comb_test_loader <span class="op">=</span> DataLoader(comb_test_dataset, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="simple-discriminator" class="level3">
<h3 class="anchored" data-anchor-id="simple-discriminator">Simple Discriminator</h3>
<p>Now the dataset is ready to go so let’s build the classifier, AKA the Discrimanator model.</p>
<p>The D model is another MLP network. The input is one or more image/s and the output is a binary classification, 1 for a real image and 0 for a generated image. Again I define a somewhat arbitrary network structure, as in the case of the G model, and I once again advise you that this is a skill you will develop by trying different things when implementing these papers. In the paper it is stated that maxout activations are used, but I use ReLU and Sigmoid there isn’t a big reason why other than that it works! I understand this answer may not be satisfactory, but when implementing papers we have to test multiple avenues and find what works. I’ve found this to be the best approach for me. As I said before one of the goals is to build your intuition and it is only done through trial and error. A tip, if something doesn’t make sense like maxout activations or seems unfamiliar use something which is familiar and see if it works, sometimes you may even get better results!</p>
<p>To wrap up, our D model is a simple 2 layer MLP and acts as a binary classifier.</p>
<div id="d9cd5ea1" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Discriminator(nn.Module): </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(<span class="dv">784</span>, <span class="dv">256</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu1 <span class="op">=</span> nn.ReLU()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout1 <span class="op">=</span> nn.Dropout(<span class="fl">0.5</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(<span class="dv">256</span>, <span class="dv">1</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout2 <span class="op">=</span> nn.Dropout(<span class="fl">0.5</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use sigmoid to ensure output is a probability for the loss function</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigmoid <span class="op">=</span> nn.Sigmoid()</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear1(x)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu1(x)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout1(x)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear2(x)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sigmoid(x)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>discriminator <span class="op">=</span> Discriminator()</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="co"># We also send our device to the "device", i.e. the GPU if available</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>discriminator.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>Discriminator(
  (linear1): Linear(in_features=784, out_features=256, bias=True)
  (relu1): ReLU()
  (dropout1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=256, out_features=1, bias=True)
  (dropout2): Dropout(p=0.5, inplace=False)
  (sigmoid): Sigmoid()
)</code></pre>
</div>
</div>
</section>
<section id="training-the-noise-classifier" class="level3">
<h3 class="anchored" data-anchor-id="training-the-noise-classifier">Training the Noise Classifier</h3>
<p>Now we can get to the fun stuff and train our noise classifier (the D model), I call it this as we will be classifying between real images and the generated images which are noise.</p>
<p>First, we must choose our loss function. I use the Binary Cross Entropy Loss here, it the binary equivalent of Cross Entropy Loss. Cross Entropy Loss is a good metric for classification problems and when you implement different papers in the deep learning space you’ll come across it alot. For our purposes we just need to know it is our loss function, i.e.&nbsp;how good or bad our model is performing. Next we initialise our optimizer, I’ll skip over the details of this here<a href="#4"><sup>4</sup></a> (I may or may not make a post explaining all the moving parts of the training loop).</p>
<div id="d8ed7c97" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(discriminator.parameters(), lr<span class="op">=</span><span class="fl">0.05</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next we setup the training loop, I have added comments to describe the role of each piece of the loop.</p>
<div id="0d0ce36d" class="cell" data-scrolled="true" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># An epoch just means 1 iteration, here we train for only 3 iterations. You'll see the model converges quickly</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&nbsp;This is becuase the task is so simple</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;Running loss keeps track of the loss at each forward/backward pass of the network, we use it to calculate</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;average loss of the network on each epoch</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;.train() sets the model to train mode, this is PyTorch behavior. You see later we have .eval() both these </span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># methods change properties of some layers in the network.</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    discriminator.train()</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#&nbsp;We iterate over each batch in the train_loader</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, data <span class="kw">in</span> <span class="bu">enumerate</span>(comb_train_loader, <span class="dv">0</span>):</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;data is a tuple of inputs, labels so we split them up</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        inputs, labels <span class="op">=</span> data        </span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Flatten the input, it is current a tensor of dimension (batch_size, 28, 28) </span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the first layer of the network expects a 784 length vector so after flattening </span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dimension is (batch_size, 784)</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> torch.flatten(inputs, start_dim<span class="op">=</span><span class="dv">1</span>)        </span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Push the inputs and labels to the GPU if available</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>        inputs, labels <span class="op">=</span> inputs.to(device), labels.to(device)</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Zero the gradients of the optimizer, this is standard in training loops</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># it ensures our gradient steps are not too large</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Perform the forward pass on our data</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> discriminator(inputs)</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Ensure outputs and labels have the same shape</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.<span class="bu">float</span>()</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the loss of our network, i.e. how good/bad were it's prediction</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Using the loss perform backpropagation</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Using the calculated gradients bump the parameters of the model</span></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">+=</span> loss.item() </span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print the average loss for the epoch</span></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Epoch [</span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">] loss: </span><span class="sc">{</span>running_loss <span class="op">/</span> <span class="bu">len</span>(train_loader)<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># As before we set the model to eval mode</span></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>    discriminator.<span class="bu">eval</span>()</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Since we're not training, we don't need to calculate the gradients for our outputs</span></span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Perform a forward pass on the network and calculate the loss</span></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># When evaluating we do not need to calculate gradients or perform a step</span></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> data <span class="kw">in</span> comb_test_loader:</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>            images, labels <span class="op">=</span> data</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>            images <span class="op">=</span> torch.flatten(images, start_dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> labels.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> labels.<span class="bu">float</span>()</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Push images and labels to gpu</span></span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>            images, labels <span class="op">=</span> images.to(device), labels.to(device)</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>            <span class="co"># calculate outputs by running images through the network</span></span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> discriminator(images)</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>            <span class="co"># As we have 2 classes we interpret any prediction above 0.5 as a 1 and below a 0</span></span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>            predicted <span class="op">=</span> (outputs <span class="op">&gt;</span> <span class="fl">0.5</span>).<span class="bu">float</span>()  <span class="co"># Convert probabilities to binary predictions</span></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>    test_accuracy <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> correct <span class="op">/</span> <span class="bu">len</span>(comb_test_dataset)</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>test_accuracy<span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Finished Training'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [1] loss: 0.001
Accuracy: 100.00%
Epoch [2] loss: 0.000
Accuracy: 100.00%
Epoch [3] loss: 0.000
Accuracy: 100.00%
Finished Training</code></pre>
</div>
</div>
<p>Observing the accuracy of the network we see 100% accuracy, this may be alarming at first but given the nature of the task it makes sense. It’s a very simple task and the network is doing well, we can verify if it works by generating a new random sample and checking the output of the network.</p>
<div id="9e0f8ed7" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>noise_value <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>g_in <span class="op">=</span> torch.tensor(noise_value, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>g_out <span class="op">=</span> generator(g_in)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>g_out <span class="op">=</span> g_out.to(device)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>discriminator(torch.flatten(g_out, start_dim<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>tensor([[2.1980e-07]], device='mps:0', grad_fn=&lt;SigmoidBackward0&gt;)</code></pre>
</div>
</div>
<p>The output is very small, which means the model correctly classified the input as a generated image.</p>
<p>Success! We now have a generator which generates random images and a discriminator which can determine between generated and real images. But wait, what does this have to do with GANs? Well, the goal of a GAN is to train a D model to detect generated images and a G model to generate good generated images (or to fool the D model). What we have done above is the first step in the back and forth process, we have created a D model which can detect the poorly generated images.</p>
<hr>
<p><a id="skip" href=""></a></p>
</section>
</section>
<section id="the-generative-adversarial-network" class="level2">
<h2 class="anchored" data-anchor-id="the-generative-adversarial-network">The Generative Adversarial Network</h2>
<p>Now lets extend this to implement the GAN proper!</p>
<p>From section 3 in the paper, the goal in training our two networks is to:</p>
<p><span class="math display">\[ \min \log(D(x)) \tag{1}\]</span> <span class="math display">\[ \max \log(D(G(z))) \tag{2}\]</span></p>
<p>What the <span class="math inline">\(D()\)</span> and <span class="math inline">\(D(G())\)</span> actually refer to are the outputs of the model, however we do not mean the raw outputs but rather the output after being passed through the loss function.</p>
<p>So in (1) we are dealing with the D model and we are minimising the loss function of the D model. The input x consists of both real and generated images. This means we want the D model to get its classifications between generated and real images correct, we want the D model to become a better classifier.</p>
<p>Then in (2), we are maximising the loss of the discriminator when the inputs are generated images. The input to D is G(z), where z is random noise and G(z) being generated images. So, the loss of D(G(z)) will be high when the discriminator incorrectly classifies the generated images as real images and this is exactly what we want. Now, in practice we flip the labels of the generated images (so they have label 1 instead of 0), this allows us to turn this into a minimisation problem where we want D to classify our generated images as real. The flipping of labels and transformation to a minimisation problem also presents better gradient properties, meaning we get a better model<a href="#5"><sup>5</sup></a>.</p>
<hr>
<section id="the-new-d-and-g-models" class="level3">
<h3 class="anchored" data-anchor-id="the-new-d-and-g-models">The new D and G models</h3>
<p>Now we understand the training regime of our GAN, how do we go about implementing it? Given the nature of our task it lends us well to increase the complexity of our D and G models (they are still MLPs), I will redefine them below. My models here worked well, but feel free to add/remove layers and make your own changes and see how the output changes. Despite the changes to the models, the key differences come in the form of the new training loop.</p>
<p>The D model now has 4 linear layers with dropout and ReLU being applied to layers 1-3 and the output of layer 4 is passed through a Sigmoid function. This scheme arises from the paper where it is stated in section 5:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GenerativeAdversarialNetworks_Goodfellow_files/figure-html/5306fe85-1-image.png" class="img-fluid figure-img"></p>
<figcaption>Architectural hints</figcaption>
</figure>
</div>
<p>Here instead of maxout activations we use ReLU within layers and Sigmoid for the output to ensure comptability with our BCE loss.</p>
<div id="c455b1db" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note the layer size choice is arbitrary in that I have no good reason for choosing it other than</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># that it works. This is why I advise you to play around with, e.g. see what happens if the first layer</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># is nn.Linear(784, 256) etc.</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Discriminator(nn.Module): </span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(<span class="dv">784</span>, <span class="dv">1024</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu1 <span class="op">=</span> nn.ReLU()</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout1 <span class="op">=</span> nn.Dropout(<span class="fl">0.3</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(<span class="dv">1024</span>, <span class="dv">512</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu2 <span class="op">=</span> nn.ReLU()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout2 <span class="op">=</span> nn.Dropout(<span class="fl">0.3</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear3 <span class="op">=</span> nn.Linear(<span class="dv">512</span>, <span class="dv">256</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu3 <span class="op">=</span> nn.ReLU()</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout3 <span class="op">=</span> nn.Dropout(<span class="fl">0.3</span>)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear4 <span class="op">=</span> nn.Linear(<span class="dv">256</span>, <span class="dv">1</span>)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use sigmoid to ensure output is a probability</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigmoid <span class="op">=</span> nn.Sigmoid()</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We transform the input of (batch_size, 28, 28) to (batch_size, 784)</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="dv">784</span>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear1(x)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu1(x)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout1(x)</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear2(x)</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu2(x)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout2(x)</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear3(x)</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu3(x)</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout3(x)</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear4(x)</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.sigmoid(x)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>discriminator <span class="op">=</span> Discriminator().to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Similarily, the G model has 4 linear layers and it takes as input a vector of length 100. The change from input size 1 to 100 is another choice driven by empirical evidence, I’m not entirely sure why it works but my intuition is that as the task is more complex the higher dimensionality aids learning. Mess around with the size and see what happens if you make it smaller or bigger, be aware that it’s usually the case that the input size is smaller than what we are trying to generate (784 in this case).</p>
<div id="34f90d57" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Generator(nn.Module): </span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(<span class="dv">100</span>, <span class="dv">256</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu1 <span class="op">=</span> nn.ReLU()</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(<span class="dv">256</span>, <span class="dv">512</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu2 <span class="op">=</span> nn.ReLU()</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear3 <span class="op">=</span> nn.Linear(<span class="dv">512</span>, <span class="dv">1024</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu3 <span class="op">=</span> nn.ReLU()</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear4 <span class="op">=</span> nn.Linear(<span class="dv">1024</span>, <span class="dv">784</span>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tanh <span class="op">=</span> nn.Tanh()</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear1(x)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu1(x)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear2(x)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu2(x)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear3(x)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu3(x)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear4(x)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.tanh(x)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape the output from (batch_size, 784) to a (batch_size, 28, 28) matrix</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> Generator().to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here we go we’re at the crux of the implementation the training loop, let’s dive right in!</p>
<div id="b5f062d1" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># As before we use the Binary Cross Entropy Loss</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Intialise two optimisers, we use the Adam optimiser as it performs better than Stochastic Gradient Descent,</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># however this will work if you use Stochastic Gradient Descent as in the paper (just replace .Adam with .SGD)</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>optimizer_D <span class="op">=</span> torch.optim.Adam(discriminator.parameters(), lr<span class="op">=</span><span class="fl">0.0001</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>optimizer_G <span class="op">=</span> torch.optim.Adam(generator.parameters(), lr<span class="op">=</span><span class="fl">0.0001</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s look at algorithm 1 again. In algo 1 there are two loops, one lines up with ours the outer loop represents the number of epochs however we do not include the inner loop from algo 1. Our inner loop is simply iterating over our dataset and updating the model in batches. In algo 1 the inner loop has k=1 so in practice we can ignore it. The rest of algo 1 lines up with our code pretty nicely. Let’s break down each step in the algorithm and it’s representation in python:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="GenerativeAdversarialNetworks_Goodfellow_files/figure-html/47c02a4d-1-image-5.png" class="img-fluid figure-img"></p>
<figcaption>Algorithm 1</figcaption>
</figure>
</div>
<p>I have numbered the key parts of Algorithm 1 and will refer to these numbers here for brevity.</p>
<p><strong>1</strong> - Corresponds to the number of training iterations or the number of epochs, so line 4 in the next code block. We run our training loop for 50 epochs, feel free to run for more/less and observe the changes in the generated images.</p>
<p><strong>2</strong> - Is the loop we ignore as k=1.</p>
<p><strong>3</strong> - Here we are simply generating our $ z $ inputs (the noise) for the G model. In line 11 of the code, we generate the tensor of noise inputs and then in line 13 we pass these to the G model to create the generated images.</p>
<p><strong>4</strong> - This step is implemented across a few lines. The algortihm does not use batches, but we do this leads to a small change in our code. Lines 5 and 6 handle the selection of the batch of data. We then need to combine these images with the generated images, the combined images will be our input to the D model. The combining of images is handled in lines 16, 17, 20, 21, 24, 25 and 26.</p>
<p><strong>5</strong> - Lines 29 - 33 handle the updating of the model. We calculate the output of the D model and update it’s parameter appropriately.</p>
<p><strong>6</strong> - We sample a new tensor of noise input in line 36 and generate the images in line 40.</p>
<p><strong>7</strong> - Lines 43 - 46 handle this, we pass the newly generated images to the updated discriminator model and then bump the gradient based on the loss of the discriminator</p>
<p>Thats it! We have implemented the training algorithm from the paper, all that’s left is to run the code and look at out results :)</p>
<div id="bbf59c97" class="cell" data-scrolled="true" data-execution_count="18">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Whenever you see a .to(device) it means we are sending that data to the GPU memory</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co"># We now run the training for 50 epochs</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>):</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, data <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        real_images, _ <span class="op">=</span> data  <span class="co">#&nbsp;We dont care about the MNIST labels we generate a vector of all 1s to</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>                               <span class="co"># simulate them</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        real_images <span class="op">=</span> real_images.to(device)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample from noise and generate the fake images</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        noise_tensor <span class="op">=</span> torch.randn((batch_size, <span class="dv">100</span>)).to(device)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>            gen_images <span class="op">=</span> generator(noise_tensor)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create the real and fake labels</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        gen_labels <span class="op">=</span> torch.zeros((batch_size, <span class="dv">1</span>)).to(device)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        real_labels <span class="op">=</span> torch.ones((batch_size, <span class="dv">1</span>)).to(device)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Concat fake and real images</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        combined_images <span class="op">=</span> torch.cat((real_images, gen_images))</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        combined_labels <span class="op">=</span> torch.cat((real_labels, gen_labels))</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># shuffle the combined batch to prevent the model from learning order</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> torch.randperm(combined_images.size(<span class="dv">0</span>))</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        combined_images <span class="op">=</span> combined_images[indices]</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        combined_labels <span class="op">=</span> combined_labels[indices]</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;First update the D model</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>        discriminator.zero_grad()</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>        d_outputs_combined <span class="op">=</span> discriminator(combined_images)  </span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>        loss_d <span class="op">=</span> criterion(d_outputs_combined, combined_labels)</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>        loss_d.backward()</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        optimizer_D.step()</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate new images for updating G</span></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>        noise_tensor <span class="op">=</span> torch.randn((batch_size, <span class="dv">100</span>)).to(device)</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Next update the G model, </span></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>        generator.zero_grad()</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>        gen_images <span class="op">=</span> generator(noise_tensor)  <span class="co">#&nbsp;Gen new images for training G</span></span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># For generator updating we need the labels for generated images to be 1's to fool the discriminator</span></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">#&nbsp;We do this by just passing the real_labels to the loss function</span></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note we use the D model, the equation in the paper is max log(D(G(z))) and we already have G(z)</span></span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>        d_outputs_generated <span class="op">=</span> discriminator(gen_images)</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>        loss_g <span class="op">=</span> criterion(d_outputs_generated, real_labels)</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>        loss_g.backward()</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>        optimizer_G.step()</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">==</span> batch_size<span class="op">-</span><span class="dv">1</span>:  </span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"g grads"</span>)</span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> name, param <span class="kw">in</span> generator.named_parameters():</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> param.grad <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"No gradient for </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> param.grad.<span class="bu">abs</span>().<span class="bu">sum</span>() <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"Zero gradient for </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(param.grad)</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"d grads"</span>)</span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> name, param <span class="kw">in</span> discriminator.named_parameters():</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> param.grad <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"No gradient for </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> param.grad.<span class="bu">abs</span>().<span class="bu">sum</span>() <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"Zero gradient for </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(param.grad)</span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">: Loss_D: </span><span class="sc">{</span>loss_d<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, Loss_G: </span><span class="sc">{</span>loss_g<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a>            imshow(torchvision.utils.make_grid(gen_images.cpu()))</span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training complete"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>g grads
tensor([[ 1.7906e-05,  6.2438e-04, -2.5892e-05,  ..., -8.0373e-05,
          1.3099e-04,  3.9312e-05],
        [-4.6635e-04,  8.2587e-04, -2.4207e-04,  ..., -3.9348e-05,
         -2.7440e-04,  3.7046e-04],
        [-2.0526e-04,  9.7104e-06, -2.0193e-04,  ..., -4.5834e-04,
         -2.3660e-04, -1.2392e-04],
        ...,
        [-2.9217e-04,  2.0086e-04, -2.5426e-04,  ..., -2.6085e-04,
         -2.1680e-04,  3.8881e-05],
        [-9.8636e-04,  6.2404e-04, -2.4182e-04,  ..., -7.9872e-04,
         -1.1489e-04,  3.1784e-04],
        [-1.1822e-04,  4.1731e-04, -7.3456e-05,  ...,  3.4561e-04,
         -3.0619e-04, -9.9277e-05]], device='mps:0')
tensor([-1.1681e-03, -1.3792e-03, -7.9194e-04, -5.1010e-04, -5.9320e-04,
        -1.0719e-03, -1.5279e-03,  2.0260e-05, -1.2164e-03, -1.8579e-03,
        -6.9648e-04, -2.0512e-03,  2.5979e-04, -6.3421e-04, -6.0130e-04,
        -1.3855e-03, -3.2168e-03, -1.0869e-03, -8.5763e-04, -2.0611e-03,
        -9.4351e-04, -6.5234e-04, -2.2607e-04, -2.5189e-03, -1.2162e-03,
        -1.0867e-03, -2.5491e-03, -2.0672e-03, -9.5341e-04, -1.6248e-03,
        -1.1071e-03, -1.1185e-03, -1.1211e-03, -1.9967e-03, -1.0240e-03,
        -1.0272e-04, -3.6398e-04, -4.9313e-04, -1.1992e-03, -1.5645e-03,
        -1.8401e-03, -6.4351e-04, -1.6682e-03, -2.2963e-03, -1.2637e-03,
        -1.1974e-03, -9.6966e-04, -1.5348e-03, -1.5009e-03, -4.3995e-04,
        -2.0071e-03,  3.3487e-04, -1.9120e-03, -9.9021e-04, -2.1681e-03,
        -2.4307e-03, -1.2014e-03,  3.5994e-04, -8.4145e-04, -7.1390e-04,
        -2.2573e-03, -1.0293e-04,  3.9598e-04, -1.3820e-03, -2.0913e-05,
        -1.3143e-03, -1.0404e-03, -1.0262e-03, -1.2713e-03, -1.2923e-03,
        -8.2040e-04, -1.0028e-03, -1.1069e-03, -2.7243e-03, -1.0782e-03,
        -2.4864e-04, -1.2321e-05, -1.5537e-03, -9.3840e-04, -1.6964e-03,
        -1.6694e-03, -1.8589e-03, -1.3662e-03, -5.5624e-04, -5.5953e-04,
        -2.2077e-03, -1.3525e-03, -2.2070e-03, -1.6371e-03, -1.8140e-03,
        -7.2276e-04, -2.8312e-04, -4.1158e-04, -9.9488e-04, -2.3175e-03,
        -1.9820e-03, -1.5062e-03, -9.8764e-04, -1.3460e-03, -7.9530e-04,
        -4.8565e-04, -9.1880e-04, -1.7002e-03, -1.0016e-03, -4.9125e-04,
        -6.1132e-04, -9.9965e-04, -1.0264e-03, -7.0168e-04, -4.8070e-04,
        -5.7732e-04, -4.0907e-04, -1.4263e-03,  2.9884e-04, -1.4522e-04,
         3.4159e-04, -1.8038e-03, -1.1351e-03, -2.6437e-03, -9.1199e-04,
        -2.0469e-03, -7.9002e-04, -8.0092e-04, -4.6833e-04, -5.1829e-04,
        -1.4095e-03, -1.2088e-03, -4.2598e-04, -1.1696e-03, -6.5573e-04,
        -2.2288e-03, -7.8035e-04, -6.6086e-04, -1.5782e-03, -9.5118e-04,
        -1.5074e-03, -5.6845e-04, -7.1938e-04, -1.1986e-03, -5.1960e-04,
        -5.9730e-04, -1.3024e-03, -7.1347e-04, -8.3470e-04, -1.5687e-03,
        -1.6667e-04, -2.7295e-03, -1.1901e-03, -2.7814e-03, -6.5106e-04,
        -5.0103e-04, -9.0604e-04, -1.5901e-03, -8.4806e-04, -1.2081e-03,
         3.1678e-05, -1.6236e-03, -1.4172e-03, -7.5967e-04, -1.7625e-03,
        -6.9219e-04, -1.6411e-05, -2.1739e-03, -4.7276e-04, -1.5882e-03,
        -3.6915e-04, -8.0224e-04,  2.1227e-04, -1.0894e-03, -8.3533e-04,
        -6.5202e-04, -8.1396e-04,  1.6888e-05, -1.8630e-03, -1.7787e-03,
        -5.7695e-04, -1.0048e-03, -9.5995e-04, -7.3526e-04, -1.1960e-03,
        -4.5185e-04, -1.7963e-03, -1.1455e-03, -1.6152e-03, -1.3266e-03,
        -1.6927e-03, -1.8608e-03, -1.6342e-03, -6.2044e-04, -1.2159e-03,
        -1.2429e-03, -9.7310e-04, -1.2002e-03, -1.4531e-04, -1.5324e-03,
        -1.2787e-03, -1.8850e-03, -6.9366e-04, -8.0429e-04, -4.3287e-04,
        -1.4405e-03, -6.6731e-04, -9.4607e-04, -5.9061e-04, -8.2275e-04,
        -5.4431e-04, -5.9874e-04, -1.4608e-04, -4.4023e-04, -1.3713e-03,
        -3.2234e-04, -1.7021e-03, -8.1392e-04, -1.0755e-03, -1.4081e-03,
        -1.2406e-03, -7.0252e-04,  2.1128e-04, -1.0184e-03, -1.4245e-03,
        -6.8947e-04, -7.4319e-04, -3.0961e-03, -3.4407e-04, -2.7924e-04,
        -6.4369e-04, -1.1271e-03, -1.2985e-03, -5.7376e-04, -1.4354e-03,
        -1.2931e-03, -9.7941e-04, -1.2359e-03, -1.6060e-03, -9.7745e-04,
        -1.7245e-03, -2.4675e-03, -8.7967e-04, -1.2187e-03, -1.0772e-03,
        -9.3730e-04, -2.2844e-03, -1.8982e-03, -6.8865e-04, -1.8446e-03,
        -1.3930e-03, -1.6655e-03, -3.5750e-05, -7.7061e-04, -5.3764e-04,
        -1.9129e-03, -1.4853e-03, -1.2557e-03, -6.8427e-04, -1.9723e-03,
        -1.4908e-03], device='mps:0')
tensor([[-7.3394e-05, -4.0594e-04, -5.1566e-04,  ..., -2.8727e-04,
         -3.1509e-04, -3.4215e-04],
        [ 3.5687e-05,  4.4908e-05,  5.4934e-06,  ...,  5.5635e-05,
          1.6969e-05,  7.9428e-06],
        [-1.3548e-04, -2.4675e-04, -3.0783e-04,  ..., -3.6739e-04,
         -2.2682e-04, -3.3381e-04],
        ...,
        [-2.8062e-04, -7.3703e-04, -7.7704e-04,  ..., -4.9799e-04,
         -7.2080e-04, -5.1490e-04],
        [-3.0565e-04, -6.2493e-04, -8.9576e-04,  ..., -6.9833e-04,
         -6.7597e-04, -6.0137e-04],
        [-3.6773e-04, -9.6033e-04, -1.0027e-03,  ..., -7.0367e-04,
         -9.2536e-04, -7.8207e-04]], device='mps:0')
tensor([-1.2161e-03,  1.6346e-04, -1.1483e-03, -2.3374e-03, -9.0172e-04,
        -8.9193e-04, -1.5360e-03, -1.6348e-03, -1.1155e-03, -4.3369e-04,
        -1.2025e-03, -3.2889e-04, -2.6859e-04, -3.8438e-03,  8.8150e-05,
        -1.5130e-03, -6.0944e-04, -3.2641e-03, -1.3197e-03, -2.1138e-04,
        -5.1900e-04, -1.7341e-04, -3.5489e-04, -1.2887e-03, -1.8103e-03,
        -2.8639e-03,  6.3944e-04, -1.9974e-05, -9.8261e-04, -2.3740e-03,
        -2.4246e-03, -6.3931e-04, -5.7795e-06, -7.5028e-04,  5.0826e-05,
        -2.0910e-04, -4.9668e-04, -4.9776e-04, -1.0374e-03, -5.5062e-04,
        -3.6637e-04, -8.6872e-04, -9.7532e-04, -3.1852e-03, -8.1737e-04,
        -1.3872e-03, -2.3047e-03, -4.0035e-04, -9.8490e-05, -1.4266e-03,
        -2.1849e-04, -1.9297e-03, -1.2575e-03, -1.7963e-03, -4.4272e-04,
        -2.0181e-03, -1.3418e-03, -7.0252e-04, -6.9691e-04, -5.3553e-04,
        -4.7922e-04, -4.7982e-04,  6.5033e-05, -9.2441e-04, -1.4287e-03,
        -1.8687e-03, -8.8856e-05, -1.5737e-03, -8.1860e-04, -2.0250e-03,
        -8.2996e-04, -1.6634e-03, -2.0482e-03, -1.0835e-03, -2.1065e-03,
        -2.1576e-03, -4.9415e-05, -1.0068e-03, -1.2283e-03, -1.8980e-03,
        -4.5073e-04, -1.0488e-03, -2.1883e-03, -1.6696e-03, -7.2037e-04,
        -7.7534e-04, -1.7817e-03, -8.9227e-04, -2.7481e-03, -2.0142e-03,
         7.6688e-04, -2.0011e-03, -3.4309e-04, -8.4625e-04, -1.5591e-03,
        -2.4316e-04, -1.7592e-03, -2.1717e-04, -2.3431e-03, -1.3141e-03,
        -2.6219e-03, -3.5954e-03, -1.3692e-03, -2.6769e-03, -9.3234e-04,
        -1.9559e-03,  9.5706e-05,  2.3104e-04, -1.0029e-03, -5.0487e-04,
        -2.0201e-03, -6.7946e-04, -1.6507e-03, -2.6617e-03, -5.5776e-04,
        -1.6588e-04, -1.6705e-03, -2.5074e-03, -1.2561e-03, -1.6096e-03,
        -7.7519e-04, -1.3892e-03, -1.0058e-03, -1.0771e-03, -1.3815e-03,
        -1.3953e-04,  3.6623e-06, -1.8260e-03, -3.3199e-04,  1.5001e-04,
        -3.3116e-04, -1.4697e-03, -3.4354e-03, -1.5476e-03,  1.0876e-04,
         1.8299e-05, -9.9397e-04, -2.1207e-03, -2.1347e-03, -4.4299e-04,
        -7.1496e-04, -1.7295e-03, -2.1502e-03, -1.2888e-03, -6.7538e-04,
        -5.6249e-04, -1.2261e-03, -4.3728e-04, -5.2149e-06, -2.7064e-03,
        -3.0791e-04, -2.8350e-03, -8.3714e-04, -2.1977e-03, -1.8777e-03,
        -1.2701e-03, -1.4215e-03, -2.6878e-03, -1.6878e-03, -1.1436e-03,
        -6.4636e-05, -1.5390e-03, -1.4001e-03, -1.1705e-03, -8.7714e-04,
        -2.2793e-03,  3.4001e-05, -1.5786e-03, -5.7137e-05, -1.5639e-03,
        -7.1038e-04, -1.1935e-03, -6.3197e-05,  1.2355e-04, -1.2530e-03,
        -5.7234e-04, -1.5135e-04, -1.5956e-03, -1.2822e-03, -3.0164e-03,
        -2.5339e-03, -1.8966e-03, -2.8903e-03, -1.4977e-03, -9.6947e-04,
        -4.6520e-04, -2.3562e-03, -1.2459e-03, -3.2485e-04, -1.4924e-03,
        -1.2418e-05, -5.6443e-04,  2.1972e-04, -7.4826e-04,  3.4921e-04,
        -2.4248e-03, -1.0614e-04, -1.7011e-03, -1.1977e-03, -1.0497e-04,
        -2.3846e-03, -1.4314e-03, -1.6620e-03, -5.5595e-05, -1.6680e-03,
        -6.6361e-04, -8.0647e-04, -2.0969e-03, -1.5045e-03, -4.0163e-04,
        -2.1777e-04, -8.1211e-04, -4.5922e-04, -2.5022e-03, -1.1124e-03,
        -1.3492e-03, -2.0662e-03, -1.5954e-04, -3.9444e-04, -1.0028e-03,
        -3.7047e-04, -1.4292e-03, -2.9131e-03, -1.6559e-04, -7.3342e-04,
        -1.4092e-03, -3.0485e-03,  1.0056e-04, -7.4260e-04, -7.3150e-04,
        -2.0136e-03, -2.8670e-04, -1.0269e-03, -1.0235e-03, -9.0807e-04,
        -2.9040e-05, -2.8382e-04, -1.6729e-04, -1.0182e-03, -7.5755e-04,
        -1.6266e-03, -5.1589e-04, -3.2993e-04,  2.2572e-05, -1.9400e-03,
         6.5632e-05, -4.9477e-04, -5.0132e-04, -9.4898e-04, -1.3598e-03,
        -8.5796e-04, -1.6679e-03,  3.6422e-04,  4.1818e-05, -2.1207e-03,
        -5.4129e-05, -2.4553e-03, -1.7901e-03, -1.5409e-05, -8.2122e-04,
        -4.4182e-04, -7.6582e-04, -1.7679e-04, -5.5666e-04, -9.6126e-04,
        -2.9159e-03, -6.7313e-04, -1.7369e-03, -6.5990e-05, -2.8101e-03,
        -2.5552e-03, -1.1911e-03, -2.1578e-03, -8.8176e-04, -4.6077e-04,
         1.2041e-04, -2.0575e-03, -2.1458e-03, -2.4331e-04, -4.0457e-04,
        -1.9383e-03, -1.6812e-03, -1.3872e-03, -1.9438e-04, -1.4132e-03,
        -2.6978e-04, -3.4086e-04, -2.7757e-03, -1.0485e-03, -9.8250e-04,
        -2.6513e-03, -1.4640e-03, -9.0122e-04, -3.0159e-03, -4.2357e-04,
        -1.0503e-03, -1.3489e-03, -6.8860e-04, -5.1432e-04, -7.9035e-05,
        -6.4441e-04, -1.6866e-03, -1.1877e-03, -1.1504e-03, -4.4389e-03,
        -3.6984e-04, -2.5714e-04, -4.1235e-04, -5.8451e-04, -2.6803e-04,
        -3.8619e-03, -4.9522e-04, -2.7996e-03, -2.0488e-03, -2.0995e-03,
        -6.1009e-04, -1.1142e-03, -1.9536e-03, -1.5178e-03, -3.8405e-05,
        -9.4571e-04, -1.7744e-03, -2.0557e-03, -2.2905e-03, -1.3917e-03,
        -2.2488e-04, -2.4508e-03, -4.9906e-03, -1.5616e-03, -2.2928e-04,
        -6.2384e-04, -7.1487e-04, -2.5431e-03, -2.6191e-03, -2.8806e-05,
        -2.3780e-03, -6.4164e-04, -1.0714e-03, -1.5708e-03, -1.3268e-03,
        -2.2533e-03, -2.8362e-04, -2.8422e-04, -2.8306e-03, -1.3564e-03,
        -1.5373e-04, -2.8742e-03, -2.6290e-03, -1.3725e-03,  0.0000e+00,
        -1.7628e-03,  6.4165e-05, -1.2560e-03, -1.6069e-03, -5.3721e-04,
        -1.7960e-03, -1.2918e-05, -7.3018e-04, -2.9112e-04, -4.2916e-04,
        -9.9985e-05, -1.0215e-03, -1.7333e-03, -1.6335e-04, -6.7242e-04,
        -6.0089e-04, -9.5845e-04, -1.9218e-03, -6.4273e-04, -2.2194e-04,
        -1.1209e-03, -6.0212e-04, -3.8477e-05, -6.1321e-04, -1.1931e-03,
        -9.3838e-04, -1.0761e-03, -7.1152e-05, -7.2097e-04, -6.4724e-04,
        -8.2816e-04, -3.3976e-04, -1.2963e-03, -8.3756e-04,  4.6115e-06,
         7.7543e-05, -1.6366e-03, -1.7027e-03, -4.5320e-05, -1.7714e-03,
        -1.4486e-04, -2.5389e-04, -2.1578e-03, -1.0462e-03, -1.5208e-03,
        -1.8125e-04, -1.0996e-04, -2.6306e-03,  7.7771e-05, -2.7298e-03,
        -4.5614e-05, -1.7288e-03, -2.5428e-03,  0.0000e+00, -1.5052e-03,
        -3.6694e-03,  4.5480e-04, -3.1416e-03, -6.2046e-05, -9.9076e-04,
         2.9589e-04, -4.6397e-04, -1.4106e-03, -1.9782e-03, -3.4914e-03,
         2.9450e-05, -1.9147e-03, -2.6863e-03, -2.3269e-03, -2.3784e-03,
        -3.1991e-03, -4.9166e-04, -3.1484e-04,  1.1692e-04, -1.8805e-03,
        -1.0019e-03, -5.6115e-04, -2.3255e-03, -9.7705e-04, -4.6794e-04,
        -3.3320e-04, -8.7003e-04, -2.9545e-04, -1.6985e-03, -1.8548e-03,
        -8.1979e-04,  1.3872e-04, -6.7655e-04, -3.6480e-04, -2.2779e-03,
        -2.0094e-03, -9.5291e-04,  8.7020e-05, -1.9531e-03, -6.2212e-04,
        -2.1871e-03, -2.2574e-04, -1.5281e-03,  1.3011e-04, -1.1937e-03,
        -2.1476e-04,  3.5228e-04, -2.5178e-03, -1.6009e-03, -1.8853e-03,
        -1.7841e-04, -5.3392e-04, -5.3562e-04, -1.5907e-04, -1.3082e-03,
        -2.5165e-03,  2.4746e-05,  1.5231e-04,  2.0794e-04, -9.5217e-04,
        -2.4814e-04, -1.2028e-03, -7.9265e-04, -2.1310e-03,  6.3536e-05,
         2.5965e-04, -5.8787e-04, -8.0431e-04, -1.8141e-03, -2.4499e-03,
        -1.4120e-03, -6.7719e-04, -2.1873e-03, -1.8111e-03, -2.3545e-03,
         1.5871e-04, -8.9973e-04, -3.8451e-03, -1.7661e-03, -3.5763e-04,
         4.7394e-06, -7.8325e-04, -2.0703e-03, -1.4420e-03,  1.4245e-04,
        -1.6040e-03, -2.0823e-03, -7.1423e-05, -5.6353e-04, -1.3748e-03,
         7.0101e-04, -4.2734e-04, -3.5506e-03, -3.2201e-03, -6.1884e-04,
        -1.8687e-03, -1.0754e-03, -1.2523e-03, -3.6008e-03, -1.0791e-03,
        -1.1077e-03, -2.6271e-04, -4.4922e-04, -8.7014e-05, -2.2718e-03,
        -2.5019e-03, -2.9792e-03], device='mps:0')
tensor([[-5.9989e-04, -5.8212e-04, -1.9755e-04,  ..., -6.6139e-04,
         -2.2780e-04, -6.4035e-04],
        [ 1.9336e-04,  2.5026e-04,  7.9822e-05,  ...,  2.9955e-04,
          8.7650e-05,  2.6766e-04],
        [-2.1228e-04, -2.1531e-04, -7.4877e-05,  ..., -2.3819e-04,
         -1.1891e-04, -2.0334e-04],
        ...,
        [ 6.0533e-05,  1.1259e-04,  4.2132e-05,  ...,  1.3562e-04,
          2.1282e-05,  1.0616e-04],
        [-8.0726e-04, -7.8875e-04, -2.8999e-04,  ..., -9.3927e-04,
         -3.8508e-04, -9.0217e-04],
        [-7.0185e-04, -6.8420e-04, -2.4402e-04,  ..., -8.2128e-04,
         -2.7714e-04, -7.0899e-04]], device='mps:0')
tensor([-0.0020,  0.0008, -0.0007,  ...,  0.0003, -0.0028, -0.0023],
       device='mps:0')
tensor([[ 1.2960e-03,  1.9469e-03,  4.7409e-04,  ...,  1.2901e-03,
          7.5468e-04,  7.0213e-04],
        [ 6.5224e-04,  1.0036e-03,  2.5694e-04,  ...,  6.4733e-04,
          3.9498e-04,  3.4164e-04],
        [ 4.4562e-04,  6.6165e-04,  1.7454e-04,  ...,  4.4328e-04,
          2.5675e-04,  2.1654e-04],
        ...,
        [ 1.9700e-04,  2.8550e-04,  6.8193e-05,  ...,  1.5563e-04,
          8.9528e-05,  9.5728e-05],
        [ 1.0831e-03,  1.6125e-03,  3.2630e-04,  ...,  1.0944e-03,
          6.3199e-04,  6.0597e-04],
        [-1.2499e-04, -1.6354e-04, -1.9219e-05,  ..., -1.5894e-04,
         -6.4944e-05, -5.9074e-05]], device='mps:0')
tensor([ 4.2709e-03,  2.1740e-03,  1.4671e-03,  1.7210e-03, -5.6185e-04,
         6.9836e-04,  3.3163e-03,  7.4250e-04, -2.3411e-04,  6.6064e-04,
         7.6310e-06, -1.9882e-03,  1.5200e-03,  2.7224e-03, -1.9806e-04,
         4.5154e-03,  1.8857e-03,  7.0884e-04,  4.3854e-03,  2.7646e-03,
         2.2672e-03,  7.7683e-04,  1.3764e-03,  1.4883e-03,  1.2080e-03,
         4.4390e-04,  2.6894e-03,  2.5621e-03,  9.7219e-04,  3.0863e-04,
        -9.6531e-04,  2.9413e-03,  1.3704e-03,  6.1688e-05,  2.4481e-03,
         4.7804e-03,  4.3629e-03,  7.4721e-04,  4.7234e-03,  2.0272e-03,
         3.8492e-04, -7.1252e-04, -4.4281e-04,  1.8638e-03,  2.5729e-03,
         2.0084e-03,  3.4275e-03,  3.3907e-03,  2.0150e-03, -2.0909e-04,
         3.1609e-03,  3.8717e-03,  8.7756e-04,  1.9104e-03,  1.9996e-03,
         5.1181e-03,  1.0460e-04,  1.6731e-03,  3.6597e-03,  2.5892e-04,
         1.6959e-03, -1.1784e-03,  4.1513e-03,  8.1797e-04, -1.4414e-03,
         1.8805e-03,  5.0801e-03,  3.7493e-03,  2.3312e-03, -4.0968e-04,
         1.0291e-03, -1.1127e-03, -1.0903e-04,  5.0328e-04,  7.7743e-04,
        -4.1028e-04,  6.2063e-03,  1.1046e-03,  6.0174e-04, -2.6236e-04,
         1.9832e-03,  4.9230e-03,  5.9470e-04,  1.4954e-03, -3.1340e-04,
        -2.2424e-04,  8.4281e-04,  1.7115e-03,  6.0539e-03,  1.2985e-03,
         2.8733e-03,  4.7074e-03,  2.9699e-03,  2.0065e-03,  3.3947e-03,
         1.4141e-04,  2.2501e-03,  1.5113e-03,  1.5467e-03,  3.5268e-03,
         8.5723e-04,  1.0128e-03, -6.0138e-04,  1.6849e-03,  2.2865e-03,
         2.1632e-03,  5.3992e-04,  1.3081e-03,  1.9700e-03,  2.2159e-03,
         1.7666e-03,  5.1051e-04,  2.1946e-03,  9.6254e-04, -4.4347e-04,
         1.4370e-03,  1.3170e-03, -2.8400e-04,  3.2116e-03, -8.2716e-04,
         4.6318e-03,  1.6211e-03,  1.9115e-03,  1.7300e-03,  3.1608e-04,
        -1.8370e-03,  3.5392e-04,  5.6595e-03,  2.2519e-04,  3.9512e-03,
        -8.3550e-04,  1.4102e-03,  5.4532e-03,  3.4751e-03,  1.0092e-03,
         4.7095e-04,  3.6465e-03,  3.5985e-03,  2.9438e-03,  5.6565e-04,
         3.9175e-04,  1.3580e-03,  3.7820e-03,  2.1617e-03,  5.1572e-03,
         4.4082e-03,  4.5676e-03, -9.3182e-05,  5.9336e-03,  8.2333e-04,
         3.7928e-03,  2.0392e-03,  4.9131e-03,  3.9454e-03,  3.1222e-03,
         4.2381e-03,  3.6913e-03,  1.0095e-03,  1.4001e-04, -1.5871e-03,
         3.3489e-03,  1.0335e-03,  1.3264e-03,  3.8549e-03,  7.9395e-04,
         3.9449e-03,  2.1191e-03,  4.1655e-03,  4.2655e-03,  5.4247e-04,
         8.0160e-04,  1.6482e-03,  4.4412e-04,  1.3284e-03, -7.2068e-04,
         1.1949e-03,  3.1989e-04, -4.0329e-04, -1.2349e-03,  9.2894e-04,
         1.8827e-03, -2.0017e-03,  2.2627e-03,  8.2166e-04, -1.2565e-03,
        -2.4230e-03, -2.0230e-03,  3.4426e-03, -3.6679e-05,  7.1650e-03,
         2.6187e-03,  4.3513e-04, -2.8536e-04,  1.1555e-03,  1.1780e-03,
         3.0290e-03,  1.7069e-03,  5.3583e-03,  8.0800e-04,  3.4453e-04,
         6.1737e-04,  2.7934e-04,  3.3045e-04, -1.6881e-03,  2.6682e-03,
         2.0132e-03,  5.3994e-04, -4.5037e-04,  2.3838e-03,  4.2724e-03,
        -3.6011e-03, -2.3771e-03,  2.1466e-03, -1.5138e-05,  6.4651e-04,
         4.1882e-03,  1.9041e-03, -1.3852e-03,  6.6631e-04,  5.1774e-03,
         5.7026e-04,  1.8315e-03,  1.0370e-04,  1.3931e-03,  2.4653e-03,
         4.3024e-03,  1.8917e-03,  1.0503e-03,  1.7416e-03, -1.9145e-03,
         1.2212e-03, -1.3770e-03, -2.4540e-04,  1.6112e-03, -2.4152e-03,
         1.2816e-03, -2.1090e-03, -4.2900e-05,  4.5179e-03, -2.8073e-03,
         8.3754e-04,  1.7150e-03, -9.0452e-04,  1.1354e-03,  6.4910e-04,
         1.3488e-03,  1.9380e-04,  1.7601e-03,  1.9392e-03,  3.0821e-03,
         2.5249e-03,  2.2732e-05,  1.8532e-03,  6.1860e-04,  2.1427e-03,
         4.7713e-03,  3.0251e-03,  3.0131e-03,  1.3498e-03,  7.7993e-04,
         1.1662e-03, -3.3951e-04, -1.5834e-03,  2.4994e-03,  2.6457e-03,
        -1.0430e-03, -2.9236e-03, -3.7859e-03,  3.3376e-03,  3.6410e-03,
        -1.6255e-03,  1.1740e-03, -6.0383e-04,  2.4499e-04,  1.5724e-03,
         2.3568e-04,  3.3797e-03,  7.4682e-04,  2.1599e-03, -9.9103e-05,
         1.9540e-04, -3.1529e-04,  3.5644e-04,  4.9482e-04,  2.4359e-03,
         1.5869e-03, -1.8944e-04,  2.0588e-03, -1.0215e-03, -1.0961e-04,
         3.8264e-05, -4.8309e-04,  3.4435e-03,  7.5063e-04,  4.1464e-05,
         3.1820e-03,  1.8904e-03,  1.2953e-03,  1.1290e-04, -1.5795e-03,
        -1.4649e-03, -1.5619e-03,  3.1935e-03, -5.8408e-04, -3.1546e-04,
         9.2363e-04,  7.6563e-04,  2.3838e-03,  2.0454e-03,  1.7043e-03,
         5.8937e-03,  4.3685e-03, -1.0504e-04,  2.6345e-03,  5.8544e-03,
        -2.0242e-03,  1.1514e-03,  7.7827e-04, -5.8633e-06,  7.3698e-04,
         3.9515e-03,  4.4914e-03,  1.2061e-03,  4.1427e-03, -2.4376e-03,
        -4.3394e-03,  2.6513e-03,  1.8962e-03, -1.8452e-03, -1.9048e-05,
         1.6684e-03,  2.9791e-03, -3.2270e-04,  8.3800e-04,  4.7653e-03,
         3.3167e-03,  1.4774e-03, -3.6556e-04,  3.5520e-03,  1.4504e-03,
         8.8520e-04,  5.8012e-03, -2.0368e-03,  2.0979e-03, -1.0237e-03,
        -1.8832e-03,  3.4663e-03,  2.8938e-03,  1.3259e-03,  3.7669e-03,
         4.4091e-03, -5.7612e-04,  9.0452e-04,  1.3838e-03, -1.7375e-03,
        -1.3022e-03,  1.2673e-03,  3.6051e-03,  1.2485e-03,  6.4649e-03,
         3.1382e-03,  3.8414e-03,  1.3162e-03,  2.5433e-03,  1.8726e-03,
         4.8445e-04,  1.5823e-03,  1.3112e-03,  1.7622e-03,  1.4949e-04,
         1.1472e-03, -6.5442e-04,  5.2494e-04, -1.5742e-03, -1.2479e-03,
         2.1486e-03, -8.6964e-04,  2.5048e-03,  1.6026e-03, -1.4292e-03,
        -2.4673e-03,  3.4401e-03, -8.7888e-05,  7.5801e-05,  2.4694e-04,
         4.9416e-03,  1.8654e-03, -7.4359e-05,  1.9026e-03,  3.7255e-05,
        -1.8169e-03,  2.9864e-03,  4.3087e-03,  1.1547e-03,  1.2353e-03,
         1.4401e-03,  5.3333e-03,  2.0736e-03,  3.2393e-03, -1.3875e-04,
         1.8067e-03, -4.6193e-04, -2.6522e-04, -5.3020e-04, -8.6484e-04,
         3.1315e-03, -4.4284e-04,  1.5799e-03, -1.8975e-03, -1.2573e-03,
        -4.1714e-03, -3.8484e-04, -3.4258e-04,  2.5431e-04,  3.2532e-03,
         7.7858e-04,  3.6834e-04,  3.5742e-04, -2.3017e-04,  1.4476e-03,
         3.6562e-04,  3.2059e-05,  3.5532e-03,  4.3928e-03,  5.6090e-03,
         5.8321e-03,  1.0773e-03, -1.9976e-03, -2.5911e-03,  2.9826e-03,
        -1.9676e-04,  2.1872e-03,  1.9450e-03,  5.4283e-04,  3.3095e-03,
         1.1581e-03, -4.2798e-04,  2.0804e-03,  2.7684e-05,  5.1038e-03,
        -1.9746e-04,  3.4549e-03, -8.6117e-04,  3.7028e-03,  1.3298e-03,
         1.8751e-03,  9.9283e-05,  1.4992e-04,  1.2205e-03, -5.5023e-04,
         1.3047e-03,  5.4682e-04,  9.8280e-04, -7.4527e-04,  4.1806e-04,
         6.0011e-04,  2.9815e-03,  1.3312e-03,  1.7885e-03, -8.0601e-04,
         2.6053e-03, -7.8870e-04,  1.4314e-03, -8.2935e-04, -3.6161e-03,
         5.0520e-04,  4.0314e-03,  6.6915e-04, -1.6421e-03,  4.0554e-03,
         1.9491e-03,  1.4215e-03,  1.2256e-04,  5.0164e-03,  2.4203e-03,
        -3.3917e-04,  1.0808e-03,  6.6209e-04, -5.4590e-04, -5.1423e-04,
         2.2270e-03,  1.2273e-03,  2.1345e-03,  3.6532e-03, -1.2831e-04,
         7.6319e-05,  1.3777e-03,  2.3613e-04, -2.5446e-03,  3.9338e-04,
        -1.0680e-03,  7.2482e-04,  1.1660e-03,  5.3094e-04,  1.9781e-03,
        -8.5035e-04,  2.9947e-03,  1.6217e-04,  1.6012e-03, -4.3866e-04,
         7.3741e-04, -1.0498e-03, -1.8687e-04,  2.8572e-03,  6.2271e-04,
         3.7326e-03,  3.6556e-03,  1.2773e-03,  1.2282e-03,  3.3666e-03,
         1.9208e-03,  5.2119e-04, -1.7758e-04, -3.3342e-04, -1.8702e-03,
         1.8266e-03, -2.1304e-03, -2.4180e-04,  2.2172e-03, -4.2609e-03,
        -2.5066e-03,  7.6189e-04, -1.0200e-04, -1.9123e-03,  3.4142e-03,
         2.1702e-03,  4.6687e-03, -9.7522e-04,  3.0216e-03,  3.8424e-03,
         2.0004e-03,  1.4919e-03,  1.9976e-03,  9.0825e-05,  1.4123e-03,
        -6.5147e-04,  5.8248e-04,  1.2700e-03,  6.4187e-04,  8.3666e-04,
         4.2104e-04,  1.7665e-04,  3.9859e-03,  3.6208e-03, -1.4133e-03,
        -2.1209e-03, -2.1226e-03,  3.3529e-03,  4.3006e-03, -1.5647e-03,
         8.2428e-05, -2.9227e-05,  4.3454e-04,  2.2029e-03,  3.6477e-03,
        -2.0556e-04,  8.1633e-04,  2.3668e-03,  2.0388e-03,  2.4611e-03,
         1.0032e-03,  5.0130e-03,  7.0216e-04,  3.5583e-03,  1.0158e-03,
         1.7185e-03, -3.7751e-04,  9.1409e-04, -7.1622e-05,  3.3561e-03,
        -1.6080e-03, -1.6398e-03,  1.3224e-03,  3.7312e-03,  1.1679e-03,
         3.0714e-03, -4.1583e-04,  2.4512e-04, -2.6589e-03, -1.2960e-03,
        -2.6394e-03,  6.4156e-04,  5.1069e-03, -8.2052e-04,  1.6372e-03,
         7.5428e-04, -1.1965e-03, -1.6961e-03,  1.4210e-03,  1.9848e-03,
         5.9280e-04,  1.5321e-03,  1.5128e-03,  1.2018e-03,  4.9304e-04,
        -2.8865e-03,  2.4341e-03,  2.3623e-03,  3.5101e-03,  2.2993e-03,
         4.0382e-04, -1.1137e-03,  8.4881e-04, -3.8008e-03, -2.9840e-03,
        -2.5587e-03,  3.6516e-04, -2.7034e-04,  2.1489e-03,  2.2118e-03,
         1.0076e-03,  3.5530e-04,  2.5196e-03,  3.7937e-03,  6.5176e-04,
         6.7777e-04,  5.6915e-04, -2.1022e-03,  2.4597e-04,  1.5965e-03,
         4.0258e-03,  2.7571e-04,  1.4090e-03, -8.8601e-04, -5.4937e-04,
        -1.0352e-03, -7.7331e-05, -3.2277e-04,  7.4888e-04,  1.7635e-03,
         2.6947e-03, -2.5674e-04, -6.6704e-04,  1.3550e-03,  4.6897e-04,
         1.3852e-03, -2.6163e-04, -6.9975e-04,  1.3787e-03,  6.2182e-04,
         2.0834e-03, -1.0086e-03,  6.9201e-03,  3.3265e-03,  4.4190e-03,
         5.3096e-03, -1.8417e-03, -7.0444e-04,  3.0574e-03,  4.1074e-03,
        -1.5722e-03,  2.7195e-03,  3.4753e-03, -1.1634e-03,  1.0332e-03,
        -1.8250e-03, -2.2607e-03, -2.2315e-03,  1.5812e-04, -4.8911e-04,
         4.0481e-03, -5.4977e-04,  1.5108e-03,  1.4380e-04,  9.2349e-04,
         5.3257e-03, -1.7364e-03, -6.3665e-04,  1.8364e-03,  4.2593e-03,
         5.2209e-04, -4.9864e-06, -2.0621e-03,  4.1189e-03, -8.7437e-04,
         2.5050e-03,  3.0995e-03,  8.8259e-04,  3.9576e-03,  3.7981e-03,
         9.3032e-05,  2.2031e-03,  2.6519e-03,  3.9946e-03, -7.0711e-05,
         3.7501e-04,  2.5507e-03,  2.3154e-03,  1.8575e-04,  1.4560e-03,
         5.3900e-03,  4.7681e-04, -1.0267e-04,  1.7432e-03,  1.3478e-03,
         1.8096e-03,  6.3613e-03,  4.6818e-03, -6.3766e-04,  7.6275e-04,
         1.0440e-03,  5.1410e-04,  1.3930e-03,  1.8343e-03,  4.0972e-03,
         2.0035e-03,  5.6651e-04,  3.7048e-03, -9.8833e-04,  1.1240e-03,
         1.2003e-03,  1.2328e-03,  6.2902e-04,  9.0489e-04,  7.4238e-04,
         2.2557e-03,  2.7401e-03,  5.4656e-03,  4.4597e-04,  2.7257e-03,
         4.1918e-03,  7.4885e-05,  2.3935e-03,  4.5293e-03,  2.6551e-03,
        -6.9655e-04,  8.3193e-04,  7.6939e-04,  1.0971e-03,  2.3427e-03,
         1.8746e-03,  2.3239e-03,  2.5850e-03,  6.2610e-04, -5.0997e-04,
         1.8017e-03,  2.3337e-03,  1.5520e-03,  1.6539e-03,  2.3285e-03,
        -3.6299e-04,  4.8669e-03,  2.5862e-04,  2.5668e-03,  6.1828e-03,
         1.4741e-03,  2.0452e-04,  3.0180e-03,  3.8832e-03,  5.2142e-05,
         4.7867e-05,  6.3593e-04,  1.3391e-03,  1.4942e-03, -8.9371e-04,
        -1.2983e-03,  6.8469e-04,  1.1627e-03,  1.5503e-03, -3.2816e-04,
         1.3796e-03,  3.4370e-03,  9.9429e-04,  8.1375e-04,  5.7465e-03,
         3.2289e-03,  1.9019e-03,  2.6459e-03,  6.8624e-04,  6.5221e-04,
        -8.7707e-04,  7.7466e-04, -6.7480e-04,  5.4987e-03,  1.1362e-03,
         3.6543e-03, -3.6875e-04,  3.9262e-03,  2.4376e-03,  1.4412e-03,
         3.1480e-04,  5.9453e-04,  3.5393e-03, -3.8769e-04], device='mps:0')
d grads
tensor([[ 2.6715e-04,  4.7620e-04,  8.2526e-04,  ...,  6.3638e-04,
          2.4945e-04,  6.4586e-04],
        [ 2.6010e-04,  2.1903e-04,  2.5900e-04,  ...,  2.5720e-04,
          2.2105e-04,  2.9242e-04],
        [ 1.8123e-04,  1.4882e-04,  1.6013e-04,  ...,  1.5821e-04,
          1.4772e-04,  1.9023e-04],
        ...,
        [-2.2214e-04, -4.6975e-04, -9.7230e-04,  ..., -6.8067e-04,
         -1.2103e-04, -7.3944e-04],
        [ 1.9377e-06,  2.9227e-05,  1.0847e-05,  ...,  1.6090e-05,
         -1.3056e-05,  3.3091e-07],
        [-6.2218e-05, -1.5892e-04, -3.6689e-04,  ..., -2.6682e-04,
         -2.4185e-05, -2.6857e-04]], device='mps:0')
tensor([-1.1949e-03, -3.1642e-04, -9.9085e-05,  ...,  1.5807e-03,
        -4.0283e-05,  6.7784e-04], device='mps:0')
tensor([[ 3.1051e-05, -1.0553e-04, -5.5401e-05,  ..., -7.2156e-06,
         -1.8858e-04, -3.6795e-05],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.0801e-04, -1.8465e-04, -1.9409e-04,  ...,  6.6565e-06,
          1.2722e-04, -3.9124e-05],
        ...,
        [-8.5790e-06,  2.0587e-05, -2.8986e-05,  ...,  6.4772e-07,
          2.1134e-04,  1.7465e-04],
        [-7.2462e-05, -1.0519e-04, -3.5333e-06,  ..., -1.0507e-05,
         -1.9653e-04, -1.0412e-04],
        [-5.3113e-04, -3.4219e-04, -1.5257e-04,  ..., -1.0358e-04,
         -1.4832e-04, -8.1940e-04]], device='mps:0')
tensor([-1.8553e-04,  0.0000e+00,  4.4440e-05,  0.0000e+00,  1.0704e-03,
         5.1290e-06, -1.3726e-03,  0.0000e+00,  5.7630e-04,  0.0000e+00,
         3.8973e-04, -3.7481e-03,  1.1741e-03, -1.0668e-04,  3.1683e-05,
         9.2646e-05, -2.9550e-05,  7.3610e-04,  5.8331e-04,  6.5570e-04,
        -1.6570e-03,  3.7596e-04, -1.0925e-04, -1.6879e-04, -1.7576e-06,
         0.0000e+00, -7.3190e-05, -1.7047e-04,  1.2364e-03,  3.4192e-04,
        -1.9058e-03, -3.7153e-04,  9.1705e-05, -4.1357e-04,  1.1495e-03,
         1.0047e-03, -2.3857e-03, -3.5460e-03, -2.3661e-03, -3.6272e-04,
         0.0000e+00, -8.2209e-04, -4.3023e-04, -1.1908e-03,  0.0000e+00,
         2.4827e-03,  0.0000e+00,  4.8395e-04,  1.2851e-03,  0.0000e+00,
        -1.7076e-03, -2.0297e-03, -1.2572e-03,  5.1658e-04,  4.5917e-04,
        -1.9251e-03,  5.8988e-04,  0.0000e+00, -1.1440e-03, -4.1632e-04,
        -1.3544e-05,  0.0000e+00, -1.6332e-03,  0.0000e+00,  6.2044e-05,
        -4.6403e-05, -4.0674e-04,  1.2594e-03, -4.9259e-05,  2.4095e-03,
         1.3589e-03,  8.8914e-04,  5.3723e-05, -2.9535e-04,  3.5658e-03,
         1.1185e-03,  1.1948e-03,  0.0000e+00, -8.6312e-04,  1.3405e-03,
        -1.4546e-03,  0.0000e+00, -3.1898e-04,  0.0000e+00, -1.8557e-03,
         0.0000e+00, -6.0832e-04, -3.8788e-03,  9.6692e-05,  6.6258e-04,
        -4.6003e-03,  3.3349e-05, -2.0839e-04,  1.0606e-04, -7.6136e-04,
         0.0000e+00,  0.0000e+00,  0.0000e+00,  5.5308e-04, -2.1104e-04,
        -4.2217e-03, -2.1725e-03,  2.3741e-03, -5.9185e-04,  2.8382e-04,
         1.1391e-03,  4.6584e-05, -8.8905e-04,  6.1419e-04, -6.3988e-04,
        -3.2919e-03,  3.7288e-04, -2.3401e-03, -1.2347e-03,  0.0000e+00,
         1.2820e-03,  0.0000e+00,  9.0343e-06, -2.4680e-03,  0.0000e+00,
        -4.3546e-05, -4.5969e-05, -9.7634e-04,  0.0000e+00, -9.6188e-04,
         2.5568e-03,  1.1079e-03,  9.3799e-05,  3.8518e-04, -1.3296e-03,
        -7.0393e-04, -3.4316e-03, -2.2239e-03,  3.4999e-05, -7.9039e-04,
         2.0007e-04, -5.2138e-04, -1.2188e-03, -1.3081e-03,  4.5973e-04,
        -1.4414e-04, -2.0714e-04,  1.0293e-05,  5.2472e-03, -2.8650e-04,
         2.0283e-04, -1.5613e-03,  5.7552e-03,  0.0000e+00, -2.1720e-03,
        -1.0250e-03,  7.3758e-04, -1.0664e-03,  0.0000e+00,  5.0054e-04,
         0.0000e+00,  4.7983e-04,  9.6509e-04,  1.2647e-03, -8.9849e-04,
        -7.8995e-06,  2.7109e-03,  1.1483e-03,  4.6369e-05,  4.9504e-04,
         2.0393e-03, -3.3035e-04,  4.2790e-03,  4.9566e-04, -5.9997e-03,
         0.0000e+00,  7.3079e-05, -3.7843e-03, -1.4559e-03, -2.1676e-03,
         3.9159e-04,  2.5668e-04, -4.2857e-05,  2.9959e-05,  1.2154e-05,
        -1.3500e-04, -8.2387e-03, -1.4922e-03,  1.0984e-04, -1.6130e-04,
        -8.7651e-06,  3.4332e-03, -5.5385e-04, -6.0059e-04, -2.8314e-04,
         1.8969e-03, -7.7641e-04,  5.2355e-05,  7.6582e-04, -2.8007e-03,
         4.5260e-04,  4.5048e-03, -8.9276e-05,  1.0965e-03,  0.0000e+00,
        -9.9673e-04, -2.3588e-04, -9.8655e-04,  6.9254e-04,  5.8377e-04,
         4.5799e-04,  1.2649e-04,  7.7706e-05,  0.0000e+00,  7.4527e-04,
        -3.4537e-03,  3.1587e-03,  2.0258e-04,  1.8231e-04,  0.0000e+00,
         0.0000e+00, -1.4492e-04,  5.0065e-04,  0.0000e+00, -6.1302e-04,
         0.0000e+00,  1.3883e-03, -3.0948e-03,  3.4751e-03,  1.9276e-03,
         3.5650e-04, -1.4245e-03, -5.6188e-06, -6.1577e-04, -5.1981e-04,
         0.0000e+00, -7.8985e-04,  0.0000e+00,  9.3315e-04, -1.7848e-05,
         0.0000e+00, -5.1418e-05, -2.0592e-04,  1.6323e-04, -2.4667e-03,
        -8.0850e-05, -3.5603e-04,  8.9417e-04,  0.0000e+00,  1.2624e-04,
         1.8463e-03, -4.7891e-04,  8.2311e-04, -1.9706e-04,  0.0000e+00,
        -8.6831e-04, -5.8138e-04,  0.0000e+00,  3.6928e-04,  5.5254e-04,
         0.0000e+00,  2.8297e-04,  0.0000e+00,  6.1924e-05, -3.0758e-03,
        -6.3834e-04, -1.0895e-03, -1.8788e-05, -6.3979e-04,  0.0000e+00,
        -1.7386e-04, -2.2827e-03,  0.0000e+00,  1.2160e-03, -7.1492e-05,
         2.6493e-04,  1.4282e-03,  2.0094e-03,  1.5162e-04, -1.1925e-03,
         2.7231e-06, -3.6009e-03,  0.0000e+00,  0.0000e+00, -4.1212e-04,
        -1.2879e-03,  5.8854e-07, -1.3417e-03,  2.9968e-05,  9.7143e-04,
         3.3361e-04,  0.0000e+00, -3.1506e-05,  8.3852e-05, -2.4042e-05,
         0.0000e+00, -3.3563e-03,  0.0000e+00, -4.1990e-04,  2.2305e-06,
         2.8079e-04,  4.2957e-04, -1.2574e-03,  2.2728e-04, -6.5429e-04,
        -1.4593e-03,  0.0000e+00, -1.2198e-04,  9.3951e-06,  3.7476e-04,
        -6.7724e-04,  1.8817e-04,  0.0000e+00, -2.4723e-03,  0.0000e+00,
         7.9072e-04,  0.0000e+00,  0.0000e+00,  9.8638e-04, -3.6258e-04,
         1.1560e-03, -1.2948e-04,  4.1474e-03,  0.0000e+00, -1.8443e-03,
         8.4297e-04,  0.0000e+00, -4.9093e-04, -3.6884e-03, -3.6322e-04,
         5.4830e-05,  0.0000e+00,  2.2824e-04, -1.8797e-03,  0.0000e+00,
         5.9426e-03, -2.0726e-03, -1.7301e-05,  2.4288e-04,  1.7387e-03,
         2.1701e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.3931e-04,
        -1.8892e-04,  3.8604e-03, -6.1678e-04,  4.7809e-05,  4.8318e-04,
        -2.6460e-03,  3.6835e-04, -2.4661e-03,  1.0063e-03,  1.5561e-03,
         9.9803e-05,  3.2735e-05,  0.0000e+00, -1.2937e-03, -2.9845e-03,
         3.9932e-05, -1.6370e-03,  2.7355e-03, -4.4171e-04,  9.2699e-07,
        -4.9852e-05, -1.8918e-03, -1.0872e-03, -1.7796e-05,  0.0000e+00,
         4.8706e-04,  5.4763e-04,  1.0986e-03, -1.6453e-03, -9.0513e-04,
         5.6049e-05, -2.7251e-03,  5.5000e-04, -1.3864e-03, -1.4856e-04,
        -1.6338e-03, -1.0966e-03, -1.7633e-03,  0.0000e+00,  0.0000e+00,
         1.9130e-04, -1.4368e-04, -1.6389e-03, -5.3807e-04, -6.3905e-04,
         5.1195e-04, -1.8251e-03,  1.5259e-03,  3.1077e-03,  4.0742e-04,
        -4.8129e-04,  4.0569e-05,  1.9120e-05, -5.2255e-03,  2.3989e-03,
        -1.3638e-04, -1.2391e-03,  0.0000e+00,  0.0000e+00, -1.1460e-03,
         0.0000e+00,  4.7558e-03, -1.5879e-03, -4.7408e-04,  7.9712e-04,
        -2.6111e-03,  7.8626e-05,  1.1435e-04, -3.0583e-04, -2.0749e-04,
         0.0000e+00, -1.0375e-03, -1.0736e-04, -3.0652e-03,  1.1664e-03,
        -1.6867e-03,  0.0000e+00, -8.3942e-04, -1.4081e-03, -5.5759e-04,
         1.7310e-03,  2.6056e-04, -1.6188e-03, -8.5212e-05,  6.8632e-04,
         8.1788e-04,  0.0000e+00,  5.4342e-03, -1.9699e-04, -4.3193e-05,
         0.0000e+00,  9.4637e-04,  0.0000e+00, -3.7318e-06,  0.0000e+00,
        -1.3626e-03,  2.4086e-04, -2.2509e-04,  5.4342e-04,  3.8815e-03,
        -6.6703e-04,  8.5311e-04, -1.0915e-03,  6.0240e-04, -1.1876e-04,
         1.4692e-04, -2.2632e-04, -8.9077e-04,  0.0000e+00, -8.9829e-04,
         3.0368e-04,  2.1413e-03, -2.9970e-04,  1.6216e-04,  3.5228e-05,
        -4.1683e-04,  2.6281e-05, -9.9946e-04, -4.0608e-04,  4.9353e-05,
         6.0286e-06,  6.8928e-05,  0.0000e+00, -5.7381e-04, -1.0066e-03,
        -1.0117e-04, -1.4817e-04,  1.1659e-04, -1.2306e-06, -9.3449e-04,
         7.7385e-04,  1.4968e-04,  1.6394e-04, -3.7941e-04, -4.0628e-04,
         1.6125e-04,  6.1386e-04, -2.6176e-03, -1.5921e-03,  3.7145e-04,
         0.0000e+00, -5.0266e-04, -1.7360e-06,  4.9381e-04,  1.7904e-03,
        -2.3003e-06,  0.0000e+00,  0.0000e+00, -2.5785e-03, -1.6967e-03,
        -2.4076e-03, -6.8232e-04, -3.1269e-04,  0.0000e+00,  0.0000e+00,
        -2.2704e-04,  2.2338e-04,  1.7718e-03,  2.2974e-05,  2.0770e-03,
         7.6605e-04,  0.0000e+00, -1.0086e-03,  0.0000e+00, -3.9996e-05,
         9.4957e-04,  6.4386e-04, -3.0988e-03,  2.4984e-05,  1.8495e-04,
        -1.9314e-04, -1.2518e-03], device='mps:0')
tensor([[-2.5388e-05,  0.0000e+00, -6.1200e-05,  ...,  1.1553e-05,
          0.0000e+00, -1.0760e-04],
        [-1.0150e-04,  0.0000e+00, -5.0336e-05,  ...,  2.6574e-05,
         -7.9457e-06, -4.4028e-05],
        [ 1.1655e-04,  0.0000e+00,  9.1256e-05,  ..., -2.4392e-05,
          0.0000e+00, -6.3221e-04],
        ...,
        [ 0.0000e+00,  0.0000e+00,  4.2394e-05,  ...,  0.0000e+00,
          0.0000e+00,  1.8344e-04],
        [ 3.3149e-06,  0.0000e+00,  9.7968e-06,  ...,  0.0000e+00,
          0.0000e+00,  4.0209e-05],
        [-1.7937e-04,  0.0000e+00, -2.6428e-04,  ...,  2.5266e-06,
          2.0708e-05, -3.7219e-04]], device='mps:0')
tensor([-6.1339e-04, -1.5156e-03, -6.5278e-04, -1.7810e-03,  0.0000e+00,
         1.9864e-04,  6.2319e-03, -9.9126e-04, -4.7440e-03, -4.8637e-05,
         4.6541e-06,  7.8931e-05,  1.9714e-04, -4.6587e-03, -2.4603e-03,
        -2.1156e-03, -1.3273e-03,  1.1695e-03,  3.1722e-03,  1.2211e-03,
         0.0000e+00,  4.7223e-03, -2.2818e-03,  5.6569e-06, -5.1719e-04,
         1.0062e-03,  0.0000e+00, -1.3912e-03, -2.2193e-03, -1.6035e-03,
         3.9926e-03, -1.5572e-03, -6.5427e-03, -4.0064e-04,  2.2010e-03,
         8.9640e-03,  1.1124e-04,  7.3957e-04,  1.2368e-04, -6.3106e-03,
        -9.9112e-03, -2.5114e-04,  0.0000e+00,  8.3927e-04,  4.9261e-04,
         2.3749e-03, -4.1315e-04, -4.7499e-03, -9.2331e-04,  2.4134e-03,
         1.5937e-03,  7.2382e-04, -2.5163e-04, -6.7813e-04,  4.0872e-05,
        -5.0189e-04,  5.4840e-03,  6.8414e-04, -8.3495e-03,  0.0000e+00,
         4.8512e-05,  0.0000e+00,  1.6447e-04,  7.0745e-03,  1.6025e-03,
         1.8332e-03, -3.7466e-04,  8.8987e-04, -2.2546e-05,  6.0993e-04,
         2.4011e-04, -2.4548e-03, -4.6307e-05,  1.1961e-04,  0.0000e+00,
         9.0348e-07,  6.8052e-04, -4.4358e-03,  2.5017e-03, -4.6965e-03,
         0.0000e+00, -8.3401e-04,  3.3977e-03,  1.0208e-02,  5.5400e-03,
         1.2716e-04,  4.2338e-03, -7.9291e-03,  0.0000e+00, -6.5638e-03,
         1.4491e-05,  3.2562e-04,  4.8872e-03,  6.4361e-05,  8.4505e-03,
         0.0000e+00, -4.8240e-04,  1.2428e-03,  4.2327e-04,  4.8855e-03,
         1.5783e-05,  6.0522e-04, -6.4592e-03, -9.6284e-03, -9.6329e-04,
        -8.8446e-04, -1.1262e-03,  6.9257e-05,  1.7996e-03, -5.7960e-04,
        -2.5331e-04,  4.5761e-03,  3.4909e-03, -7.9303e-04,  2.5836e-05,
        -3.7875e-03,  2.7511e-05, -1.3214e-03, -6.3835e-03,  0.0000e+00,
         5.1868e-05,  1.9584e-03,  0.0000e+00, -1.1758e-03,  0.0000e+00,
         1.2742e-03,  4.1453e-03,  6.4768e-04,  2.5833e-03,  0.0000e+00,
         3.0381e-05, -1.5583e-03, -4.3673e-03,  0.0000e+00,  4.5141e-03,
         0.0000e+00,  1.8774e-03,  4.2198e-04,  1.4028e-03, -6.4582e-03,
        -1.0100e-02,  4.3190e-03,  7.9234e-04, -3.5809e-03, -1.6251e-03,
        -5.7162e-03, -2.3573e-04,  3.6800e-03,  1.5246e-04, -1.8565e-03,
         6.2348e-03, -4.2200e-03,  2.3104e-03, -5.4616e-04,  4.2419e-03,
         2.1932e-03,  1.7203e-03, -7.8122e-06, -4.2371e-03,  2.2325e-03,
        -4.5610e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.3113e-03,
         6.7067e-03, -5.4316e-04,  6.0300e-03,  0.0000e+00,  5.5006e-04,
        -3.4277e-03,  3.3264e-04,  0.0000e+00,  2.5295e-04,  0.0000e+00,
         1.0923e-05,  1.9960e-03, -1.5021e-03, -1.3630e-03, -5.6369e-05,
         6.9375e-05,  1.0906e-03,  2.4380e-03,  0.0000e+00,  0.0000e+00,
        -9.8842e-03, -8.3771e-03, -4.1933e-03, -2.8200e-05,  5.6244e-04,
         0.0000e+00, -2.2555e-05,  5.0358e-04,  2.8437e-03,  0.0000e+00,
         0.0000e+00, -1.0542e-02, -1.8915e-03,  1.1763e-03, -4.5499e-04,
        -4.2628e-03, -4.4200e-03, -8.5007e-03, -6.8729e-03, -6.7733e-04,
         6.8519e-04, -1.1699e-03,  4.0613e-04,  7.4753e-03,  1.2209e-03,
         2.7368e-04, -6.1441e-04, -2.2858e-03, -1.7839e-03,  0.0000e+00,
        -4.4641e-03, -1.3293e-03, -7.0080e-06, -8.4169e-03,  1.9390e-03,
         0.0000e+00,  6.6445e-04,  4.7345e-04,  3.2026e-03,  0.0000e+00,
         2.6396e-03, -2.9397e-03,  1.5654e-03,  6.5770e-03,  0.0000e+00,
         9.9968e-04,  0.0000e+00,  2.5437e-03,  2.6930e-03,  6.3036e-04,
         1.9579e-03, -3.5380e-03,  1.1547e-03, -1.9277e-04,  3.9723e-04,
         6.4982e-04,  3.5547e-04,  3.7267e-07, -8.1558e-03,  7.0456e-04,
        -5.9961e-03,  0.0000e+00,  0.0000e+00, -9.2249e-05,  1.7610e-03,
        -1.3095e-03, -3.0885e-04, -9.8330e-03,  6.1150e-04,  4.2492e-05,
        -1.6291e-03], device='mps:0')
tensor([[-3.7399e-03, -3.5339e-03, -3.4562e-03, -8.1044e-05,  0.0000e+00,
          7.4568e-03, -1.2316e-02,  1.2355e-03, -2.2120e-02, -4.2579e-04,
         -7.6866e-04, -8.3921e-05,  4.9392e-03, -4.2631e-02, -1.9006e-02,
         -1.0349e-02,  1.6680e-03, -3.9500e-03, -4.4520e-02, -4.0833e-03,
          0.0000e+00, -1.3374e-02, -3.3901e-02, -3.1484e-04,  6.9654e-05,
         -4.4976e-03,  0.0000e+00, -1.4870e-02, -1.7032e-02, -4.0954e-03,
          1.2236e-02, -2.2471e-02, -1.2318e-02, -1.7414e-03, -1.5024e-02,
         -1.9822e-02, -6.0847e-03, -8.0005e-04, -1.5014e-03, -5.5894e-02,
         -3.2195e-02,  1.4813e-03,  0.0000e+00, -1.1271e-02,  1.1031e-03,
          4.0163e-03, -1.1470e-04, -1.5427e-02,  8.6116e-04, -4.4383e-03,
         -1.1050e-02, -6.2839e-04, -5.0709e-03, -2.5251e-03, -1.4314e-04,
         -1.6832e-02, -2.0543e-02, -4.3085e-03, -9.6113e-02,  0.0000e+00,
          4.9757e-03,  0.0000e+00,  3.6390e-03, -2.2729e-02, -3.4406e-03,
         -1.9801e-02, -2.1424e-02, -1.2582e-02,  6.5175e-04, -1.0679e-04,
         -2.3982e-03, -5.0174e-02, -3.0086e-04, -2.9968e-04,  0.0000e+00,
         -1.6414e-02, -3.9293e-02, -1.1860e-02, -1.7925e-02, -3.7157e-02,
          0.0000e+00, -5.9479e-03, -7.8152e-03, -2.2925e-02, -1.2429e-02,
         -2.2630e-03, -1.0190e-03, -2.1095e-02,  0.0000e+00, -7.7476e-02,
         -1.6933e-04,  8.0284e-04, -1.6217e-02,  2.7800e-04, -3.5046e-02,
          0.0000e+00, -5.0338e-03, -1.6729e-03, -6.4820e-03, -2.0033e-03,
          2.7778e-03, -7.7577e-04, -5.5414e-02, -5.4976e-02,  5.5958e-03,
         -7.5389e-03,  2.8446e-03, -6.3881e-03, -2.1811e-03, -6.1795e-03,
         -4.0264e-03, -4.2986e-02, -2.7560e-02,  4.8009e-03,  2.7412e-03,
         -1.5099e-02, -5.7130e-04, -6.1132e-06, -1.5742e-02,  0.0000e+00,
         -5.0458e-03, -1.0260e-03,  0.0000e+00, -4.1826e-02,  0.0000e+00,
         -4.6927e-03, -9.2719e-03, -3.2699e-04, -1.5277e-02,  0.0000e+00,
         -4.0374e-03, -6.5670e-03, -1.3870e-02,  0.0000e+00, -1.5350e-02,
          0.0000e+00, -2.8041e-02, -7.4123e-03,  1.3908e-04, -3.3806e-02,
         -4.7842e-02, -1.5954e-02, -7.7117e-04, -8.6691e-03, -1.6673e-02,
         -3.0884e-02, -9.0037e-03, -3.7844e-02, -1.5196e-02, -6.4401e-03,
         -1.3384e-02, -2.4040e-02, -2.9384e-02, -4.4785e-03, -2.6419e-02,
         -4.4895e-03, -9.9664e-03,  3.4340e-03, -3.4635e-02, -8.2984e-03,
         -1.3668e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.0544e-02,
         -2.4832e-02, -4.5752e-05, -2.3001e-02,  0.0000e+00,  1.6663e-04,
         -1.3010e-02,  8.0562e-04,  0.0000e+00, -8.2947e-04,  0.0000e+00,
         -8.4188e-05, -2.6111e-02, -1.7449e-03, -3.0741e-03, -8.0151e-03,
         -5.5913e-05, -1.1542e-03,  4.8789e-03,  0.0000e+00,  0.0000e+00,
         -6.6278e-02, -4.5541e-02, -1.4221e-02, -5.4150e-03,  9.9481e-03,
          0.0000e+00, -4.5330e-03,  9.3210e-04, -7.5830e-03,  0.0000e+00,
          0.0000e+00, -4.3558e-02, -2.4364e-02, -1.7771e-04, -1.7131e-02,
         -2.0474e-03, -7.7663e-03, -4.8281e-02, -3.1053e-02,  1.9062e-03,
         -6.6571e-04, -4.1450e-05, -6.3032e-04, -5.6899e-02, -1.1182e-03,
         -1.3121e-02, -2.1999e-03, -8.3097e-03, -8.9046e-03,  0.0000e+00,
         -1.6303e-02, -2.5754e-02, -2.0817e-03, -7.7002e-02, -1.8954e-03,
          0.0000e+00, -8.6035e-03, -3.4868e-03,  1.2916e-03,  0.0000e+00,
         -2.0972e-02, -8.2394e-03,  1.0403e-03, -1.9748e-02,  0.0000e+00,
         -4.1417e-03,  0.0000e+00, -2.4642e-02, -1.3492e-02,  3.1036e-03,
         -1.2151e-02, -7.0376e-03, -6.4710e-03, -1.5860e-02,  5.0983e-03,
         -2.3548e-02,  6.9280e-04, -3.2129e-05, -5.0462e-02, -1.6356e-04,
         -1.9278e-02,  0.0000e+00,  0.0000e+00, -1.9854e-03, -1.2618e-03,
         -2.4345e-02,  2.1534e-04, -8.1552e-02, -1.9902e-04, -7.6086e-04,
         -1.0680e-02]], device='mps:0')
tensor([-0.1001], device='mps:0')
Epoch 0: Loss_D: 0.5536324977874756, Loss_G: 0.5059375762939453</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="GenerativeAdversarialNetworks_Goodfellow_files/figure-html/cell-15-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-error">
<pre><code>KeyboardInterrupt: </code></pre>
</div>
</div>
<p>When running this code locally, there are some issues you should be aware of. Firstly, due to the stochastic nature of neural networks it is likely your generated images won’t match mine exactly. A more pressing issue can occur where the generated images all look bad and do not seem to improve, when this happens the best solution is to reinitialise the networks and run the training loop again.</p>
<hr>
</section>
</section>
<section id="congrats-youve-implemented-and-trained-a-gan" class="level2">
<h2 class="anchored" data-anchor-id="congrats-youve-implemented-and-trained-a-gan">Congrats, you’ve implemented and trained a GAN</h2>
<p>You can now see the outputs of your model and they look pretty good, perhaps you can get them to look better with more epochs or a different model architecture. Also, here’s a cool project you could try after this: traing your GAN and generate a bunch of samples of the generated digits, then build an MNIST classifier and pass these through the trained classifier and see if it gets them correct.</p>
<p>I’ll leave you with an issue with this setup of GANs. The updating of the G model is dependent upon the performance of the D model, in essence the better the feedback the D model gives the better our G model will become. However, when the G model gets good enough such that the accuracy of the D model becomes 0.5 (its guessing randomly) it’s feedback is essentially meaningless and our G model stops improving. This can be seen in our model too.</p>
<p>The cool idea that we should make clear is that the GAN truly does generate new images, it does not learn the training data but it generates new images. Exactly how may not be fully understood (by me anyways) but this is what is happening, isn’t that amazing!</p>
<hr>
<p><a id="1" style="text-decoration: none; color: inherit;" href=""><sup>1</sup></a> <a href="https://arxiv.org/pdf/1406.2661">https://arxiv.org/pdf/1406.2661</a></p>
<p><a id="2" style="text-decoration: none; color: inherit;" href=""><sup>2</sup></a> If you don’t know here’s a good resource to learn: <a href="https://www.deeplearningbook.org/contents/mlp.html">https://www.deeplearningbook.org/contents/mlp.html</a></p>
<p><a id="3" style="text-decoration: none; color: inherit;" href=""><sup>3</sup></a> Here’s a great book to get familiar with PyTorch <a href="https://www.manning.com/books/deep-learning-with-pytorch">https://www.manning.com/books/deep-learning-with-pytorch</a></p>
<p><a id="4" style="text-decoration: none; color: inherit;" href=""><sup>4</sup></a> I know this can be a bit frustrating to hear, but if you have any questions on this or anything discussed here feel free to reach out to me @ <a href="mailto:y%75sufmohamma%64@l%69ve.com">yusufmohammad@live.com</a></p>
<p><a id="5" style="text-decoration: none; color: inherit;" href=""><sup>5</sup></a> https://github.com/soumith/ganhacks check hack 2</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>