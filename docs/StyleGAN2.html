<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yusuf Mohammad">

<title>The Path to StyleGan2 - The Finale</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="StyleGAN2_files/libs/clipboard/clipboard.min.js"></script>
<script src="StyleGAN2_files/libs/quarto-html/quarto.js"></script>
<script src="StyleGAN2_files/libs/quarto-html/popper.min.js"></script>
<script src="StyleGAN2_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="StyleGAN2_files/libs/quarto-html/anchor.min.js"></script>
<link href="StyleGAN2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="StyleGAN2_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="StyleGAN2_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="StyleGAN2_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="StyleGAN2_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#stylegan2---a-modification-of-the-original-stylegan" id="toc-stylegan2---a-modification-of-the-original-stylegan" class="nav-link active" data-scroll-target="#stylegan2---a-modification-of-the-original-stylegan">StyleGAN2 - A modification of the original StyleGAN</a>
  <ul>
  <li><a href="#issue-1---adaptive-instance-normalisation" id="toc-issue-1---adaptive-instance-normalisation" class="nav-link" data-scroll-target="#issue-1---adaptive-instance-normalisation">Issue 1 - Adaptive Instance Normalisation</a></li>
  <li><a href="#issue-2---progressive-growing" id="toc-issue-2---progressive-growing" class="nav-link" data-scroll-target="#issue-2---progressive-growing">Issue 2 - Progressive Growing</a></li>
  <li><a href="#redesigning-the-normalisation" id="toc-redesigning-the-normalisation" class="nav-link" data-scroll-target="#redesigning-the-normalisation">Redesigning the Normalisation</a></li>
  <li><a href="#removing-issues-of-progressive-growing" id="toc-removing-issues-of-progressive-growing" class="nav-link" data-scroll-target="#removing-issues-of-progressive-growing">Removing Issues of Progressive Growing</a></li>
  </ul></li>
  <li><a href="#the-path-to-implementation---a-roadmap" id="toc-the-path-to-implementation---a-roadmap" class="nav-link" data-scroll-target="#the-path-to-implementation---a-roadmap">The Path to Implementation - A Roadmap</a>
  <ul>
  <li><a href="#the-new-style-block" id="toc-the-new-style-block" class="nav-link" data-scroll-target="#the-new-style-block">1 - The New Style Block</a></li>
  <li><a href="#the-new-architecture" id="toc-the-new-architecture" class="nav-link" data-scroll-target="#the-new-architecture">2 - The New Architecture</a></li>
  <li><a href="#generator" id="toc-generator" class="nav-link" data-scroll-target="#generator">Generator</a></li>
  <li><a href="#discriminator" id="toc-discriminator" class="nav-link" data-scroll-target="#discriminator">Discriminator</a></li>
  </ul></li>
  <li><a href="#regularisation-in-the-stylegan2" id="toc-regularisation-in-the-stylegan2" class="nav-link" data-scroll-target="#regularisation-in-the-stylegan2">Regularisation in the StyleGAN2</a>
  <ul>
  <li><a href="#r1-regularisation" id="toc-r1-regularisation" class="nav-link" data-scroll-target="#r1-regularisation">R1 Regularisation</a></li>
  <li><a href="#perceptual-path-length-ppl-regularisation" id="toc-perceptual-path-length-ppl-regularisation" class="nav-link" data-scroll-target="#perceptual-path-length-ppl-regularisation">Perceptual Path Length (PPL) Regularisation</a></li>
  <li><a href="#implementing-the-two-regularisers" id="toc-implementing-the-two-regularisers" class="nav-link" data-scroll-target="#implementing-the-two-regularisers">Implementing the Two Regularisers</a></li>
  </ul></li>
  <li><a href="#implementing-the-training-loop" id="toc-implementing-the-training-loop" class="nav-link" data-scroll-target="#implementing-the-training-loop">Implementing the Training Loop</a></li>
  <li><a href="#training---a-story-of-time-and-energy" id="toc-training---a-story-of-time-and-energy" class="nav-link" data-scroll-target="#training---a-story-of-time-and-energy">Training - A Story of Time and Energy</a>
  <ul>
  <li><a href="#failure-patterns-i-observed" id="toc-failure-patterns-i-observed" class="nav-link" data-scroll-target="#failure-patterns-i-observed">Failure Patterns I Observed</a>
  <ul class="collapse">
  <li><a href="#a-critical-discovery-one-bug-led-to-a-breakthrough" id="toc-a-critical-discovery-one-bug-led-to-a-breakthrough" class="nav-link" data-scroll-target="#a-critical-discovery-one-bug-led-to-a-breakthrough">A Critical Discovery, one bug led to a breakthrough</a></li>
  <li><a href="#some-of-my-fid-patterns" id="toc-some-of-my-fid-patterns" class="nav-link" data-scroll-target="#some-of-my-fid-patterns">Some of my FID patterns</a></li>
  </ul></li>
  <li><a href="#segfaults-and-illegal-instructions" id="toc-segfaults-and-illegal-instructions" class="nav-link" data-scroll-target="#segfaults-and-illegal-instructions">SegFaults and Illegal Instructions</a></li>
  <li><a href="#the-successful-training-run" id="toc-the-successful-training-run" class="nav-link" data-scroll-target="#the-successful-training-run">The Successful Training Run</a>
  <ul class="collapse">
  <li><a href="#so-what-do-our-images-look-like" id="toc-so-what-do-our-images-look-like" class="nav-link" data-scroll-target="#so-what-do-our-images-look-like">So what do our images look like?</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="quarto-other-links"><h2>Other Links</h2><ul><li><a href="https://ym2132.github.io/"><i class="bi bi-link-45deg"></i>Yusuf's Deep Learning Blog</a></li><li><a href="https://github.com/YM2132?tab=repositories"><i class="bi bi-link-45deg"></i>Yusuf's GitHub</a></li><li><a href="https://ym2132.github.io/Progressive_GAN"><i class="bi bi-link-45deg"></i>Progressive Growing GAN Blog post</a></li><li><a href="https://ym2132.github.io/StyleGAN"><i class="bi bi-link-45deg"></i>StyleGAN 1 Blog Post</a></li></ul></div><div class="quarto-code-links"><h2>Code Links</h2><ul><li><a href="https://github.com/YM2132/YMPaperImplementations/blob/main/paper_implementations/python_implementations/StyleGAN2"><i class="bi bi-file-code"></i>StyleGAN2</a></li><li><a href="https://github.com/YM2132/YMPaperImplementations/blob/main/paper_implementations/StyleGAN2/final_fid_scores.txt"><i class="bi bi-file-text"></i>Final FID scores</a></li><li><a href="https://github.com/YM2132/YMPaperImplementations/blob/main/paper_implementations/StyleGAN2/run_StyleGAN2_inference.ipynb"><i class="bi bi-file-code"></i>Generate Images</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The Path to StyleGan2 - The Finale</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Yusuf Mohammad </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<style>
.scroll-box {
  max-height: 300px;
  overflow-y: auto;
  border: 1px solid #ddd;
  padding: 1em;
}

.scroll-box-x {
  max-width: 500px;
  overflow-x: auto;
  white-space: nowrap;
}

/* For tables */
.table-scroll {
  max-height: 400px;
  overflow-y: auto;
  display: block;
}
</style>
<hr>
<p>This post concludes The Path to StyleGan2 series and will see us implement the StyleGAN2 model (there is a StyleGAN3 but I will not cover it, for now).</p>
<p>The link for the StyleGAN2 paper is <a href="https://arxiv.org/pdf/1912.04958">https://arxiv.org/pdf/1912.04958</a>. I’d recommend giving it a read before and alongside this post :)</p>
<p>As I write the training for this model is still underway, if anything it has shown to me the challenge of training GANs. I have ran at least 100 training runs (over the course of four months) which either explode in FID or outright crash due to a segmentation fault. Read on for some of the errors I faced and what to look out for in GAN training!</p>
<p>Also, our implementation here is a little unique due to image size and the number of images we can fit in our GPU. To help out, I made use of a few other implementations of the StyleGAN2 to aid my discussion below and I urge you to check them out too they are great, also these posts have quite a lot of cool insights which warrant further reading.</p>
<p><b>GPU NOTE</b> For this implementation you will require at least a 12gb GPU, however you can edit it to fit your GPU<a href="#10"><sup>10</sup></a>.</p>
<p>Links to other repos/posts:</p>
<ul>
<li><p><a href="https://gwern.net/face">https://gwern.net/face</a> - A great post which contains many interesting points about the nature of StyleGAN2</p></li>
<li><p><a href="https://github.com/l4rz/practical-aspects-of-stylegan2-training?tab=readme-ov-file">https://github.com/l4rz/practical-aspects-of-stylegan2-training?tab=readme-ov-file</a> - Another great post containing information on training StyleGANs</p></li>
<li><p><a href="https://github.com/NVlabs/stylegan2">https://github.com/NVlabs/stylegan2</a> - Official repo for StyleGAN2 by Karras</p></li>
<li><p><a href="https://github.com/NVlabs/stylegan2-ada-pytorch/blob/main/train.py#L157">https://github.com/NVlabs/stylegan2-ada-pytorch/blob/main/train.py#L157</a> - The official repo for ADA StyleGAN2 (the addition ADA deserves another blog post) by Karras et al.&nbsp;ADA is an adaptation of StyleGAN2 to work with less images, you can read about it at <a href="https://arxiv.org/abs/2006.06676">https://arxiv.org/abs/2006.06676</a></p></li>
<li><p><a href="https://github.com/NVlabs/stylegan3/blob/c233a919a6faee6e36a316ddd4eddababad1adf9/docs/configs.md">https://github.com/NVlabs/stylegan3/blob/c233a919a6faee6e36a316ddd4eddababad1adf9/docs/configs.md</a> - The repository for StyleGAN3 contains a list of parameters for the StyleGAN2 setup. These are the hyperparameters I used for my successful training run, note that these are gotten by the auto setting in the StyleGAN2-ADA code.</p></li>
<li><p><a href="https://github.com/rosinality/stylegan2-pytorch/tree/master">https://github.com/rosinality/stylegan2-pytorch/tree/master</a> - Another repo which implements the StyleGAN2</p></li>
</ul>
<p>We make use of the FFHQ dataset this time around, with 256x256 images (as I am GPU poor). You can find the dataset at <a href="https://www.kaggle.com/datasets/denislukovnikov/ffhq256-images-only/data">https://www.kaggle.com/datasets/denislukovnikov/ffhq256-images-only/data</a>. It is a similar dataset to CelebA-HQ-256 the key difference being it includes more diverse images.</p>
<p>Lastly, I’d like to say that the StyleGAN series of models are amazing, I hope you enjoyed this series and I thank you for taking the time to read these posts!</p>
<p>Now let’s get into StyleGAN2</p>
<hr>
<p>Expand this block to show code for imports and some helper functions :)</p>
<div id="9fc57a34-e942-4285-8e1c-e4bc6a3d9ab1" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2"></a>os.environ[<span class="st">'PYTORCH_CUDA_ALLOC_CONF'</span>] <span class="op">=</span> <span class="st">'expandable_segments:True'</span></span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> torch</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">import</span> torchvision</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="im">import</span> torch.nn.init <span class="im">as</span> init</span>
<span id="cb1-10"><a href="#cb1-10"></a></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="im">from</span> torchvision.datasets <span class="im">import</span> ImageFolder</span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> save_image</span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms, utils</span>
<span id="cb1-15"><a href="#cb1-15"></a></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb1-17"><a href="#cb1-17"></a></span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="im">from</span> torchmetrics.image.fid <span class="im">import</span> FrechetInceptionDistance</span>
<span id="cb1-19"><a href="#cb1-19"></a></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-21"><a href="#cb1-21"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-22"><a href="#cb1-22"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb1-23"><a href="#cb1-23"></a><span class="im">from</span> datetime <span class="im">import</span> datetime</span>
<span id="cb1-24"><a href="#cb1-24"></a><span class="im">from</span> math <span class="im">import</span> sqrt</span>
<span id="cb1-25"><a href="#cb1-25"></a><span class="im">import</span> math</span>
<span id="cb1-26"><a href="#cb1-26"></a><span class="im">import</span> sys</span>
<span id="cb1-27"><a href="#cb1-27"></a><span class="im">import</span> random</span>
<span id="cb1-28"><a href="#cb1-28"></a></span>
<span id="cb1-29"><a href="#cb1-29"></a></span>
<span id="cb1-30"><a href="#cb1-30"></a><span class="co"># We can make use of a GPU if you have one on your computer. This works for Nvidia and M series GPU's</span></span>
<span id="cb1-31"><a href="#cb1-31"></a><span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb1-32"><a href="#cb1-32"></a>    device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)</span>
<span id="cb1-33"><a href="#cb1-33"></a>    <span class="co"># These 2 lines assign some data on the memory of the device and output it. The output confirms</span></span>
<span id="cb1-34"><a href="#cb1-34"></a>    <span class="co"># if we have set the intended device</span></span>
<span id="cb1-35"><a href="#cb1-35"></a>    x <span class="op">=</span> torch.ones(<span class="dv">1</span>, device<span class="op">=</span>device)</span>
<span id="cb1-36"><a href="#cb1-36"></a>    <span class="bu">print</span> (x)</span>
<span id="cb1-37"><a href="#cb1-37"></a><span class="cf">elif</span> torch.backends.cuda.is_built():</span>
<span id="cb1-38"><a href="#cb1-38"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb1-39"><a href="#cb1-39"></a>    x <span class="op">=</span> torch.ones(<span class="dv">1</span>, device<span class="op">=</span>device)</span>
<span id="cb1-40"><a href="#cb1-40"></a>    <span class="bu">print</span> (x)</span>
<span id="cb1-41"><a href="#cb1-41"></a><span class="cf">else</span>:</span>
<span id="cb1-42"><a href="#cb1-42"></a>    device <span class="op">=</span> (<span class="st">"cpu"</span>)</span>
<span id="cb1-43"><a href="#cb1-43"></a>    x <span class="op">=</span> torch.ones(<span class="dv">1</span>, device<span class="op">=</span>device)</span>
<span id="cb1-44"><a href="#cb1-44"></a>    <span class="bu">print</span> (x)</span>
<span id="cb1-45"><a href="#cb1-45"></a>    </span>
<span id="cb1-46"><a href="#cb1-46"></a><span class="co"># I also define a function we use to examine the outputs of the Generator</span></span>
<span id="cb1-47"><a href="#cb1-47"></a><span class="kw">def</span> show_images(images, num_images<span class="op">=</span><span class="dv">16</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">10</span>)):</span>
<span id="cb1-48"><a href="#cb1-48"></a>    <span class="co"># Ensure the input is on CPU</span></span>
<span id="cb1-49"><a href="#cb1-49"></a>    images <span class="op">=</span> images.cpu().detach()</span>
<span id="cb1-50"><a href="#cb1-50"></a>    </span>
<span id="cb1-51"><a href="#cb1-51"></a>    <span class="co"># Normalize images from [-1, 1] to [0, 1]</span></span>
<span id="cb1-52"><a href="#cb1-52"></a>    images <span class="op">=</span> (images <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb1-53"><a href="#cb1-53"></a>    </span>
<span id="cb1-54"><a href="#cb1-54"></a>    <span class="co"># Clamp values to [0, 1] range</span></span>
<span id="cb1-55"><a href="#cb1-55"></a>    images <span class="op">=</span> torch.clamp(images, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb1-56"><a href="#cb1-56"></a>    </span>
<span id="cb1-57"><a href="#cb1-57"></a>    <span class="co"># Make a grid of images</span></span>
<span id="cb1-58"><a href="#cb1-58"></a>    grid <span class="op">=</span> torchvision.utils.make_grid(images[:num_images], nrow<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb1-59"><a href="#cb1-59"></a>    </span>
<span id="cb1-60"><a href="#cb1-60"></a>    <span class="co"># Convert to numpy and transpose</span></span>
<span id="cb1-61"><a href="#cb1-61"></a>    grid <span class="op">=</span> grid.numpy().transpose((<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb1-62"><a href="#cb1-62"></a>    </span>
<span id="cb1-63"><a href="#cb1-63"></a>    <span class="co"># Display the grid</span></span>
<span id="cb1-64"><a href="#cb1-64"></a>    plt.figure(figsize<span class="op">=</span>figsize)</span>
<span id="cb1-65"><a href="#cb1-65"></a>    plt.imshow(grid)</span>
<span id="cb1-66"><a href="#cb1-66"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb1-67"><a href="#cb1-67"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([1.], device='cuda:0')</code></pre>
</div>
</div>
<section id="stylegan2---a-modification-of-the-original-stylegan" class="level2">
<h2 class="anchored" data-anchor-id="stylegan2---a-modification-of-the-original-stylegan">StyleGAN2 - A modification of the original StyleGAN</h2>
<p>StyleGAN2 is an adaptation of StyleGAN, if you read the <a href="https://ym2132.github.io/StyleGAN">StyleGAN post</a> (shameless self-plug alert: if you haven’t I suggest you stop here and check it out) you will discover today that StyleGAN2 takes many elements of that model and adapts them to improve the quality of generated images. Also, the images from StyleGAN have two specific issues (I didn’t see this in my images, but this is because I didn’t train my StyleGAN to convergence), the issues arise from Adaptive Instance Normalisation (AdaIN) and the progressive growing regime. Let’s explore these before we move on to the implementation.</p>
<section id="issue-1---adaptive-instance-normalisation" class="level4">
<h4 class="anchored" data-anchor-id="issue-1---adaptive-instance-normalisation">Issue 1 - Adaptive Instance Normalisation</h4>
<p>The first issue we want to fix is “water droplets”. Take a look at Figure 1, they appear as distortions in specific areas of the images. The authors state this is related to the Adaptive Instance Normalisation (AdaIN) operation and when AdaIN is removed these artefacts do not appear<a href="#1"><sup>1</sup></a>. AdaIN normalises the mean and variance of each image separately, potentially destroying any information found in magnitudes of the features relative to each other. The “water droplet” effect maybe an attempt of the G model to sneak signal strength information past the AdaIN by creating localised spikes which will dominate the statistics<a href="#2"><sup>2</sup></a>. Now to stop this effect we could just remove AdaIN and the authors state this does stop the water dropletting and reduces FID slightly, but a key strength of StyleGAN is the “Style” transfer which takes place currently in the AdaIN operation. We need another way to enact style transfer but without AdaIN.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/d5591042-5687-48db-848a-b632bc18894d-1-7b1dc57a-2880-43d7-9d49-8dc196ba54e6.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1 - StyleGAN Artefacts</figcaption>
</figure>
</div>
</section>
<section id="issue-2---progressive-growing" class="level4">
<h4 class="anchored" data-anchor-id="issue-2---progressive-growing">Issue 2 - Progressive Growing</h4>
<p>The second issue is a result of the progressive growing regime. This scheme has been in play ever since the first post in this series (I wrote that a while ago). Progressive growing was the key idea which allowed the authors to achieve such great image quality. A quick recap, we first focus on lower level details by starting off training on 4x4 images, then we upscale to 8x8 -&gt; 16x16 -&gt; 32x32 -&gt; … -&gt; 1024x1024 in the end (I only go up to 256x256 due to GPU constraints) for more in depth coverage refer to <a href="https://ym2132.github.io/Progressive_GAN">https://ym2132.github.io/Progressive_GAN</a>. Anyways, the problem can be seen in Figure 2, they name these type of issues “phase artifacts”. It’s a subtle problem but look at the teeth, they stay aligned with the camera rather than the pose which is not how it works in the real world. This issue is supposedly caused by the way we implement progressive growing, the G model has a strong location preference for specific details (e.g.&nbsp;here teeth) and this is because we focus on low level details first before moving to higher level ones, so the detail will be in one place for a while and then randomly jump in the growing to a more preferred location. Whereas we want them to move smoothly overtime. To resolve this we scrap a part of the old progressive growing approach, namely where we change the network structure throughout training. Instead we still grow the image progressively, but the output is always the final resolution so from iteration 1 we go from a 4x4 image to 1024x1024 (or in our case 256x256). This retains the stability of progressive growing without the phase artifacts.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/a751d025-fed2-4a7e-9d08-fdc4e133b765-1-d3a48c88-a999-4576-99f1-baf1ac3128c8.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2 - Progressive Growing Issue</figcaption>
</figure>
</div>
<p>Let’s take a deeper dive on the plan to rectify these problems before we get coding :)</p>
<hr>
</section>
<section id="redesigning-the-normalisation" class="level3">
<h3 class="anchored" data-anchor-id="redesigning-the-normalisation">Redesigning the Normalisation</h3>
<p>The first major change in StyleGAN2 is a redesign of the normalisation, let’s get into it!</p>
<p>Remember the goal is to remove AdaIN but retain the ability to perform style transfer. Refer to Figure 3 for an overview of what is about to happen.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/9dbe16ff-0889-4782-a71c-deaaee02aded-1-446fa1fa-bc3a-4d3d-983b-901bf1d4d8b7.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3 - StyleGAN2 New Style Block Setup. (a) is the original StyleGAN block, (b) is the original StyleGAN block but split up so that each block has only one conv operation. (c) is the new style block for StyleGAN2 and (d) is the same as (c) but with the modulation operation expanded.</figcaption>
</figure>
</div>
<p>So, we replace AdaIN with a new operation called modulation (which itself consists of two parts - modulation and demodulation). Before getting to that, let’s understand what the AdaIN operation does. In Figure 3b we show the two parts of AdaIN normalisation and modulation, recall the formula for AdaIN has two parts the normalisation part and the modulation where we add the statistics (mean and standard deviation) of the latent vector used for style transfer. In the original StyleGAN the bias and noise is applied within the style block, causing the relative impact to be inversly proportional to the current style’s magnitude. If you move the operation outside the style block the results are slightly improved, this is because they operate on normalised data. After this change you may also only operate on standard deviation alone, you do not need to modulate the mean of generated images.</p>
<p>The issue of style modulation currently is that it may amplify the magnitudes of certain feature maps more than others, for it to work this amplification needs to be counteracted on a per-sample basis (feature map by feature map). This is what I mentioned earlier where the authors performed an ablation study and found it improves FID if you just remove AdaIN but it’s better to keep style transfer in. The method to do so is to base normalisation on expected statistics of incoming feature maps.</p>
<section id="modulation-and-demodulation" class="level5">
<h5 class="anchored" data-anchor-id="modulation-and-demodulation">Modulation and Demodulation</h5>
<p>Now, let’s explore the new normalisation and style transfer procedure. Review equation (1), this is the formula for modulation. Essentially, we scale each input feature map of the convolution based on the incoming style.</p>
<p><span class="math display">\[ w'_{ijk} = s_i \cdot w_{ijk} \tag{1}\]</span></p>
<p><span class="math inline">\(w\)</span> and <span class="math inline">\(w'\)</span> are the original and modulated weights respectively, and <span class="math inline">\(s_{i}\)</span> is the scale corresponding the to the <span class="math inline">\(i\)</span>th input. The purpose of instance normalisation is then to remove the effect of <span class="math inline">\(s\)</span> from the statistics of the convolutions output feature maps. This goal can be achieved direclty by equation (2).</p>
<p><span class="math display">\[ w''_{ijk} = w'_{ijk}/\sqrt{\sum_{i,k} {w'_{ijk}}^2 + \epsilon} \tag{2} \]</span></p>
<p><span class="math inline">\(\epsilon\)</span> is a small constant to avoid divide by 0 errors. This demodulation operation is now based upon statistical assumptions about the signal instead of actual contents of the feature maps. Thats it for demodulation! We’ll take a look at the implementation later, it has a pretty neat trick baked into it :)</p>
<hr>
</section>
</section>
<section id="removing-issues-of-progressive-growing" class="level3">
<h3 class="anchored" data-anchor-id="removing-issues-of-progressive-growing">Removing Issues of Progressive Growing</h3>
<p>Progressive growing is one the best things to happen to GANs, as it massively increased stability when training to generate high resolution images. So, despite the issue it causes we don’t want to remove it. The fix is quite simple, we retain progressive growing but unlike in the StyleGAN where we output each resolution at each layer and change the network structure as training progresses, we just output the final resolution and train the entire network right from the start. Now, it is a little more involved as we introduce skip connections and residual connections but overall it’s a nice fix and seems pretty intuitive as a first experiment right?</p>
<p>Examine Figure 4. We introduce skip connections in the G model and residual connections in the D model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/52c5d86a-07c3-4b5e-9590-8b478cf6feef-1-2f4c51da-6c50-4fac-a988-fa0613e2b463.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4 - The New Generator and Discriminator architectures</figcaption>
</figure>
</div>
<p>These new architectures still retain the ability to focus first on lower resolutions before moving to higher ones without progressive growing<a href="#3"><sup>3</sup></a>. This behaviour is not explicitly forced, so the G only does it if it is beneficial to training (which is quite awesome). Now it’s time to write some code!</p>
<hr>
</section>
</section>
<section id="the-path-to-implementation---a-roadmap" class="level2">
<h2 class="anchored" data-anchor-id="the-path-to-implementation---a-roadmap">The Path to Implementation - A Roadmap</h2>
<p>So, let’s lay out a path for us to travel in order to implement the StyleGAN2:</p>
<pre><code>1 - The new style block - modulation and demodulation
2 - G and D architecture redesign - removal of Progressive Growing, skip connections for G and residual connections for D
3 - Lazy + Path Length Regularisation
4 - Changing the loss function (no more WGAN-GP)</code></pre>
<p>We discussed changes 1 and 2 already, but changes 3 and 4 are still a mystery. I think these are best explored in conjunction with the code. 3 especially highlights some core behaviours of GANs.</p>
<hr>
<p>Expand the following to check out the old code (stuff we covered in the previous blog post). Note in the old blog posts the Mapping Network had hidden dimensions of 256x256 now I use 512x512 to reflect the paper.</p>
<div id="62faf559-a061-48af-b7f4-f29d66cb8a6d" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="co"># Credit: https://github.com/rosinality/stylegan2-pytorch/blob/master/model.py#L94</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="kw">class</span> EqualLRConv2d(nn.Module):</span>
<span id="cb4-3"><a href="#cb4-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb4-4"><a href="#cb4-4"></a>        <span class="va">self</span>, in_channel, out_channel, kernel_size, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>, bias<span class="op">=</span><span class="va">True</span></span>
<span id="cb4-5"><a href="#cb4-5"></a>    ):</span>
<span id="cb4-6"><a href="#cb4-6"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-7"><a href="#cb4-7"></a></span>
<span id="cb4-8"><a href="#cb4-8"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(</span>
<span id="cb4-9"><a href="#cb4-9"></a>            torch.randn(out_channel, in_channel, kernel_size, kernel_size)</span>
<span id="cb4-10"><a href="#cb4-10"></a>        )</span>
<span id="cb4-11"><a href="#cb4-11"></a>        <span class="va">self</span>.scale <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> math.sqrt(in_channel <span class="op">*</span> kernel_size <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb4-12"><a href="#cb4-12"></a></span>
<span id="cb4-13"><a href="#cb4-13"></a>        <span class="va">self</span>.stride <span class="op">=</span> stride</span>
<span id="cb4-14"><a href="#cb4-14"></a>        <span class="va">self</span>.padding <span class="op">=</span> padding</span>
<span id="cb4-15"><a href="#cb4-15"></a></span>
<span id="cb4-16"><a href="#cb4-16"></a>        <span class="cf">if</span> bias:</span>
<span id="cb4-17"><a href="#cb4-17"></a>            <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(out_channel))</span>
<span id="cb4-18"><a href="#cb4-18"></a></span>
<span id="cb4-19"><a href="#cb4-19"></a>        <span class="cf">else</span>:</span>
<span id="cb4-20"><a href="#cb4-20"></a>            <span class="va">self</span>.bias <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-21"><a href="#cb4-21"></a></span>
<span id="cb4-22"><a href="#cb4-22"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb4-23"><a href="#cb4-23"></a>        out <span class="op">=</span> F.conv2d(</span>
<span id="cb4-24"><a href="#cb4-24"></a>            <span class="bu">input</span>,</span>
<span id="cb4-25"><a href="#cb4-25"></a>            <span class="va">self</span>.weight <span class="op">*</span> <span class="va">self</span>.scale,</span>
<span id="cb4-26"><a href="#cb4-26"></a>            bias<span class="op">=</span><span class="va">self</span>.bias,</span>
<span id="cb4-27"><a href="#cb4-27"></a>            stride<span class="op">=</span><span class="va">self</span>.stride,</span>
<span id="cb4-28"><a href="#cb4-28"></a>            padding<span class="op">=</span><span class="va">self</span>.padding,</span>
<span id="cb4-29"><a href="#cb4-29"></a>        )</span>
<span id="cb4-30"><a href="#cb4-30"></a></span>
<span id="cb4-31"><a href="#cb4-31"></a>        <span class="cf">return</span> out</span>
<span id="cb4-32"><a href="#cb4-32"></a></span>
<span id="cb4-33"><a href="#cb4-33"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb4-34"><a href="#cb4-34"></a>        <span class="cf">return</span> (</span>
<span id="cb4-35"><a href="#cb4-35"></a>            <span class="ss">f"</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>__class__<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">(</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>weight<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>weight<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">,"</span></span>
<span id="cb4-36"><a href="#cb4-36"></a>            <span class="ss">f" </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>weight<span class="sc">.</span>shape[<span class="dv">2</span>]<span class="sc">}</span><span class="ss">, stride=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>stride<span class="sc">}</span><span class="ss">, padding=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>padding<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb4-37"><a href="#cb4-37"></a>        )</span>
<span id="cb4-38"><a href="#cb4-38"></a></span>
<span id="cb4-39"><a href="#cb4-39"></a><span class="kw">class</span> EqualLRLinear(nn.Module):</span>
<span id="cb4-40"><a href="#cb4-40"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb4-41"><a href="#cb4-41"></a>        <span class="va">self</span>, in_dim, out_dim, bias<span class="op">=</span><span class="va">True</span>, bias_init<span class="op">=</span><span class="dv">0</span>, lr_mul<span class="op">=</span><span class="fl">0.01</span>, activation<span class="op">=</span><span class="va">None</span>  <span class="co"># lr_mul from rosinality </span></span>
<span id="cb4-42"><a href="#cb4-42"></a>    ):</span>
<span id="cb4-43"><a href="#cb4-43"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-44"><a href="#cb4-44"></a></span>
<span id="cb4-45"><a href="#cb4-45"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(torch.randn(out_dim, in_dim).div_(lr_mul))</span>
<span id="cb4-46"><a href="#cb4-46"></a></span>
<span id="cb4-47"><a href="#cb4-47"></a>        <span class="cf">if</span> bias:</span>
<span id="cb4-48"><a href="#cb4-48"></a>            <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(out_dim).fill_(bias_init))</span>
<span id="cb4-49"><a href="#cb4-49"></a></span>
<span id="cb4-50"><a href="#cb4-50"></a>        <span class="cf">else</span>:</span>
<span id="cb4-51"><a href="#cb4-51"></a>            <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(out_dim))</span>
<span id="cb4-52"><a href="#cb4-52"></a></span>
<span id="cb4-53"><a href="#cb4-53"></a>        <span class="va">self</span>.activation <span class="op">=</span> activation</span>
<span id="cb4-54"><a href="#cb4-54"></a></span>
<span id="cb4-55"><a href="#cb4-55"></a>        <span class="va">self</span>.scale <span class="op">=</span> (<span class="dv">1</span> <span class="op">/</span> math.sqrt(in_dim)) <span class="op">*</span> lr_mul</span>
<span id="cb4-56"><a href="#cb4-56"></a>        <span class="va">self</span>.lr_mul <span class="op">=</span> lr_mul</span>
<span id="cb4-57"><a href="#cb4-57"></a></span>
<span id="cb4-58"><a href="#cb4-58"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb4-59"><a href="#cb4-59"></a>        <span class="cf">if</span> <span class="va">self</span>.activation:</span>
<span id="cb4-60"><a href="#cb4-60"></a>            out <span class="op">=</span> F.linear(<span class="bu">input</span>, <span class="va">self</span>.weight <span class="op">*</span> <span class="va">self</span>.scale)</span>
<span id="cb4-61"><a href="#cb4-61"></a>            out <span class="op">=</span> fused_leaky_relu(out, <span class="va">self</span>.bias <span class="op">*</span> <span class="va">self</span>.lr_mul)</span>
<span id="cb4-62"><a href="#cb4-62"></a>        <span class="cf">else</span>:</span>
<span id="cb4-63"><a href="#cb4-63"></a>            out <span class="op">=</span> F.linear(</span>
<span id="cb4-64"><a href="#cb4-64"></a>                <span class="bu">input</span>, <span class="va">self</span>.weight <span class="op">*</span> <span class="va">self</span>.scale, bias<span class="op">=</span><span class="va">self</span>.bias <span class="op">*</span> <span class="va">self</span>.lr_mul</span>
<span id="cb4-65"><a href="#cb4-65"></a>            )</span>
<span id="cb4-66"><a href="#cb4-66"></a></span>
<span id="cb4-67"><a href="#cb4-67"></a>        <span class="cf">return</span> out</span>
<span id="cb4-68"><a href="#cb4-68"></a></span>
<span id="cb4-69"><a href="#cb4-69"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb4-70"><a href="#cb4-70"></a>        <span class="cf">return</span> (</span>
<span id="cb4-71"><a href="#cb4-71"></a>            <span class="ss">f"</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>__class__<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">(</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>weight<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>weight<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb4-72"><a href="#cb4-72"></a>        )</span>
<span id="cb4-73"><a href="#cb4-73"></a></span>
<span id="cb4-74"><a href="#cb4-74"></a><span class="kw">def</span> EMA(model1, model2, decay<span class="op">=</span><span class="fl">0.999</span>):</span>
<span id="cb4-75"><a href="#cb4-75"></a>    par1 <span class="op">=</span> <span class="bu">dict</span>(model1.named_parameters())</span>
<span id="cb4-76"><a href="#cb4-76"></a>    par2 <span class="op">=</span> <span class="bu">dict</span>(model2.named_parameters())</span>
<span id="cb4-77"><a href="#cb4-77"></a></span>
<span id="cb4-78"><a href="#cb4-78"></a>    <span class="cf">for</span> k <span class="kw">in</span> par1.keys():</span>
<span id="cb4-79"><a href="#cb4-79"></a>        par1[k].data.mul_(decay).add_(par2[k].data, alpha<span class="op">=</span><span class="dv">1</span> <span class="op">-</span> decay) </span>
<span id="cb4-80"><a href="#cb4-80"></a></span>
<span id="cb4-81"><a href="#cb4-81"></a><span class="kw">class</span> PixelNorm(nn.Module):</span>
<span id="cb4-82"><a href="#cb4-82"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb4-83"><a href="#cb4-83"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-84"><a href="#cb4-84"></a></span>
<span id="cb4-85"><a href="#cb4-85"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-86"><a href="#cb4-86"></a>        <span class="cf">return</span> x <span class="op">/</span> torch.sqrt(torch.mean(x <span class="op">**</span> <span class="dv">2</span>, dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)<span class="op">+</span> <span class="fl">1e-8</span>)</span>
<span id="cb4-87"><a href="#cb4-87"></a></span>
<span id="cb4-88"><a href="#cb4-88"></a><span class="kw">class</span> MiniBatchStdDev(nn.Module):</span>
<span id="cb4-89"><a href="#cb4-89"></a>    <span class="co"># The original StyleGAN paper states 8, but in StyleGAN2 repo they use 4 when they have batch size of 32</span></span>
<span id="cb4-90"><a href="#cb4-90"></a>    <span class="co">#&nbsp;I set batch_size=16 and still keep group_size=4.</span></span>
<span id="cb4-91"><a href="#cb4-91"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, group_size<span class="op">=</span><span class="dv">4</span>):  </span>
<span id="cb4-92"><a href="#cb4-92"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-93"><a href="#cb4-93"></a>        <span class="va">self</span>.group_size <span class="op">=</span> group_size</span>
<span id="cb4-94"><a href="#cb4-94"></a>    </span>
<span id="cb4-95"><a href="#cb4-95"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-96"><a href="#cb4-96"></a>        N, C, H, W <span class="op">=</span> x.shape </span>
<span id="cb4-97"><a href="#cb4-97"></a>        G <span class="op">=</span> <span class="bu">min</span>(<span class="va">self</span>.group_size, N) </span>
<span id="cb4-98"><a href="#cb4-98"></a>        </span>
<span id="cb4-99"><a href="#cb4-99"></a>        y <span class="op">=</span> x.view(G, <span class="op">-</span><span class="dv">1</span>, C, H, W)</span>
<span id="cb4-100"><a href="#cb4-100"></a>        </span>
<span id="cb4-101"><a href="#cb4-101"></a>        y <span class="op">=</span> y <span class="op">-</span> torch.mean(y, dim<span class="op">=</span><span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-102"><a href="#cb4-102"></a>        y <span class="op">=</span> torch.mean(torch.square(y), dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-103"><a href="#cb4-103"></a>        y <span class="op">=</span> torch.sqrt(y <span class="op">+</span> <span class="fl">1e-8</span>)</span>
<span id="cb4-104"><a href="#cb4-104"></a>        </span>
<span id="cb4-105"><a href="#cb4-105"></a>        y <span class="op">=</span> torch.mean(y, dim<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>], keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-106"><a href="#cb4-106"></a>        </span>
<span id="cb4-107"><a href="#cb4-107"></a>        y <span class="op">=</span> y.repeat(G, <span class="dv">1</span>, H, W)</span>
<span id="cb4-108"><a href="#cb4-108"></a>        </span>
<span id="cb4-109"><a href="#cb4-109"></a>        <span class="cf">return</span> torch.cat([x,y], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-110"><a href="#cb4-110"></a></span>
<span id="cb4-111"><a href="#cb4-111"></a><span class="kw">class</span> LearnedConstant(nn.Module):</span>
<span id="cb4-112"><a href="#cb4-112"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_c):</span>
<span id="cb4-113"><a href="#cb4-113"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-114"><a href="#cb4-114"></a>        </span>
<span id="cb4-115"><a href="#cb4-115"></a>        <span class="va">self</span>.constant <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>, in_c, <span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb4-116"><a href="#cb4-116"></a>        </span>
<span id="cb4-117"><a href="#cb4-117"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, batch_size):</span>
<span id="cb4-118"><a href="#cb4-118"></a>        <span class="cf">return</span> <span class="va">self</span>.constant.expand(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-119"><a href="#cb4-119"></a></span>
<span id="cb4-120"><a href="#cb4-120"></a><span class="kw">class</span> NoiseLayer(nn.Module):</span>
<span id="cb4-121"><a href="#cb4-121"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, channels):</span>
<span id="cb4-122"><a href="#cb4-122"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-123"><a href="#cb4-123"></a>        </span>
<span id="cb4-124"><a href="#cb4-124"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, channels, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb4-125"><a href="#cb4-125"></a>    </span>
<span id="cb4-126"><a href="#cb4-126"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, gen_image, noise<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-127"><a href="#cb4-127"></a>        <span class="cf">if</span> noise <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-128"><a href="#cb4-128"></a>            N, _, H, W <span class="op">=</span> gen_image.shape</span>
<span id="cb4-129"><a href="#cb4-129"></a>            noise <span class="op">=</span> torch.randn(N, <span class="dv">1</span>, H, W, device<span class="op">=</span>gen_image.device)</span>
<span id="cb4-130"><a href="#cb4-130"></a>        </span>
<span id="cb4-131"><a href="#cb4-131"></a>        <span class="cf">return</span> gen_image <span class="op">+</span> (noise <span class="op">*</span> <span class="va">self</span>.weight)</span>
<span id="cb4-132"><a href="#cb4-132"></a></span>
<span id="cb4-133"><a href="#cb4-133"></a><span class="kw">class</span> MappingNetwork(nn.Module):</span>
<span id="cb4-134"><a href="#cb4-134"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb4-135"><a href="#cb4-135"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-136"><a href="#cb4-136"></a>        </span>
<span id="cb4-137"><a href="#cb4-137"></a>        <span class="va">self</span>.norm <span class="op">=</span> PixelNorm()</span>
<span id="cb4-138"><a href="#cb4-138"></a>        </span>
<span id="cb4-139"><a href="#cb4-139"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.Sequential(</span>
<span id="cb4-140"><a href="#cb4-140"></a>            EqualLRLinear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb4-141"><a href="#cb4-141"></a>            nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb4-142"><a href="#cb4-142"></a>            EqualLRLinear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb4-143"><a href="#cb4-143"></a>            nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb4-144"><a href="#cb4-144"></a>            EqualLRLinear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb4-145"><a href="#cb4-145"></a>            nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb4-146"><a href="#cb4-146"></a>            EqualLRLinear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb4-147"><a href="#cb4-147"></a>            nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb4-148"><a href="#cb4-148"></a>            EqualLRLinear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb4-149"><a href="#cb4-149"></a>            nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb4-150"><a href="#cb4-150"></a>            EqualLRLinear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb4-151"><a href="#cb4-151"></a>            nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb4-152"><a href="#cb4-152"></a>            EqualLRLinear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb4-153"><a href="#cb4-153"></a>            nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb4-154"><a href="#cb4-154"></a>            EqualLRLinear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb4-155"><a href="#cb4-155"></a>            nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb4-156"><a href="#cb4-156"></a>        )</span>
<span id="cb4-157"><a href="#cb4-157"></a>    </span>
<span id="cb4-158"><a href="#cb4-158"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-159"><a href="#cb4-159"></a>        <span class="co"># Normalise input latent</span></span>
<span id="cb4-160"><a href="#cb4-160"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb4-161"><a href="#cb4-161"></a>        </span>
<span id="cb4-162"><a href="#cb4-162"></a>        out <span class="op">=</span> <span class="va">self</span>.layers(x)</span>
<span id="cb4-163"><a href="#cb4-163"></a>        </span>
<span id="cb4-164"><a href="#cb4-164"></a>        <span class="cf">return</span> out</span>
<span id="cb4-165"><a href="#cb4-165"></a></span>
<span id="cb4-166"><a href="#cb4-166"></a><span class="co">#&nbsp;To get the params for the mapping network we look for parameters with "mapping" in the name</span></span>
<span id="cb4-167"><a href="#cb4-167"></a><span class="kw">def</span> get_params_with_lr(model):</span>
<span id="cb4-168"><a href="#cb4-168"></a>    mapping_params <span class="op">=</span> []</span>
<span id="cb4-169"><a href="#cb4-169"></a>    other_params <span class="op">=</span> []</span>
<span id="cb4-170"><a href="#cb4-170"></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb4-171"><a href="#cb4-171"></a>        <span class="cf">if</span> <span class="st">'mapping'</span> <span class="kw">in</span> name:  <span class="co"># Adjust this condition based on your actual naming convention</span></span>
<span id="cb4-172"><a href="#cb4-172"></a>            mapping_params.append(param)</span>
<span id="cb4-173"><a href="#cb4-173"></a>        <span class="cf">else</span>:</span>
<span id="cb4-174"><a href="#cb4-174"></a>            other_params.append(param)</span>
<span id="cb4-175"><a href="#cb4-175"></a>    <span class="cf">return</span> mapping_params, other_params</span>
<span id="cb4-176"><a href="#cb4-176"></a></span>
<span id="cb4-177"><a href="#cb4-177"></a><span class="co"># Now sample 30k fake and add them to fid</span></span>
<span id="cb4-178"><a href="#cb4-178"></a><span class="kw">def</span> resize(images):</span>
<span id="cb4-179"><a href="#cb4-179"></a>    <span class="co"># Resize to 299x299, the inception v3 model expects 299,299 images so we just resize our images to</span></span>
<span id="cb4-180"><a href="#cb4-180"></a>    <span class="co"># this size</span></span>
<span id="cb4-181"><a href="#cb4-181"></a>    transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb4-182"><a href="#cb4-182"></a>        transforms.Resize((<span class="dv">299</span>, <span class="dv">299</span>)),</span>
<span id="cb4-183"><a href="#cb4-183"></a>    ])</span>
<span id="cb4-184"><a href="#cb4-184"></a>    <span class="cf">return</span> transform(images)</span>
<span id="cb4-185"><a href="#cb4-185"></a></span>
<span id="cb4-186"><a href="#cb4-186"></a><span class="co"># We make use of the FID class provided by the torchmetrics library provided by PyTorch Lightning</span></span>
<span id="cb4-187"><a href="#cb4-187"></a><span class="co"># This class works by "adding" fake and real images to the model</span></span>
<span id="cb4-188"><a href="#cb4-188"></a><span class="co">#&nbsp;So I create two functions, one for adding fake images and one for real images</span></span>
<span id="cb4-189"><a href="#cb4-189"></a><span class="kw">def</span> add_fake_images(g_running, num_images, batch_size, latent_dim, device):</span>
<span id="cb4-190"><a href="#cb4-190"></a>    <span class="co">#&nbsp;The function takes in g_running and all params needed to generate images</span></span>
<span id="cb4-191"><a href="#cb4-191"></a>    <span class="co"># we use g_running in this function as it is the model we use to output our fake images</span></span>
<span id="cb4-192"><a href="#cb4-192"></a>    g_running.<span class="bu">eval</span>()</span>
<span id="cb4-193"><a href="#cb4-193"></a>    </span>
<span id="cb4-194"><a href="#cb4-194"></a>    <span class="co"># Set torch.no_grad to turn off gradients, it makes the code run faster and use less memory as gradients</span></span>
<span id="cb4-195"><a href="#cb4-195"></a>    <span class="co">#&nbsp;aren't tracked</span></span>
<span id="cb4-196"><a href="#cb4-196"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb4-197"><a href="#cb4-197"></a>        <span class="co"># Generate 70000 images to pass to the FID model, we pass them as we generate them to save</span></span>
<span id="cb4-198"><a href="#cb4-198"></a>        <span class="co"># images</span></span>
<span id="cb4-199"><a href="#cb4-199"></a>        <span class="cf">for</span> _ <span class="kw">in</span> tqdm(<span class="bu">range</span>(<span class="dv">0</span>, num_images, batch_size), desc<span class="op">=</span><span class="st">"Generating images"</span>):</span>
<span id="cb4-200"><a href="#cb4-200"></a>            z <span class="op">=</span> torch.randn(batch_size, latent_dim, device<span class="op">=</span>device)</span>
<span id="cb4-201"><a href="#cb4-201"></a>            batch_images <span class="op">=</span> g_running(z)</span>
<span id="cb4-202"><a href="#cb4-202"></a>        </span>
<span id="cb4-203"><a href="#cb4-203"></a>            <span class="co"># resize images</span></span>
<span id="cb4-204"><a href="#cb4-204"></a>            resize_batch <span class="op">=</span> resize(batch_images)</span>
<span id="cb4-205"><a href="#cb4-205"></a>            <span class="co"># Inception v3 requires pixel ranges to be [0,255] currently it's [-1,1], </span></span>
<span id="cb4-206"><a href="#cb4-206"></a>            <span class="co"># this line handles the conversion</span></span>
<span id="cb4-207"><a href="#cb4-207"></a>            resize_batch <span class="op">=</span> ((resize_batch <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> <span class="fl">127.5</span>).clamp(<span class="dv">0</span>, <span class="dv">255</span>)</span>
<span id="cb4-208"><a href="#cb4-208"></a>            <span class="co"># Inception v3 also expects input data type to be uint8, this can be handled with a simple cast</span></span>
<span id="cb4-209"><a href="#cb4-209"></a>            resize_batch <span class="op">=</span> resize_batch.to(torch.uint8)</span>
<span id="cb4-210"><a href="#cb4-210"></a>            </span>
<span id="cb4-211"><a href="#cb4-211"></a>            <span class="co"># Update FID</span></span>
<span id="cb4-212"><a href="#cb4-212"></a>            fid.update(resize_batch, real<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-213"><a href="#cb4-213"></a>            </span>
<span id="cb4-214"><a href="#cb4-214"></a>            <span class="co"># Clear GPU cache, to save memory</span></span>
<span id="cb4-215"><a href="#cb4-215"></a>            torch.cuda.empty_cache()</span>
<span id="cb4-216"><a href="#cb4-216"></a></span>
<span id="cb4-217"><a href="#cb4-217"></a><span class="co">#&nbsp;The second function just takes in the data_loader as a parm</span></span>
<span id="cb4-218"><a href="#cb4-218"></a><span class="kw">def</span> add_real_imgs(data_loader):</span>
<span id="cb4-219"><a href="#cb4-219"></a>    <span class="co"># we pass all batches to the FID model i.e. 70k images</span></span>
<span id="cb4-220"><a href="#cb4-220"></a>    <span class="cf">for</span> batch <span class="kw">in</span> tqdm(data_loader, desc<span class="op">=</span><span class="st">"Processing real images"</span>):</span>
<span id="cb4-221"><a href="#cb4-221"></a>        imgs, _ <span class="op">=</span> batch</span>
<span id="cb4-222"><a href="#cb4-222"></a>        <span class="co"># Resize, convert to [0,255] range and cast to uint8 as before</span></span>
<span id="cb4-223"><a href="#cb4-223"></a>        imgs <span class="op">=</span> resize(imgs)</span>
<span id="cb4-224"><a href="#cb4-224"></a>        imgs <span class="op">=</span> imgs.to(device)</span>
<span id="cb4-225"><a href="#cb4-225"></a>        imgs <span class="op">=</span> ((imgs <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> <span class="fl">127.5</span>).clamp(<span class="dv">0</span>, <span class="dv">255</span>)</span>
<span id="cb4-226"><a href="#cb4-226"></a>        imgs <span class="op">=</span> imgs.to(torch.uint8)</span>
<span id="cb4-227"><a href="#cb4-227"></a>        fid.update(imgs, real<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-228"><a href="#cb4-228"></a></span>
<span id="cb4-229"><a href="#cb4-229"></a>        <span class="kw">del</span> imgs</span>
<span id="cb4-230"><a href="#cb4-230"></a>        torch.cuda.empty_cache()</span>
<span id="cb4-231"><a href="#cb4-231"></a></span>
<span id="cb4-232"><a href="#cb4-232"></a><span class="co"># Final function which combines both of the image adding functions</span></span>
<span id="cb4-233"><a href="#cb4-233"></a><span class="kw">def</span> calculate_and_save_fid(iteration, data_loader, g_running, num_fake_images, batch_size, latent_dim, device, fid_file):</span>
<span id="cb4-234"><a href="#cb4-234"></a>    <span class="co">#&nbsp;We reset the FID score statistics for recalculation</span></span>
<span id="cb4-235"><a href="#cb4-235"></a>    fid.reset()</span>
<span id="cb4-236"><a href="#cb4-236"></a>    <span class="co"># Add the real and fake images</span></span>
<span id="cb4-237"><a href="#cb4-237"></a>    add_fake_images(g_running, num_fake_images, batch_size, latent_dim, device)</span>
<span id="cb4-238"><a href="#cb4-238"></a>    add_real_imgs(data_loader)</span>
<span id="cb4-239"><a href="#cb4-239"></a>    <span class="co"># Compute FID score and output it</span></span>
<span id="cb4-240"><a href="#cb4-240"></a>    fid_score <span class="op">=</span> fid.compute()</span>
<span id="cb4-241"><a href="#cb4-241"></a>    <span class="bu">print</span>(<span class="ss">f"FID score for iteration </span><span class="sc">{</span>iteration<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>fid_score<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-242"><a href="#cb4-242"></a>    </span>
<span id="cb4-243"><a href="#cb4-243"></a>    <span class="co">#&nbsp;We also save the scores to a file</span></span>
<span id="cb4-244"><a href="#cb4-244"></a>    <span class="cf">with</span> <span class="bu">open</span>(fid_file, <span class="st">'a'</span>) <span class="im">as</span> f:</span>
<span id="cb4-245"><a href="#cb4-245"></a>        f.write(<span class="ss">f"Iteration </span><span class="sc">{</span>iteration<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>fid_score<span class="sc">.</span>item()<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<hr>
<section id="the-new-style-block" class="level3">
<h3 class="anchored" data-anchor-id="the-new-style-block">1 - The New Style Block</h3>
<p>This part requires two steps, first we implement the modulation and demodulation. Then we create a new style block to accomodate. Refer to Figure 5 for a mapping between the image and pieces of code.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/a3d207f9-620f-46a4-9ce5-82f189a28677-1-283aa2d0-ee85-4753-a33a-8d7b96ec2d77.png" class="img-fluid figure-img"></p>
<figcaption>Figure 5 - Order of Implementation</figcaption>
</figure>
</div>
<div id="7cc67308-7f22-4da3-b10d-5718ce600578" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">class</span> Conv2d_mod(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, kernel_size, latent_dim, padding):</span>
<span id="cb5-3"><a href="#cb5-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4"></a>        </span>
<span id="cb5-5"><a href="#cb5-5"></a>        <span class="co"># Weight initialization for our conv</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(</span>
<span id="cb5-7"><a href="#cb5-7"></a>            torch.randn(<span class="dv">1</span>, out_channels, in_channels, kernel_size, kernel_size)</span>
<span id="cb5-8"><a href="#cb5-8"></a>        )</span>
<span id="cb5-9"><a href="#cb5-9"></a>        </span>
<span id="cb5-10"><a href="#cb5-10"></a>        <span class="va">self</span>.scale <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> math.sqrt(in_channels <span class="op">*</span> kernel_size <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb5-11"><a href="#cb5-11"></a>        </span>
<span id="cb5-12"><a href="#cb5-12"></a>        <span class="co"># Style modulation layer, just a linear layer. Which takes the incoming style vector w and </span></span>
<span id="cb5-13"><a href="#cb5-13"></a>        <span class="co"># aligns it with number of feature maps the network expects</span></span>
<span id="cb5-14"><a href="#cb5-14"></a>        <span class="va">self</span>.modulation <span class="op">=</span> EqualLRLinear(latent_dim, in_channels, bias_init<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-15"><a href="#cb5-15"></a>        </span>
<span id="cb5-16"><a href="#cb5-16"></a>        <span class="va">self</span>.in_channels <span class="op">=</span> in_channels</span>
<span id="cb5-17"><a href="#cb5-17"></a>        <span class="va">self</span>.out_channels <span class="op">=</span> out_channels</span>
<span id="cb5-18"><a href="#cb5-18"></a>        <span class="va">self</span>.kernel_size <span class="op">=</span> kernel_size</span>
<span id="cb5-19"><a href="#cb5-19"></a>        <span class="va">self</span>.padding <span class="op">=</span> kernel_size <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb5-20"><a href="#cb5-20"></a></span>
<span id="cb5-21"><a href="#cb5-21"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, style):</span>
<span id="cb5-22"><a href="#cb5-22"></a>        batch, channels, height, width <span class="op">=</span> x.shape</span>
<span id="cb5-23"><a href="#cb5-23"></a>        </span>
<span id="cb5-24"><a href="#cb5-24"></a>        <span class="co"># Style modulation</span></span>
<span id="cb5-25"><a href="#cb5-25"></a>        style <span class="op">=</span> <span class="va">self</span>.modulation(style).view(batch, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb5-26"><a href="#cb5-26"></a>        </span>
<span id="cb5-27"><a href="#cb5-27"></a>        <span class="co"># Scale weights and apply style - here weight = w'</span></span>
<span id="cb5-28"><a href="#cb5-28"></a>        weight <span class="op">=</span> <span class="va">self</span>.scale <span class="op">*</span> <span class="va">self</span>.weight <span class="op">*</span> style</span>
<span id="cb5-29"><a href="#cb5-29"></a>        </span>
<span id="cb5-30"><a href="#cb5-30"></a>        <span class="co"># Demodulation - Implementation of equation (2)</span></span>
<span id="cb5-31"><a href="#cb5-31"></a>        demod <span class="op">=</span> torch.rsqrt(weight.<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>([<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]) <span class="op">+</span> <span class="fl">1e-8</span>)</span>
<span id="cb5-32"><a href="#cb5-32"></a>        <span class="co">#&nbsp;rsqrt means 1/sqrt, so weight * demod is the same as what is in the equation (2)</span></span>
<span id="cb5-33"><a href="#cb5-33"></a>        weight <span class="op">=</span> weight <span class="op">*</span> demod.view(batch, <span class="va">self</span>.out_channels, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb5-34"><a href="#cb5-34"></a>        </span>
<span id="cb5-35"><a href="#cb5-35"></a>        <span class="co"># Reshape for grouped convolution</span></span>
<span id="cb5-36"><a href="#cb5-36"></a>        weight <span class="op">=</span> weight.view(</span>
<span id="cb5-37"><a href="#cb5-37"></a>            batch <span class="op">*</span> <span class="va">self</span>.out_channels, <span class="va">self</span>.in_channels, </span>
<span id="cb5-38"><a href="#cb5-38"></a>            <span class="va">self</span>.kernel_size, <span class="va">self</span>.kernel_size</span>
<span id="cb5-39"><a href="#cb5-39"></a>        )</span>
<span id="cb5-40"><a href="#cb5-40"></a>        </span>
<span id="cb5-41"><a href="#cb5-41"></a>        <span class="co"># Reshape input and perform convolution</span></span>
<span id="cb5-42"><a href="#cb5-42"></a>        x <span class="op">=</span> x.view(<span class="dv">1</span>, batch <span class="op">*</span> channels, height, width)</span>
<span id="cb5-43"><a href="#cb5-43"></a>        out <span class="op">=</span> F.conv2d(</span>
<span id="cb5-44"><a href="#cb5-44"></a>            x, </span>
<span id="cb5-45"><a href="#cb5-45"></a>            weight,</span>
<span id="cb5-46"><a href="#cb5-46"></a>            padding<span class="op">=</span><span class="va">self</span>.padding,</span>
<span id="cb5-47"><a href="#cb5-47"></a>            groups<span class="op">=</span>batch</span>
<span id="cb5-48"><a href="#cb5-48"></a>        )</span>
<span id="cb5-49"><a href="#cb5-49"></a>        </span>
<span id="cb5-50"><a href="#cb5-50"></a>        <span class="co"># Reshape output</span></span>
<span id="cb5-51"><a href="#cb5-51"></a>        _, _, height, width <span class="op">=</span> out.shape</span>
<span id="cb5-52"><a href="#cb5-52"></a>        out <span class="op">=</span> out.view(batch, <span class="va">self</span>.out_channels, height, width)</span>
<span id="cb5-53"><a href="#cb5-53"></a>        </span>
<span id="cb5-54"><a href="#cb5-54"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Thats the mod demod operation done, to better understand some of the reshapes and operations let’s explore a little with a test case.</p>
<div id="c12e8e88-7d3e-4ac7-bbf5-79ab223a9ee1" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>test_w <span class="op">=</span> torch.rand(<span class="dv">4</span>, <span class="dv">512</span>)  <span class="co">#&nbsp;Represents a latent vector w after z is passed through the mapping network</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="co"># this w vector has a batch size of 4. In the real network we set batch size to 32 but for illustration we can use 4</span></span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co"># Let's review the old style transfer methodology</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>map_layer <span class="op">=</span> EqualLRLinear(<span class="dv">512</span>, <span class="dv">512</span><span class="op">*</span><span class="dv">2</span>)  <span class="co"># Remeber that we split the mapped w into 2 for the y_s and y_b hence the out dim is 512x2</span></span>
<span id="cb6-6"><a href="#cb6-6"></a>style <span class="op">=</span> map_layer(test_w)</span>
<span id="cb6-7"><a href="#cb6-7"></a></span>
<span id="cb6-8"><a href="#cb6-8"></a>y_s, y_b <span class="op">=</span> style.chunk(<span class="dv">2</span>, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-9"><a href="#cb6-9"></a>        </span>
<span id="cb6-10"><a href="#cb6-10"></a><span class="co"># Reshape y_s and y_b to match x's dimensions</span></span>
<span id="cb6-11"><a href="#cb6-11"></a>y_s <span class="op">=</span> y_s.unsqueeze(<span class="dv">2</span>).unsqueeze(<span class="dv">3</span>)</span>
<span id="cb6-12"><a href="#cb6-12"></a>y_b <span class="op">=</span> y_b.unsqueeze(<span class="dv">2</span>).unsqueeze(<span class="dv">3</span>)</span>
<span id="cb6-13"><a href="#cb6-13"></a></span>
<span id="cb6-14"><a href="#cb6-14"></a><span class="bu">print</span>(y_s.shape, y_b.shape)</span>
<span id="cb6-15"><a href="#cb6-15"></a><span class="bu">print</span>(<span class="st">'---'</span>)</span>
<span id="cb6-16"><a href="#cb6-16"></a><span class="co"># Now here is the new style transfer code with mod and demod - note this is just to show the different resizings we perform.</span></span>
<span id="cb6-17"><a href="#cb6-17"></a>map_layer <span class="op">=</span> EqualLRLinear(<span class="dv">512</span>, <span class="dv">512</span>)</span>
<span id="cb6-18"><a href="#cb6-18"></a>s <span class="op">=</span> map_layer(test_w)</span>
<span id="cb6-19"><a href="#cb6-19"></a><span class="bu">print</span>(<span class="ss">f's shape before view: </span><span class="sc">{</span>s<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-20"><a href="#cb6-20"></a>s <span class="op">=</span> s.view(test_w.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)  <span class="co"># Expand to match dims of img, we match batch size and feature maps, but add 3 dims for RGB</span></span>
<span id="cb6-21"><a href="#cb6-21"></a><span class="bu">print</span>(<span class="ss">f's shape after view: </span><span class="sc">{</span>s<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-22"><a href="#cb6-22"></a></span>
<span id="cb6-23"><a href="#cb6-23"></a><span class="bu">print</span>()</span>
<span id="cb6-24"><a href="#cb6-24"></a></span>
<span id="cb6-25"><a href="#cb6-25"></a><span class="co">#&nbsp;Example w which is the weights of a conv layer</span></span>
<span id="cb6-26"><a href="#cb6-26"></a><span class="co"># w = weights here this is the shape of a convolutional layers weights in PyTorch</span></span>
<span id="cb6-27"><a href="#cb6-27"></a>w <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">512</span>, <span class="dv">512</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb6-28"><a href="#cb6-28"></a><span class="bu">print</span>(<span class="ss">f'w shape before mod: </span><span class="sc">{</span>w<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-29"><a href="#cb6-29"></a></span>
<span id="cb6-30"><a href="#cb6-30"></a>w <span class="op">=</span> w <span class="op">*</span> s</span>
<span id="cb6-31"><a href="#cb6-31"></a><span class="bu">print</span>(<span class="ss">f'Shape of w after modulation: </span><span class="sc">{</span>w<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>, <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb6-32"><a href="#cb6-32"></a></span>
<span id="cb6-33"><a href="#cb6-33"></a>demod <span class="op">=</span> torch.rsqrt(w.<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>([<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>], keepdim<span class="op">=</span><span class="va">True</span>) <span class="op">+</span> <span class="fl">1e-8</span>)</span>
<span id="cb6-34"><a href="#cb6-34"></a><span class="bu">print</span>(<span class="ss">f'w: </span><span class="sc">{</span>w<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> | demod: </span><span class="sc">{</span>demod<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-35"><a href="#cb6-35"></a>w <span class="op">=</span> w <span class="op">*</span> demod</span>
<span id="cb6-36"><a href="#cb6-36"></a><span class="bu">print</span>(<span class="ss">f'w shape after demod: </span><span class="sc">{</span>w<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-37"><a href="#cb6-37"></a></span>
<span id="cb6-38"><a href="#cb6-38"></a><span class="bu">print</span>(<span class="st">'---'</span>)</span>
<span id="cb6-39"><a href="#cb6-39"></a></span>
<span id="cb6-40"><a href="#cb6-40"></a><span class="co"># This covers the first part of code, namely in the mod_demod function.</span></span>
<span id="cb6-41"><a href="#cb6-41"></a><span class="co"># Let's also examine the code in the Con2d_mod and also explore how the grouped convolution works</span></span>
<span id="cb6-42"><a href="#cb6-42"></a></span>
<span id="cb6-43"><a href="#cb6-43"></a><span class="co"># Conv2d Expects a 4d tensor as it's weight, so we need to convert w to 4d</span></span>
<span id="cb6-44"><a href="#cb6-44"></a><span class="co"># current w has 5 dimensions, the first two can be condensed (batch size and feature maps) so we end up with basically</span></span>
<span id="cb6-45"><a href="#cb6-45"></a><span class="co"># the feature maps for each sample in the batch stacked on top of each other.</span></span>
<span id="cb6-46"><a href="#cb6-46"></a>w <span class="op">=</span> nn.Parameter(w.view(<span class="dv">4</span> <span class="op">*</span> <span class="dv">512</span>, <span class="dv">512</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb6-47"><a href="#cb6-47"></a>groups <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb6-48"><a href="#cb6-48"></a></span>
<span id="cb6-49"><a href="#cb6-49"></a>x <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">512</span>, <span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb6-50"><a href="#cb6-50"></a><span class="co"># Stack x in the same way we stacked the weights</span></span>
<span id="cb6-51"><a href="#cb6-51"></a>x <span class="op">=</span> x.view(<span class="dv">1</span>, <span class="dv">4</span><span class="op">*</span><span class="dv">512</span>, <span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb6-52"><a href="#cb6-52"></a></span>
<span id="cb6-53"><a href="#cb6-53"></a><span class="co"># Set the weights of conv layer to the weights which have been modulated and demodulated.</span></span>
<span id="cb6-54"><a href="#cb6-54"></a>out <span class="op">=</span> F.conv2d(x, w, groups<span class="op">=</span>groups, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-55"><a href="#cb6-55"></a>out <span class="op">=</span> out.view(<span class="dv">4</span>, <span class="op">-</span><span class="dv">1</span>, out.shape[<span class="op">-</span><span class="dv">2</span>], out.shape[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb6-56"><a href="#cb6-56"></a>out.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([4, 512, 1, 1]) torch.Size([4, 512, 1, 1])
---
s shape before view: torch.Size([4, 512])
s shape after view: torch.Size([4, 512, 1, 1, 1])

w shape before mod: torch.Size([512, 512, 3, 3])
Shape of w after modulation: torch.Size([4, 512, 512, 3, 3]) 

w: torch.Size([4, 512, 512, 3, 3]) | demod: torch.Size([4, 512, 1, 1, 1])
w shape after demod: torch.Size([4, 512, 512, 3, 3])
---</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>torch.Size([4, 512, 4, 4])</code></pre>
</div>
</div>
<p>There we have the modulation and demodulation operation, this is applied to the weights of the convolutional layers in the G network. Let’s see how it fits in the new style block for the G!</p>
<div id="ef1ab5c4-f5bd-494e-81a2-baf410f22063" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># The actual style block itself isn't much different than that of StyleGAN, except we change the conv2d with</span></span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="co"># our newly created conv2d_mod and that's it.</span></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="kw">class</span> g_style_block(nn.Module):</span>
<span id="cb9-4"><a href="#cb9-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb9-5"><a href="#cb9-5"></a>        <span class="va">self</span>, </span>
<span id="cb9-6"><a href="#cb9-6"></a>        in_c, </span>
<span id="cb9-7"><a href="#cb9-7"></a>        out_c, </span>
<span id="cb9-8"><a href="#cb9-8"></a>        ksize1, </span>
<span id="cb9-9"><a href="#cb9-9"></a>        padding,</span>
<span id="cb9-10"><a href="#cb9-10"></a>        upsample<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb9-11"><a href="#cb9-11"></a>        latent_dim<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb9-12"><a href="#cb9-12"></a>    ):</span>
<span id="cb9-13"><a href="#cb9-13"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-14"><a href="#cb9-14"></a></span>
<span id="cb9-15"><a href="#cb9-15"></a>        layers_list <span class="op">=</span> []</span>
<span id="cb9-16"><a href="#cb9-16"></a></span>
<span id="cb9-17"><a href="#cb9-17"></a>        <span class="cf">if</span> upsample:</span>
<span id="cb9-18"><a href="#cb9-18"></a>            layers_list.extend([</span>
<span id="cb9-19"><a href="#cb9-19"></a>                nn.Upsample(scale_factor<span class="op">=</span><span class="dv">2</span>, mode<span class="op">=</span><span class="st">'bilinear'</span>),</span>
<span id="cb9-20"><a href="#cb9-20"></a>                Conv2d_mod(in_c, out_c, ksize1, latent_dim, padding<span class="op">=</span>padding),</span>
<span id="cb9-21"><a href="#cb9-21"></a>                NoiseLayer(out_c),</span>
<span id="cb9-22"><a href="#cb9-22"></a>            ])</span>
<span id="cb9-23"><a href="#cb9-23"></a>        <span class="cf">else</span>:</span>
<span id="cb9-24"><a href="#cb9-24"></a>            <span class="va">self</span>.learned_constant <span class="op">=</span> LearnedConstant(in_c)</span>
<span id="cb9-25"><a href="#cb9-25"></a></span>
<span id="cb9-26"><a href="#cb9-26"></a>        layers_list.extend([</span>
<span id="cb9-27"><a href="#cb9-27"></a>            nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb9-28"><a href="#cb9-28"></a>            Conv2d_mod(out_c, out_c, ksize1, latent_dim, padding<span class="op">=</span>padding),</span>
<span id="cb9-29"><a href="#cb9-29"></a>            NoiseLayer(out_c),</span>
<span id="cb9-30"><a href="#cb9-30"></a>            nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb9-31"><a href="#cb9-31"></a>        ])</span>
<span id="cb9-32"><a href="#cb9-32"></a></span>
<span id="cb9-33"><a href="#cb9-33"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList(layers_list)</span>
<span id="cb9-34"><a href="#cb9-34"></a>        <span class="va">self</span>.upsample <span class="op">=</span> upsample</span>
<span id="cb9-35"><a href="#cb9-35"></a>        </span>
<span id="cb9-36"><a href="#cb9-36"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, w, x<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb9-37"><a href="#cb9-37"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.upsample:</span>
<span id="cb9-38"><a href="#cb9-38"></a>            x <span class="op">=</span> <span class="va">self</span>.learned_constant(w.size(<span class="dv">0</span>))</span>
<span id="cb9-39"><a href="#cb9-39"></a>        </span>
<span id="cb9-40"><a href="#cb9-40"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb9-41"><a href="#cb9-41"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(layer, LearnedConstant):</span>
<span id="cb9-42"><a href="#cb9-42"></a>                x <span class="op">=</span> layer()</span>
<span id="cb9-43"><a href="#cb9-43"></a>            <span class="cf">elif</span> <span class="bu">isinstance</span>(layer, Conv2d_mod):</span>
<span id="cb9-44"><a href="#cb9-44"></a>                x <span class="op">=</span> layer(x, w)</span>
<span id="cb9-45"><a href="#cb9-45"></a>            <span class="cf">else</span>:</span>
<span id="cb9-46"><a href="#cb9-46"></a>                x <span class="op">=</span> layer(x)</span>
<span id="cb9-47"><a href="#cb9-47"></a>            </span>
<span id="cb9-48"><a href="#cb9-48"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>While we’re here, we may as well cover the D convolutional block. It differs slightly from the StyleGAN but does not make use of modulation and demodulation.</p>
<div id="d5e06799-b171-490f-82c3-422f41cd4992" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co">#&nbsp;The module we create to apply a blur in the D downsample</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="kw">class</span> Blur(nn.Module):</span>
<span id="cb10-3"><a href="#cb10-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb10-4"><a href="#cb10-4"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-5"><a href="#cb10-5"></a>        f <span class="op">=</span> torch.Tensor([<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1</span>])</span>
<span id="cb10-6"><a href="#cb10-6"></a>        <span class="va">self</span>.register_buffer(<span class="st">'f'</span>, f)</span>
<span id="cb10-7"><a href="#cb10-7"></a></span>
<span id="cb10-8"><a href="#cb10-8"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb10-9"><a href="#cb10-9"></a>        f <span class="op">=</span> <span class="va">self</span>.f</span>
<span id="cb10-10"><a href="#cb10-10"></a>        f <span class="op">=</span> f[<span class="va">None</span>, <span class="va">None</span>, :] <span class="op">*</span> f[<span class="va">None</span>, :, <span class="va">None</span>]</span>
<span id="cb10-11"><a href="#cb10-11"></a>        f <span class="op">=</span> f <span class="op">/</span> f.<span class="bu">sum</span>()</span>
<span id="cb10-12"><a href="#cb10-12"></a>        <span class="cf">return</span> F.conv2d(x, f.expand(x.size(<span class="dv">1</span>), <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), </span>
<span id="cb10-13"><a href="#cb10-13"></a>                        groups<span class="op">=</span>x.size(<span class="dv">1</span>), padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-14"><a href="#cb10-14"></a></span>
<span id="cb10-15"><a href="#cb10-15"></a><span class="co"># D style block is pretty similar too with a few minor changes. We implement the residual connection in this block</span></span>
<span id="cb10-16"><a href="#cb10-16"></a><span class="co">#&nbsp;it makes more sense to me to do it here and we examine exactly what happens in the code here and the exposition to follow</span></span>
<span id="cb10-17"><a href="#cb10-17"></a><span class="kw">class</span> d_style_block(nn.Module):</span>
<span id="cb10-18"><a href="#cb10-18"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb10-19"><a href="#cb10-19"></a>        <span class="va">self</span>,</span>
<span id="cb10-20"><a href="#cb10-20"></a>        in_c,</span>
<span id="cb10-21"><a href="#cb10-21"></a>        out_c,</span>
<span id="cb10-22"><a href="#cb10-22"></a>        ksize1, </span>
<span id="cb10-23"><a href="#cb10-23"></a>        padding,  </span>
<span id="cb10-24"><a href="#cb10-24"></a>        ksize2<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb10-25"><a href="#cb10-25"></a>        padding2<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb10-26"><a href="#cb10-26"></a>        stride<span class="op">=</span><span class="va">None</span>,   </span>
<span id="cb10-27"><a href="#cb10-27"></a>        mbatch<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb10-28"><a href="#cb10-28"></a>    ):</span>
<span id="cb10-29"><a href="#cb10-29"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-30"><a href="#cb10-30"></a></span>
<span id="cb10-31"><a href="#cb10-31"></a>        <span class="cf">if</span> ksize2 <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb10-32"><a href="#cb10-32"></a>            ksize2 <span class="op">=</span> ksize1</span>
<span id="cb10-33"><a href="#cb10-33"></a>        <span class="cf">if</span> padding2 <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb10-34"><a href="#cb10-34"></a>            padding2 <span class="op">=</span> padding</span>
<span id="cb10-35"><a href="#cb10-35"></a>        </span>
<span id="cb10-36"><a href="#cb10-36"></a>        layers_list <span class="op">=</span> []</span>
<span id="cb10-37"><a href="#cb10-37"></a></span>
<span id="cb10-38"><a href="#cb10-38"></a>        <span class="co">#&nbsp;Instead of using bi-linear interpolation as in the StyleGAN we create a downsample using convolutional layers</span></span>
<span id="cb10-39"><a href="#cb10-39"></a>        <span class="co">#&nbsp;and we add a gaussian blur too</span></span>
<span id="cb10-40"><a href="#cb10-40"></a>        <span class="va">self</span>.down_res <span class="op">=</span> nn.Sequential(</span>
<span id="cb10-41"><a href="#cb10-41"></a>            Blur(),</span>
<span id="cb10-42"><a href="#cb10-42"></a>            EqualLRConv2d(in_c, out_c, <span class="dv">3</span>, padding <span class="op">=</span> <span class="dv">1</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb10-43"><a href="#cb10-43"></a>            EqualLRConv2d(out_c, out_c, <span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>, stride<span class="op">=</span><span class="dv">1</span>) <span class="cf">if</span> mbatch <span class="cf">else</span> nn.Identity()  <span class="co"># Needed for last layer otherwise res </span></span>
<span id="cb10-44"><a href="#cb10-44"></a>            <span class="co">#&nbsp;has two H, W instead of 1</span></span>
<span id="cb10-45"><a href="#cb10-45"></a>        )</span>
<span id="cb10-46"><a href="#cb10-46"></a>        </span>
<span id="cb10-47"><a href="#cb10-47"></a>        <span class="cf">if</span> mbatch:</span>
<span id="cb10-48"><a href="#cb10-48"></a>            layers_list.extend([</span>
<span id="cb10-49"><a href="#cb10-49"></a>                MiniBatchStdDev(),</span>
<span id="cb10-50"><a href="#cb10-50"></a>            ])</span>
<span id="cb10-51"><a href="#cb10-51"></a>            in_c <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb10-52"><a href="#cb10-52"></a>            </span>
<span id="cb10-53"><a href="#cb10-53"></a>        layers_list.extend([</span>
<span id="cb10-54"><a href="#cb10-54"></a>            EqualLRConv2d(in_c, in_c, ksize1, padding<span class="op">=</span>padding),</span>
<span id="cb10-55"><a href="#cb10-55"></a>            nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb10-56"><a href="#cb10-56"></a>            EqualLRConv2d(in_c, out_c, ksize2, padding<span class="op">=</span>padding2),</span>
<span id="cb10-57"><a href="#cb10-57"></a>            nn.LeakyReLU(<span class="fl">0.2</span>),</span>
<span id="cb10-58"><a href="#cb10-58"></a>        ])</span>
<span id="cb10-59"><a href="#cb10-59"></a></span>
<span id="cb10-60"><a href="#cb10-60"></a>        <span class="cf">if</span> <span class="kw">not</span> mbatch:  <span class="co"># No downsample if we are on the last layer</span></span>
<span id="cb10-61"><a href="#cb10-61"></a>            layers_list.extend([</span>
<span id="cb10-62"><a href="#cb10-62"></a>                EqualLRConv2d(out_c, out_c, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-63"><a href="#cb10-63"></a>            ])</span>
<span id="cb10-64"><a href="#cb10-64"></a>        </span>
<span id="cb10-65"><a href="#cb10-65"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList(layers_list)</span>
<span id="cb10-66"><a href="#cb10-66"></a>    </span>
<span id="cb10-67"><a href="#cb10-67"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb10-68"><a href="#cb10-68"></a>        <span class="co"># A residual connection is made by summing the output of the previous layer with the output of the current layer</span></span>
<span id="cb10-69"><a href="#cb10-69"></a>        <span class="co"># To do this successfully we need to down sample the output of previous layer and store it</span></span>
<span id="cb10-70"><a href="#cb10-70"></a>        res <span class="op">=</span> <span class="va">self</span>.down_res(x)</span>
<span id="cb10-71"><a href="#cb10-71"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb10-72"><a href="#cb10-72"></a>            x <span class="op">=</span> layer(x)</span>
<span id="cb10-73"><a href="#cb10-73"></a></span>
<span id="cb10-74"><a href="#cb10-74"></a>        <span class="co">#&nbsp;We perform the summation here, the second term is whats called a variance preservation term. When we sum res + x</span></span>
<span id="cb10-75"><a href="#cb10-75"></a>        <span class="co"># we also sum the variance of both tensors. Overtime then the variance would grow, so we scale by 1/sqrt(2)</span></span>
<span id="cb10-76"><a href="#cb10-76"></a>        <span class="co"># E.g: we have res with variance a^2 and x with variance a^2, when we sum them we get out with variance</span></span>
<span id="cb10-77"><a href="#cb10-77"></a>        <span class="co"># 2a^2, multiplying this by 1/sqrt(2) brings variance back to a^2</span></span>
<span id="cb10-78"><a href="#cb10-78"></a>        out <span class="op">=</span> (res <span class="op">+</span> x) <span class="op">*</span> (<span class="dv">1</span><span class="op">/</span> sqrt(<span class="dv">2</span>))       </span>
<span id="cb10-79"><a href="#cb10-79"></a>        </span>
<span id="cb10-80"><a href="#cb10-80"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And there we have it, both the G and D style/conv blocks! With those in place it’s time to introduce the new G and D architectures.</p>
<hr>
</section>
<section id="the-new-architecture" class="level3">
<h3 class="anchored" data-anchor-id="the-new-architecture">2 - The New Architecture</h3>
<p>In this section we cover the removal of progressive growing and the addition of residual connection to the D (this was briefly discussed in the code comments for D style block) and skip connections to the G. I will add Figure 4 back here for our reference (it guides the following implementation)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/8be000ef-66f6-482e-a737-a1c96e528122-1-17fa983d-84fd-42cb-ae05-26d8cefdeab5.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4b - Repeat with only relevant diagrams</figcaption>
</figure>
</div>
</section>
<section id="generator" class="level3">
<h3 class="anchored" data-anchor-id="generator">Generator</h3>
<div id="bae256d0-a706-4030-82bb-131107ac52c8" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="co">#&nbsp;Observe in the G how we have removed progressive growing. Look at the "out" variable in line 55, this variable is passed to </span></span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="co"># every layer right from the first 4x4 resolution layer to the final 256x256 resolution layer. We do not need any control</span></span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="co"># variables (num_layer or alpha) to grow the network, instead we start from iteration 0 at the full size network.</span></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="kw">class</span> Generator(nn.Module):</span>
<span id="cb11-5"><a href="#cb11-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_c<span class="op">=</span><span class="dv">512</span>):</span>
<span id="cb11-6"><a href="#cb11-6"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb11-7"><a href="#cb11-7"></a>        </span>
<span id="cb11-8"><a href="#cb11-8"></a>        <span class="va">self</span>.g_mapping <span class="op">=</span> MappingNetwork()</span>
<span id="cb11-9"><a href="#cb11-9"></a></span>
<span id="cb11-10"><a href="#cb11-10"></a>        <span class="co"># This fmaps params is another little hidden hint from: https://github.com/NVlabs/stylegan2-ada-pytorch/blob/main/train.py#L157</span></span>
<span id="cb11-11"><a href="#cb11-11"></a>        <span class="co">#&nbsp;This is a repo which came out a while after StyleGAN2 repo, this highlights the difficulty of replicating papers.</span></span>
<span id="cb11-12"><a href="#cb11-12"></a>        <span class="co"># In this repo they provide params for a 256x256 StyleGAN2 and one of the changes is to half the number of feature maps</span></span>
<span id="cb11-13"><a href="#cb11-13"></a>        <span class="co"># in the G model.</span></span>
<span id="cb11-14"><a href="#cb11-14"></a>        fmaps <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb11-15"><a href="#cb11-15"></a>        in_c <span class="op">=</span> <span class="bu">int</span>(in_c <span class="op">*</span> fmaps)</span>
<span id="cb11-16"><a href="#cb11-16"></a>        </span>
<span id="cb11-17"><a href="#cb11-17"></a>        <span class="va">self</span>.block_4x4 <span class="op">=</span> g_style_block(in_c, in_c, <span class="dv">3</span>, <span class="dv">1</span>, upsample<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-18"><a href="#cb11-18"></a>        <span class="va">self</span>.block_8x8 <span class="op">=</span> g_style_block(in_c, in_c, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb11-19"><a href="#cb11-19"></a>        <span class="va">self</span>.block_16x16 <span class="op">=</span> g_style_block(in_c, in_c, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb11-20"><a href="#cb11-20"></a>        <span class="va">self</span>.block_32x32 <span class="op">=</span> g_style_block(in_c, in_c, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb11-21"><a href="#cb11-21"></a>        <span class="va">self</span>.block_64x64 <span class="op">=</span> g_style_block(in_c, in_c<span class="op">//</span><span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb11-22"><a href="#cb11-22"></a>        <span class="va">self</span>.block_128x128 <span class="op">=</span> g_style_block(in_c<span class="op">//</span><span class="dv">2</span>, in_c<span class="op">//</span><span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb11-23"><a href="#cb11-23"></a>        <span class="va">self</span>.block_256x256 <span class="op">=</span> g_style_block(in_c<span class="op">//</span><span class="dv">4</span>, in_c<span class="op">//</span><span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb11-24"><a href="#cb11-24"></a></span>
<span id="cb11-25"><a href="#cb11-25"></a>        <span class="va">self</span>.upsample <span class="op">=</span> nn.Upsample(scale_factor<span class="op">=</span><span class="dv">2</span>, mode<span class="op">=</span><span class="st">'bilinear'</span>)</span>
<span id="cb11-26"><a href="#cb11-26"></a>        </span>
<span id="cb11-27"><a href="#cb11-27"></a>        <span class="va">self</span>.to_rgb_4 <span class="op">=</span> EqualLRConv2d(in_c, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb11-28"><a href="#cb11-28"></a>        <span class="va">self</span>.to_rgb_8 <span class="op">=</span> EqualLRConv2d(in_c, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb11-29"><a href="#cb11-29"></a>        <span class="va">self</span>.to_rgb_16 <span class="op">=</span> EqualLRConv2d(in_c, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb11-30"><a href="#cb11-30"></a>        <span class="va">self</span>.to_rgb_32 <span class="op">=</span> EqualLRConv2d(in_c, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb11-31"><a href="#cb11-31"></a>        <span class="va">self</span>.to_rgb_64 <span class="op">=</span> EqualLRConv2d(in_c<span class="op">//</span><span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb11-32"><a href="#cb11-32"></a>        <span class="va">self</span>.to_rgb_128 <span class="op">=</span> EqualLRConv2d(in_c<span class="op">//</span><span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb11-33"><a href="#cb11-33"></a>        <span class="va">self</span>.to_rgb_256 <span class="op">=</span> EqualLRConv2d(in_c<span class="op">//</span><span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb11-34"><a href="#cb11-34"></a>                </span>
<span id="cb11-35"><a href="#cb11-35"></a>        <span class="va">self</span>.tanh <span class="op">=</span> nn.Tanh()</span>
<span id="cb11-36"><a href="#cb11-36"></a></span>
<span id="cb11-37"><a href="#cb11-37"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z, return_latents<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb11-38"><a href="#cb11-38"></a>        w <span class="op">=</span> <span class="va">self</span>.g_mapping(z)</span>
<span id="cb11-39"><a href="#cb11-39"></a>        batch_size <span class="op">=</span> z.size(<span class="dv">0</span>)</span>
<span id="cb11-40"><a href="#cb11-40"></a>        </span>
<span id="cb11-41"><a href="#cb11-41"></a>        <span class="co"># Determine which samples undergo style mixing</span></span>
<span id="cb11-42"><a href="#cb11-42"></a>        <span class="co">#&nbsp;Style mixing is different here than in the StyleGAN post, I think I made a mistake with my previous implementation</span></span>
<span id="cb11-43"><a href="#cb11-43"></a>        <span class="co">#&nbsp;The mistake being that I applied style mixing across the entire batch, meaning either all samples in the batch </span></span>
<span id="cb11-44"><a href="#cb11-44"></a>        <span class="co"># undergo style mixing or none do. This is not what the authors intended, they wanted style mixing to apply per sample</span></span>
<span id="cb11-45"><a href="#cb11-45"></a>        <span class="co"># in the batch. Essentially in the same batch of 16, 32 or whatever images some may get style mixed and others not.</span></span>
<span id="cb11-46"><a href="#cb11-46"></a></span>
<span id="cb11-47"><a href="#cb11-47"></a>        <span class="co">#&nbsp;This mixing variable generates a list of 32 (if batch_size=32) True of False variables which determine whether each</span></span>
<span id="cb11-48"><a href="#cb11-48"></a>        <span class="co"># sample in the batch undergoes style micing</span></span>
<span id="cb11-49"><a href="#cb11-49"></a>        mixing <span class="op">=</span> torch.rand(batch_size, device<span class="op">=</span>z.device) <span class="op">&lt;</span> <span class="fl">0.9</span></span>
<span id="cb11-50"><a href="#cb11-50"></a>        </span>
<span id="cb11-51"><a href="#cb11-51"></a>        <span class="co"># Generate z2 and w2 for samples that undergo style mixing</span></span>
<span id="cb11-52"><a href="#cb11-52"></a>        z2 <span class="op">=</span> torch.randn_like(z)</span>
<span id="cb11-53"><a href="#cb11-53"></a>        w2 <span class="op">=</span> <span class="va">self</span>.g_mapping(z2)</span>
<span id="cb11-54"><a href="#cb11-54"></a>        </span>
<span id="cb11-55"><a href="#cb11-55"></a>        <span class="co"># Generate crossover points for each sample. layers start at 1 and go up to 7</span></span>
<span id="cb11-56"><a href="#cb11-56"></a>        crossover_points <span class="op">=</span> torch.randint(<span class="dv">1</span>, <span class="dv">7</span>, (batch_size,), device<span class="op">=</span>z.device)</span>
<span id="cb11-57"><a href="#cb11-57"></a>        </span>
<span id="cb11-58"><a href="#cb11-58"></a>        <span class="co"># Initialize a list to hold the styles for each layer</span></span>
<span id="cb11-59"><a href="#cb11-59"></a>        styles <span class="op">=</span> []</span>
<span id="cb11-60"><a href="#cb11-60"></a>        </span>
<span id="cb11-61"><a href="#cb11-61"></a>        <span class="co"># For each layer, select w or w2 based on crossover points</span></span>
<span id="cb11-62"><a href="#cb11-62"></a>        <span class="cf">for</span> layer_idx <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">7</span>):  <span class="co"># 7 layers for a 256x256 network, if you increase resolution increase this too</span></span>
<span id="cb11-63"><a href="#cb11-63"></a>            <span class="co"># Create a mask for samples that should use w2 at this layer</span></span>
<span id="cb11-64"><a href="#cb11-64"></a>            use_w2 <span class="op">=</span> mixing <span class="op">&amp;</span> (crossover_points <span class="op">&lt;=</span> layer_idx)</span>
<span id="cb11-65"><a href="#cb11-65"></a>            <span class="co"># Select w or w2 for each sample in this layer</span></span>
<span id="cb11-66"><a href="#cb11-66"></a>            style <span class="op">=</span> torch.where(use_w2.unsqueeze(<span class="dv">1</span>), w2, w)</span>
<span id="cb11-67"><a href="#cb11-67"></a>            styles.append(style)</span>
<span id="cb11-68"><a href="#cb11-68"></a>        </span>
<span id="cb11-69"><a href="#cb11-69"></a>        <span class="co"># Apply styles in generator blocks</span></span>
<span id="cb11-70"><a href="#cb11-70"></a>        out <span class="op">=</span> <span class="va">self</span>.block_4x4(styles[<span class="dv">0</span>])</span>
<span id="cb11-71"><a href="#cb11-71"></a>        out_4 <span class="op">=</span> <span class="va">self</span>.to_rgb_4(out)</span>
<span id="cb11-72"><a href="#cb11-72"></a>        out_4 <span class="op">=</span> <span class="va">self</span>.upsample(out_4)</span>
<span id="cb11-73"><a href="#cb11-73"></a>        </span>
<span id="cb11-74"><a href="#cb11-74"></a>        out <span class="op">=</span> <span class="va">self</span>.block_8x8(styles[<span class="dv">1</span>], out)</span>
<span id="cb11-75"><a href="#cb11-75"></a>        out_8 <span class="op">=</span> <span class="va">self</span>.to_rgb_8(out)</span>
<span id="cb11-76"><a href="#cb11-76"></a>        <span class="co">#&nbsp;We also reset the variance here as we did in d_style_block</span></span>
<span id="cb11-77"><a href="#cb11-77"></a>        out_8 <span class="op">+=</span> out_4 <span class="op">*</span> (<span class="dv">1</span> <span class="op">/</span> np.sqrt(<span class="dv">2</span>))</span>
<span id="cb11-78"><a href="#cb11-78"></a>        out_8 <span class="op">=</span> <span class="va">self</span>.upsample(out_8)</span>
<span id="cb11-79"><a href="#cb11-79"></a>        </span>
<span id="cb11-80"><a href="#cb11-80"></a>        out <span class="op">=</span> <span class="va">self</span>.block_16x16(styles[<span class="dv">2</span>], out)</span>
<span id="cb11-81"><a href="#cb11-81"></a>        out_16 <span class="op">=</span> <span class="va">self</span>.to_rgb_16(out)</span>
<span id="cb11-82"><a href="#cb11-82"></a>        out_16 <span class="op">+=</span> out_8 <span class="op">*</span> (<span class="dv">1</span> <span class="op">/</span> np.sqrt(<span class="dv">2</span>))</span>
<span id="cb11-83"><a href="#cb11-83"></a>        out_16 <span class="op">=</span> <span class="va">self</span>.upsample(out_16)</span>
<span id="cb11-84"><a href="#cb11-84"></a>        </span>
<span id="cb11-85"><a href="#cb11-85"></a>        out <span class="op">=</span> <span class="va">self</span>.block_32x32(styles[<span class="dv">3</span>], out)</span>
<span id="cb11-86"><a href="#cb11-86"></a>        out_32 <span class="op">=</span> <span class="va">self</span>.to_rgb_32(out)</span>
<span id="cb11-87"><a href="#cb11-87"></a>        out_32 <span class="op">+=</span> out_16 <span class="op">*</span> (<span class="dv">1</span> <span class="op">/</span> np.sqrt(<span class="dv">2</span>))</span>
<span id="cb11-88"><a href="#cb11-88"></a>        out_32 <span class="op">=</span> <span class="va">self</span>.upsample(out_32)</span>
<span id="cb11-89"><a href="#cb11-89"></a>        </span>
<span id="cb11-90"><a href="#cb11-90"></a>        out <span class="op">=</span> <span class="va">self</span>.block_64x64(styles[<span class="dv">4</span>], out)</span>
<span id="cb11-91"><a href="#cb11-91"></a>        out_64 <span class="op">=</span> <span class="va">self</span>.to_rgb_64(out)</span>
<span id="cb11-92"><a href="#cb11-92"></a>        out_64 <span class="op">+=</span> out_32 <span class="op">*</span> (<span class="dv">1</span> <span class="op">/</span> np.sqrt(<span class="dv">2</span>))</span>
<span id="cb11-93"><a href="#cb11-93"></a>        out_64 <span class="op">=</span> <span class="va">self</span>.upsample(out_64)</span>
<span id="cb11-94"><a href="#cb11-94"></a></span>
<span id="cb11-95"><a href="#cb11-95"></a>        out <span class="op">=</span> <span class="va">self</span>.block_128x128(styles[<span class="dv">5</span>], out)            </span>
<span id="cb11-96"><a href="#cb11-96"></a>        out_128 <span class="op">=</span> <span class="va">self</span>.to_rgb_128(out)</span>
<span id="cb11-97"><a href="#cb11-97"></a>        out_128 <span class="op">+=</span> out_64 <span class="op">*</span> (<span class="dv">1</span> <span class="op">/</span> np.sqrt(<span class="dv">2</span>))</span>
<span id="cb11-98"><a href="#cb11-98"></a>        out_128 <span class="op">=</span> <span class="va">self</span>.upsample(out_128)</span>
<span id="cb11-99"><a href="#cb11-99"></a></span>
<span id="cb11-100"><a href="#cb11-100"></a>        out <span class="op">=</span> <span class="va">self</span>.block_256x256(styles[<span class="dv">6</span>], out)</span>
<span id="cb11-101"><a href="#cb11-101"></a>        out_256 <span class="op">=</span> <span class="va">self</span>.to_rgb_128(out)</span>
<span id="cb11-102"><a href="#cb11-102"></a>        out_256 <span class="op">+=</span> out_128 <span class="op">*</span> (<span class="dv">1</span> <span class="op">/</span> np.sqrt(<span class="dv">2</span>))</span>
<span id="cb11-103"><a href="#cb11-103"></a></span>
<span id="cb11-104"><a href="#cb11-104"></a>        <span class="co"># Finally, for one of the regularisation's we require the latents used in this pass of the network so we can return those too.</span></span>
<span id="cb11-105"><a href="#cb11-105"></a>        <span class="cf">if</span> return_latents:</span>
<span id="cb11-106"><a href="#cb11-106"></a>            <span class="cf">return</span> out_256, styles[<span class="dv">6</span>]</span>
<span id="cb11-107"><a href="#cb11-107"></a>        <span class="cf">else</span>:</span>
<span id="cb11-108"><a href="#cb11-108"></a>            <span class="cf">return</span> out_256</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="discriminator" class="level3">
<h3 class="anchored" data-anchor-id="discriminator">Discriminator</h3>
<p>Note, that the residual connections are implemented in the d_style_block rather than in the network. Essentially we downsample the output of the previous layer and store it, we then pass the output of the previous layer through the network as usual and then we sum the two tensors.</p>
<div id="baf0defd-7ce3-4568-9485-4a784deff9ff" class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># The D model is basically the same as in StyleGAN. Except observe how we have removed progressive growing, the out variable is passed</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="co"># through the entire network. We do not have any intermediary outputs, we go straight from 256x256 passing through all the layers</span></span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="co">#&nbsp;right from the beginning of training. There are no if statements which control which layers to activate (or the num_layer variable</span></span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="co">#&nbsp;we came up with last time)</span></span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="kw">class</span> Discriminator(nn.Module):</span>
<span id="cb12-6"><a href="#cb12-6"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, out_c<span class="op">=</span><span class="dv">512</span>):</span>
<span id="cb12-7"><a href="#cb12-7"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-8"><a href="#cb12-8"></a></span>
<span id="cb12-9"><a href="#cb12-9"></a>        <span class="co"># Similar story to G for this fmaps variable. It's great to have official code repos available which show things like this,</span></span>
<span id="cb12-10"><a href="#cb12-10"></a>        <span class="co">#&nbsp;but if there arent any and you are implementing something rememeber to never give up. There are many tricks like this one</span></span>
<span id="cb12-11"><a href="#cb12-11"></a>        <span class="co"># in play and it is our job to discover them!</span></span>
<span id="cb12-12"><a href="#cb12-12"></a>        fmaps <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb12-13"><a href="#cb12-13"></a>        out_c <span class="op">=</span> <span class="bu">int</span>(out_c <span class="op">*</span> fmaps)</span>
<span id="cb12-14"><a href="#cb12-14"></a></span>
<span id="cb12-15"><a href="#cb12-15"></a>        <span class="va">self</span>.from_rgb <span class="op">=</span> EqualLRConv2d(<span class="dv">3</span>, out_c<span class="op">//</span><span class="dv">4</span>, <span class="dv">1</span>)</span>
<span id="cb12-16"><a href="#cb12-16"></a></span>
<span id="cb12-17"><a href="#cb12-17"></a>        <span class="va">self</span>.block_256x256 <span class="op">=</span> d_style_block(out_c<span class="op">//</span><span class="dv">4</span>, out_c<span class="op">//</span><span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb12-18"><a href="#cb12-18"></a>        <span class="va">self</span>.block_128x128 <span class="op">=</span> d_style_block(out_c<span class="op">//</span><span class="dv">4</span>, out_c<span class="op">//</span><span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb12-19"><a href="#cb12-19"></a>        <span class="va">self</span>.block_64x64 <span class="op">=</span> d_style_block(out_c<span class="op">//</span><span class="dv">2</span>, out_c, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb12-20"><a href="#cb12-20"></a>        <span class="va">self</span>.block_32x32 <span class="op">=</span>  d_style_block(out_c, out_c, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb12-21"><a href="#cb12-21"></a>        <span class="va">self</span>.block_16x16 <span class="op">=</span> d_style_block(out_c, out_c, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb12-22"><a href="#cb12-22"></a>        <span class="va">self</span>.block_8x8 <span class="op">=</span> d_style_block(out_c, out_c, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb12-23"><a href="#cb12-23"></a>        <span class="va">self</span>.block_4x4 <span class="op">=</span> d_style_block(out_c, out_c, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">0</span>, mbatch<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-24"><a href="#cb12-24"></a></span>
<span id="cb12-25"><a href="#cb12-25"></a>        <span class="va">self</span>.linear <span class="op">=</span> EqualLRLinear(out_c, <span class="dv">1</span>)</span>
<span id="cb12-26"><a href="#cb12-26"></a></span>
<span id="cb12-27"><a href="#cb12-27"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-28"><a href="#cb12-28"></a>        </span>
<span id="cb12-29"><a href="#cb12-29"></a>        out <span class="op">=</span> <span class="va">self</span>.from_rgb(x)</span>
<span id="cb12-30"><a href="#cb12-30"></a></span>
<span id="cb12-31"><a href="#cb12-31"></a>        out <span class="op">=</span> <span class="va">self</span>.block_256x256(out)</span>
<span id="cb12-32"><a href="#cb12-32"></a>        out <span class="op">=</span> <span class="va">self</span>.block_128x128(out)</span>
<span id="cb12-33"><a href="#cb12-33"></a>        out <span class="op">=</span> <span class="va">self</span>.block_64x64(out)</span>
<span id="cb12-34"><a href="#cb12-34"></a>        out <span class="op">=</span> <span class="va">self</span>.block_32x32(out)</span>
<span id="cb12-35"><a href="#cb12-35"></a>        out <span class="op">=</span> <span class="va">self</span>.block_16x16(out)</span>
<span id="cb12-36"><a href="#cb12-36"></a>        out <span class="op">=</span> <span class="va">self</span>.block_8x8(out)</span>
<span id="cb12-37"><a href="#cb12-37"></a>        out <span class="op">=</span> <span class="va">self</span>.block_4x4(out)</span>
<span id="cb12-38"><a href="#cb12-38"></a></span>
<span id="cb12-39"><a href="#cb12-39"></a>        out <span class="op">=</span> out.view(out.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-40"><a href="#cb12-40"></a>        out <span class="op">=</span> <span class="va">self</span>.linear(out)</span>
<span id="cb12-41"><a href="#cb12-41"></a></span>
<span id="cb12-42"><a href="#cb12-42"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="regularisation-in-the-stylegan2" class="level2">
<h2 class="anchored" data-anchor-id="regularisation-in-the-stylegan2">Regularisation in the StyleGAN2</h2>
<p>StyleGAN2 employs 2 types of regularisation, namely Perceptual Path Length (PPL) and R1 regularisation. Let’s start with R1.</p>
<section id="r1-regularisation" class="level4">
<h4 class="anchored" data-anchor-id="r1-regularisation">R1 Regularisation</h4>
<p>The authors call this lazy regularisation, and it is applied to the loss function for D. A key idea from StyleGAN2 is that this operation can be applied less frequently than at every single iteration, we apply it after every 16 iterations. The ability to run it less frequently is good as it’s a heavy computation and adds time to our training. It is heavy because it requires us to compute the gradients of the D network. You will notice the slowdown in training , if you run the .py training file the pbar stops every 16 iterations to run this operation. The regularisation term originates from the <a href="https://arxiv.org/abs/1801.04406">https://arxiv.org/abs/1801.04406</a> paper (let’s call its the R1 paper for simplicity). It is seen as</p>
<p><span class="math display">\[ R_1(\psi) := \frac{\gamma}{2} \mathbb{E}_{p_D(x)} \left[\|\nabla D_\psi(x)\|^2\right] \tag{3}\]</span></p>
<p>The topic of regularisation in GANs is very interesting and the R1 paper breaks it down, it’s worthwhile to have a discussion on the paper before we continue. The premise is that (as we know) GANs are extremely hard to train and often times they do not converge (in theory the D and G should converge to their Nash Equilibria). One of the reasons for this is that GANs may overfit<a href="#4"><sup>4</sup></a>. I think this paper is the reason Keras et al moved away from WGAN-GP (the loss function), it is stated that the WGAN and WGAN-GP scheme do not lead to convergence whereas the R1 regularized loss does. Another point is that of GAN instability, let’s think about the see-saw method of the GAN game. When the G is far away from the true data distribution the D pushes the G towards the true distribution. Simultaneously, the D model becomes better at classifying between G samples and true data samples. As this occurs, the slope of the D increases and when G reaches it’s optimal point the high gradient of D pushes it away. Once again then the G moves away from the true data distribution and the D model adjusts. This back-and-forth continues on. This overview covers the key points, but I encourage you to read the full paper for more details. The method to counteract is this the R1 regularisation! It counteracts the behaviour by applying a gradient penalty which stops the D model from deviating from the Nash Equilibirium. In the paper there are two functions for the R1 regularisation term. We use the first of the two, it makes use of real data and real predictions. The code implementation will follow after we discuss PPL regularisation :)</p>
</section>
<section id="perceptual-path-length-ppl-regularisation" class="level4">
<h4 class="anchored" data-anchor-id="perceptual-path-length-ppl-regularisation">Perceptual Path Length (PPL) Regularisation</h4>
<p>This regularisation is linked to the PPL metric, which measures the quality of the latent space captured by <span class="math inline">\(w\)</span>, and forces the G to favor “smooth” mappings. The authors observed a link between decreased the PPL metric and image subjective quality so they decided to create the PPL regularistion in order to push the PPL scores down (thus increasing image quality). The hypothesis is as follows: during training the D penalizes broken images (the worst generated images), the most direct way for G to improve is to stretch the region of the latent space which gives good images.</p>
</section>
<section id="implementing-the-two-regularisers" class="level4">
<h4 class="anchored" data-anchor-id="implementing-the-two-regularisers">Implementing the Two Regularisers</h4>
<p>Now, let’s write some code! Note, that R1 is run every 16 iterations and PPL every 4 (though the paper states it should be run every 8, their code repo they run it every 4<a href="#5"><sup>5</sup></a> xD)</p>
<p>In the training code I have bits and pieces of the regulariser code spread around, I will combine them all here for illustrative purposes. This will not be the exact order in which you wil find the files in the python code.</p>
<div id="82a4437f-f2a9-4d72-8581-d903c26a7b41" class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co">#&nbsp;This function implements equation 3 above. It does not implement the whole thing however, observe we have no gamma/2</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="co">#&nbsp;that step is performed later</span></span>
<span id="cb13-3"><a href="#cb13-3"></a></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="kw">def</span> r1_loss(real_pred, real_images):</span>
<span id="cb13-5"><a href="#cb13-5"></a>    <span class="cf">with</span> torch.set_grad_enabled(<span class="va">True</span>):        </span>
<span id="cb13-6"><a href="#cb13-6"></a>        <span class="co"># Get gradients with respect to inputs only</span></span>
<span id="cb13-7"><a href="#cb13-7"></a>        <span class="co"># This part implements the part in the square brackets delta (The big triangle) refers to the gradients</span></span>
<span id="cb13-8"><a href="#cb13-8"></a>        grad_real, <span class="op">=</span> torch.autograd.grad(</span>
<span id="cb13-9"><a href="#cb13-9"></a>            outputs<span class="op">=</span>real_pred.<span class="bu">sum</span>(),</span>
<span id="cb13-10"><a href="#cb13-10"></a>            inputs<span class="op">=</span>real_images,</span>
<span id="cb13-11"><a href="#cb13-11"></a>            grad_outputs<span class="op">=</span>torch.ones([], device<span class="op">=</span>real_pred.device),</span>
<span id="cb13-12"><a href="#cb13-12"></a>            create_graph<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-13"><a href="#cb13-13"></a>            only_inputs<span class="op">=</span><span class="va">True</span>  <span class="co"># Only compute gradients for inputs, not weights</span></span>
<span id="cb13-14"><a href="#cb13-14"></a>        )</span>
<span id="cb13-15"><a href="#cb13-15"></a>    <span class="co">#&nbsp;We sum over the axis 1 because we are operating on minibatches, so we need to sum their effect to get a scalar</span></span>
<span id="cb13-16"><a href="#cb13-16"></a>    grad_penalty <span class="op">=</span> grad_real.<span class="bu">pow</span>(<span class="dv">2</span>).reshape(grad_real.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>).<span class="bu">sum</span>(<span class="dv">1</span>).mean()</span>
<span id="cb13-17"><a href="#cb13-17"></a></span>
<span id="cb13-18"><a href="#cb13-18"></a>    <span class="cf">return</span> grad_penalty</span>
<span id="cb13-19"><a href="#cb13-19"></a></span>
<span id="cb13-20"><a href="#cb13-20"></a><span class="co"># This code is used later in the following way</span></span>
<span id="cb13-21"><a href="#cb13-21"></a></span>
<span id="cb13-22"><a href="#cb13-22"></a>d_reg_freq <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb13-23"><a href="#cb13-23"></a></span>
<span id="cb13-24"><a href="#cb13-24"></a><span class="co"># We run it every 16 minibatches</span></span>
<span id="cb13-25"><a href="#cb13-25"></a>d_r1_regularise <span class="op">=</span> i <span class="op">%</span> d_reg_freq <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb13-26"><a href="#cb13-26"></a><span class="cf">if</span> d_r1_regularise:</span>
<span id="cb13-27"><a href="#cb13-27"></a>    real_imgs.requires_grad_(<span class="va">True</span>)</span>
<span id="cb13-28"><a href="#cb13-28"></a></span>
<span id="cb13-29"><a href="#cb13-29"></a>    real_preds <span class="op">=</span> d(real_imgs)</span>
<span id="cb13-30"><a href="#cb13-30"></a>    r1_loss_val <span class="op">=</span> r1_loss(real_preds, real_imgs)</span>
<span id="cb13-31"><a href="#cb13-31"></a></span>
<span id="cb13-32"><a href="#cb13-32"></a>    d.zero_grad()</span>
<span id="cb13-33"><a href="#cb13-33"></a></span>
<span id="cb13-34"><a href="#cb13-34"></a>    <span class="co">#&nbsp;</span><span class="al">NOTE</span><span class="co">: gamma is a key parameter in training of this model. It's quite difficult to tune and the authors have created a </span></span>
<span id="cb13-35"><a href="#cb13-35"></a>    <span class="co"># heuristic formula to calculate if for your respective settings: gamma = 0.0002 * (resolution ** 2) / batch_size</span></span>
<span id="cb13-36"><a href="#cb13-36"></a>    gamma <span class="op">=</span> <span class="fl">0.8192</span> <span class="co"># from https://github.com/NVlabs/stylegan3/blob/c233a919a6faee6e36a316ddd4eddababad1adf9/docs/configs.md</span></span>
<span id="cb13-37"><a href="#cb13-37"></a>    <span class="co"># Calculate the other part of equation 3</span></span>
<span id="cb13-38"><a href="#cb13-38"></a>    <span class="co">#&nbsp;The purpose of 0 * real_preds[0] is to ensure this operation is included in the PyTorch computational graph. If we do not include </span></span>
<span id="cb13-39"><a href="#cb13-39"></a>    <span class="co"># it you may see some errors. It doesnt actually change the value as we just add 0 (a no-op)</span></span>
<span id="cb13-40"><a href="#cb13-40"></a>    r1_reg <span class="op">=</span> (gamma<span class="op">/</span><span class="dv">2</span> <span class="op">*</span> r1_loss_val <span class="op">*</span> d_reg_freq <span class="op">+</span> <span class="dv">0</span> <span class="op">*</span> real_preds[<span class="dv">0</span>])</span>
<span id="cb13-41"><a href="#cb13-41"></a>    r1_reg.backward()</span>
<span id="cb13-42"><a href="#cb13-42"></a></span>
<span id="cb13-43"><a href="#cb13-43"></a>    d_optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1bf306ac-5c74-48f5-806e-4ddd177182c9" class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="co">#&nbsp;I admit I was not fully able to implement the PPL regulariser, I made use of the one provided by Rosinality</span></span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="co"># Credit: https://github.com/rosinality/stylegan2-pytorch/blob/master/train.py#L87</span></span>
<span id="cb14-3"><a href="#cb14-3"></a><span class="kw">def</span> g_path_regularize(fake_img, latents, mean_path_length, decay<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb14-4"><a href="#cb14-4"></a>    noise <span class="op">=</span> torch.randn_like(fake_img) <span class="op">/</span> math.sqrt(</span>
<span id="cb14-5"><a href="#cb14-5"></a>        fake_img.shape[<span class="dv">2</span>] <span class="op">*</span> fake_img.shape[<span class="dv">3</span>]</span>
<span id="cb14-6"><a href="#cb14-6"></a>    )</span>
<span id="cb14-7"><a href="#cb14-7"></a>    </span>
<span id="cb14-8"><a href="#cb14-8"></a>    grad, <span class="op">=</span> torch.autograd.grad(</span>
<span id="cb14-9"><a href="#cb14-9"></a>        outputs<span class="op">=</span>(fake_img <span class="op">*</span> noise).<span class="bu">sum</span>(), inputs<span class="op">=</span>latents, create_graph<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-10"><a href="#cb14-10"></a>        retain_graph<span class="op">=</span><span class="va">True</span>, only_inputs<span class="op">=</span><span class="va">True</span></span>
<span id="cb14-11"><a href="#cb14-11"></a>    )</span>
<span id="cb14-12"><a href="#cb14-12"></a>    </span>
<span id="cb14-13"><a href="#cb14-13"></a>    path_lengths <span class="op">=</span> torch.sqrt(grad.<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>(<span class="dv">1</span>).mean(<span class="dv">0</span>))</span>
<span id="cb14-14"><a href="#cb14-14"></a>    path_mean <span class="op">=</span> mean_path_length <span class="op">+</span> decay <span class="op">*</span> (path_lengths.mean() <span class="op">-</span> mean_path_length)</span>
<span id="cb14-15"><a href="#cb14-15"></a>    path_penalty <span class="op">=</span> (path_lengths <span class="op">-</span> path_mean).<span class="bu">pow</span>(<span class="dv">2</span>).mean()</span>
<span id="cb14-16"><a href="#cb14-16"></a></span>
<span id="cb14-17"><a href="#cb14-17"></a>    <span class="kw">del</span> noise, grad. <span class="co"># To preserve memory</span></span>
<span id="cb14-18"><a href="#cb14-18"></a>    torch.cuda.empty_cache()</span>
<span id="cb14-19"><a href="#cb14-19"></a>    </span>
<span id="cb14-20"><a href="#cb14-20"></a>    <span class="cf">return</span> path_penalty, path_mean.detach(), path_lengths</span>
<span id="cb14-21"><a href="#cb14-21"></a></span>
<span id="cb14-22"><a href="#cb14-22"></a>g_reg_freq <span class="op">=</span> <span class="dv">4</span>  <span class="co"># PPL is applied to the G model</span></span>
<span id="cb14-23"><a href="#cb14-23"></a></span>
<span id="cb14-24"><a href="#cb14-24"></a><span class="co">#&nbsp;Similar logic to R1 when running PPL</span></span>
<span id="cb14-25"><a href="#cb14-25"></a>g_ppl_regularise <span class="op">=</span> i <span class="op">%</span> g_reg_freq <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb14-26"><a href="#cb14-26"></a><span class="cf">if</span> g_ppl_regularise:</span>
<span id="cb14-27"><a href="#cb14-27"></a>    z <span class="op">=</span> torch.randn(real_size, latent_dim, device<span class="op">=</span>device)</span>
<span id="cb14-28"><a href="#cb14-28"></a>    gen_imgs, latents <span class="op">=</span> g(z, return_latents<span class="op">=</span><span class="va">True</span>)  <span class="co"># PPL calculation relies on latent vectors which produced imgs</span></span>
<span id="cb14-29"><a href="#cb14-29"></a>    </span>
<span id="cb14-30"><a href="#cb14-30"></a>    ppl_loss, mean_path_length, path_lengths <span class="op">=</span> g_path_regularize(gen_imgs, latents, mean_path_length)</span>
<span id="cb14-31"><a href="#cb14-31"></a></span>
<span id="cb14-32"><a href="#cb14-32"></a>    g.zero_grad()</span>
<span id="cb14-33"><a href="#cb14-33"></a></span>
<span id="cb14-34"><a href="#cb14-34"></a>    <span class="co"># 2 is the weighting I use the same value that Rosinality used</span></span>
<span id="cb14-35"><a href="#cb14-35"></a>    ppl_loss <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> g_reg_freq <span class="op">*</span> ppl_loss</span>
<span id="cb14-36"><a href="#cb14-36"></a>    </span>
<span id="cb14-37"><a href="#cb14-37"></a>    ppl_loss.backward()</span>
<span id="cb14-38"><a href="#cb14-38"></a>    g_optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>So there we have it! We have all the bits and pieces ready to go, now it’s time to put them together.</p>
</section>
</section>
<section id="implementing-the-training-loop" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-training-loop">Implementing the Training Loop</h2>
<div id="096c1a85-e5a5-4ea4-91da-55597746f027" class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># The dataloader changes slightly in StyleGAN2</span></span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="kw">def</span> get_dataloader(image_size, batch_size<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb15-3"><a href="#cb15-3"></a>    <span class="co"># We only call this function once now, unlike in the StyleGAN where it would be required that we change resolution of the dataset when </span></span>
<span id="cb15-4"><a href="#cb15-4"></a>    <span class="co"># the network grew</span></span>
<span id="cb15-5"><a href="#cb15-5"></a>    transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb15-6"><a href="#cb15-6"></a>        <span class="co">#transforms.Resize((image_size, image_size)),  # No longer need to resize the images</span></span>
<span id="cb15-7"><a href="#cb15-7"></a>        transforms.RandomHorizontalFlip(p<span class="op">=</span><span class="fl">0.5</span>),  <span class="co"># Add a random flip</span></span>
<span id="cb15-8"><a href="#cb15-8"></a>        transforms.ToTensor(),</span>
<span id="cb15-9"><a href="#cb15-9"></a>        transforms.Normalize((<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), (<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>))</span>
<span id="cb15-10"><a href="#cb15-10"></a>    ])</span>
<span id="cb15-11"><a href="#cb15-11"></a></span>
<span id="cb15-12"><a href="#cb15-12"></a>    <span class="co">#&nbsp;Reminder we use FFHQ, another face based dataset. There are more images and the diversity is increased.</span></span>
<span id="cb15-13"><a href="#cb15-13"></a>    <span class="co">#&nbsp;An interesting note, the authors put a hell of a lot of work into building this dataset and the CelebA-HQ dataset.</span></span>
<span id="cb15-14"><a href="#cb15-14"></a>    <span class="co"># This one of the reasons the StyleGAN series performs so well, the images are front lit human faces with good</span></span>
<span id="cb15-15"><a href="#cb15-15"></a>    <span class="co"># colours and smooth transitions, they lend themselves very strongly to GAN modelling.</span></span>
<span id="cb15-16"><a href="#cb15-16"></a>    dataset <span class="op">=</span> ImageFolder(root<span class="op">=</span><span class="st">'./ffhq256_imgs'</span>, transform<span class="op">=</span>transform)</span>
<span id="cb15-17"><a href="#cb15-17"></a></span>
<span id="cb15-18"><a href="#cb15-18"></a>    dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb15-19"><a href="#cb15-19"></a>        dataset,</span>
<span id="cb15-20"><a href="#cb15-20"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb15-21"><a href="#cb15-21"></a>        shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb15-22"><a href="#cb15-22"></a>        num_workers<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb15-23"><a href="#cb15-23"></a>        drop_last<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb15-24"><a href="#cb15-24"></a>        pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb15-25"><a href="#cb15-25"></a>        persistent_workers<span class="op">=</span><span class="va">True</span></span>
<span id="cb15-26"><a href="#cb15-26"></a>    )</span>
<span id="cb15-27"><a href="#cb15-27"></a></span>
<span id="cb15-28"><a href="#cb15-28"></a>    <span class="cf">return</span> dataloader</span>
<span id="cb15-29"><a href="#cb15-29"></a></span>
<span id="cb15-30"><a href="#cb15-30"></a><span class="co"># Create new checkpoint dir with timestamp</span></span>
<span id="cb15-31"><a href="#cb15-31"></a>timestamp <span class="op">=</span> datetime.now().strftime(<span class="st">"%Y%m</span><span class="sc">%d</span><span class="st">_%H%M%S"</span>)</span>
<span id="cb15-32"><a href="#cb15-32"></a>run_dir <span class="op">=</span> os.path.join(<span class="st">'./checkpoints'</span>, <span class="ss">f'run_</span><span class="sc">{</span>timestamp<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb15-33"><a href="#cb15-33"></a>checkpoint_dir <span class="op">=</span> os.path.join(run_dir, <span class="ss">f"checkpoint_</span><span class="sc">{</span>timestamp<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-34"><a href="#cb15-34"></a>sample_dir <span class="op">=</span> os.path.join(run_dir, <span class="ss">f"sample_</span><span class="sc">{</span>timestamp<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-35"><a href="#cb15-35"></a><span class="co">#&nbsp;Create a file to log FID scores</span></span>
<span id="cb15-36"><a href="#cb15-36"></a>fid_file <span class="op">=</span> os.path.join(run_dir, <span class="st">'fid.txt'</span>)</span>
<span id="cb15-37"><a href="#cb15-37"></a></span>
<span id="cb15-38"><a href="#cb15-38"></a>os.makedirs(checkpoint_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-39"><a href="#cb15-39"></a>os.makedirs(sample_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-40"><a href="#cb15-40"></a></span>
<span id="cb15-41"><a href="#cb15-41"></a><span class="co">#&nbsp;Init models</span></span>
<span id="cb15-42"><a href="#cb15-42"></a>g <span class="op">=</span> Generator().to(device)</span>
<span id="cb15-43"><a href="#cb15-43"></a>d <span class="op">=</span> Discriminator().to(device)</span>
<span id="cb15-44"><a href="#cb15-44"></a>g_running <span class="op">=</span> Generator().to(device)</span>
<span id="cb15-45"><a href="#cb15-45"></a></span>
<span id="cb15-46"><a href="#cb15-46"></a>g_running.train(<span class="va">False</span>)</span>
<span id="cb15-47"><a href="#cb15-47"></a></span>
<span id="cb15-48"><a href="#cb15-48"></a>fid <span class="op">=</span> FrechetInceptionDistance(feature<span class="op">=</span><span class="dv">2048</span>).to(device)</span>
<span id="cb15-49"><a href="#cb15-49"></a></span>
<span id="cb15-50"><a href="#cb15-50"></a>mapping_params, other_params <span class="op">=</span> get_params_with_lr(g)</span>
<span id="cb15-51"><a href="#cb15-51"></a></span>
<span id="cb15-52"><a href="#cb15-52"></a><span class="co"># I include here the range of learning rates I tested</span></span>
<span id="cb15-53"><a href="#cb15-53"></a>lr <span class="op">=</span> <span class="fl">0.0025</span>  </span>
<span id="cb15-54"><a href="#cb15-54"></a><span class="co"># Failed LRs - An idea is that with harder datasets a lower LR is needed. FFQH is quite an easy dataset, colours are smooth and images</span></span>
<span id="cb15-55"><a href="#cb15-55"></a><span class="co"># are well formed and often have a good front lit photo.</span></span>
<span id="cb15-56"><a href="#cb15-56"></a><span class="co">#lr = 0.00016  #https://github.com/NVlabs/stylegan2-ada-pytorch/blob/main/train.py#L157</span></span>
<span id="cb15-57"><a href="#cb15-57"></a><span class="co">#lr = 0.00008</span></span>
<span id="cb15-58"><a href="#cb15-58"></a><span class="co">#lr = 0.00005  # 0.00008 is better</span></span>
<span id="cb15-59"><a href="#cb15-59"></a><span class="co">#&nbsp;Let's reduce LR after checkpoint 215k which has FID of 113</span></span>
<span id="cb15-60"><a href="#cb15-60"></a><span class="co">#lr = 0.00001 - DIDNT DO MUCH LR DECAY NOT useful for me</span></span>
<span id="cb15-61"><a href="#cb15-61"></a></span>
<span id="cb15-62"><a href="#cb15-62"></a>mapping_lr <span class="op">=</span> lr <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb15-63"><a href="#cb15-63"></a></span>
<span id="cb15-64"><a href="#cb15-64"></a>g_reg_freq <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb15-65"><a href="#cb15-65"></a>d_reg_freq <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb15-66"><a href="#cb15-66"></a></span>
<span id="cb15-67"><a href="#cb15-67"></a>g_reg_adjustment <span class="op">=</span> g_reg_freq <span class="op">/</span> (g_reg_freq <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb15-68"><a href="#cb15-68"></a>d_reg_adjustment <span class="op">=</span> d_reg_freq <span class="op">/</span> (d_reg_freq <span class="op">+</span> <span class="dv">1</span>) </span>
<span id="cb15-69"><a href="#cb15-69"></a></span>
<span id="cb15-70"><a href="#cb15-70"></a>g_optimizer <span class="op">=</span> torch.optim.Adam([</span>
<span id="cb15-71"><a href="#cb15-71"></a>    {<span class="st">'params'</span>: mapping_params, <span class="st">'lr'</span>: mapping_lr},  <span class="co"># 0.01 * LR for mapping network</span></span>
<span id="cb15-72"><a href="#cb15-72"></a>    {<span class="st">'params'</span>: other_params, <span class="st">'lr'</span>: lr <span class="op">*</span> g_reg_adjustment}  <span class="co"># Regular LR for other parts</span></span>
<span id="cb15-73"><a href="#cb15-73"></a>], betas<span class="op">=</span>(<span class="fl">0.0</span> <span class="op">**</span> g_reg_adjustment, <span class="fl">0.99</span> <span class="op">**</span> g_reg_adjustment))</span>
<span id="cb15-74"><a href="#cb15-74"></a>d_optimizer <span class="op">=</span> torch.optim.Adam(d.parameters(), lr<span class="op">=</span>lr<span class="op">*</span>d_reg_adjustment, betas<span class="op">=</span>(<span class="fl">0.0</span> <span class="op">**</span> d_reg_adjustment, <span class="fl">0.99</span> <span class="op">**</span> d_reg_adjustment))</span>
<span id="cb15-75"><a href="#cb15-75"></a></span>
<span id="cb15-76"><a href="#cb15-76"></a>start_iter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-77"><a href="#cb15-77"></a></span>
<span id="cb15-78"><a href="#cb15-78"></a>resume_checkpoint <span class="op">=</span> <span class="va">False</span></span>
<span id="cb15-79"><a href="#cb15-79"></a><span class="cf">if</span> resume_checkpoint:</span>
<span id="cb15-80"><a href="#cb15-80"></a>    <span class="cf">if</span> os.path.isfile(resume_checkpoint):</span>
<span id="cb15-81"><a href="#cb15-81"></a>        <span class="bu">print</span>(<span class="ss">f"=&gt; loading checkpoint '</span><span class="sc">{</span>resume_checkpoint<span class="sc">}</span><span class="ss">'"</span>)</span>
<span id="cb15-82"><a href="#cb15-82"></a>        checkpoint <span class="op">=</span> torch.load(resume_checkpoint, weights_only<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-83"><a href="#cb15-83"></a>        start_iter <span class="op">=</span> checkpoint[<span class="st">'iteration'</span>] <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb15-84"><a href="#cb15-84"></a>        g.load_state_dict(checkpoint[<span class="st">'g_state_dict'</span>])</span>
<span id="cb15-85"><a href="#cb15-85"></a>        d.load_state_dict(checkpoint[<span class="st">'d_state_dict'</span>])</span>
<span id="cb15-86"><a href="#cb15-86"></a>        g_running.load_state_dict(checkpoint[<span class="st">'g_running_state_dict'</span>])</span>
<span id="cb15-87"><a href="#cb15-87"></a>        g_optimizer.load_state_dict(checkpoint[<span class="st">'g_optimizer_state_dict'</span>])</span>
<span id="cb15-88"><a href="#cb15-88"></a>        d_optimizer.load_state_dict(checkpoint[<span class="st">'d_optimizer_state_dict'</span>])</span>
<span id="cb15-89"><a href="#cb15-89"></a></span>
<span id="cb15-90"><a href="#cb15-90"></a>        <span class="bu">print</span>(<span class="ss">f"=&gt; loaded checkpoint '</span><span class="sc">{</span>resume_checkpoint<span class="sc">}</span><span class="ss">' ( iteration </span><span class="sc">{</span>start_iter<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb15-91"><a href="#cb15-91"></a>    <span class="cf">else</span>:</span>
<span id="cb15-92"><a href="#cb15-92"></a>        <span class="bu">print</span>(<span class="ss">f"=&gt; no checkpoint found at '</span><span class="sc">{</span>resume_checkpoint<span class="sc">}</span><span class="ss">'"</span>)</span>
<span id="cb15-93"><a href="#cb15-93"></a><span class="cf">else</span>:</span>
<span id="cb15-94"><a href="#cb15-94"></a>    <span class="bu">print</span>(<span class="st">"Starting training from the beginning"</span>)</span>
<span id="cb15-95"><a href="#cb15-95"></a></span>
<span id="cb15-96"><a href="#cb15-96"></a><span class="co"># Init EMA </span></span>
<span id="cb15-97"><a href="#cb15-97"></a>EMA(g_running, g, <span class="dv">0</span>)</span>
<span id="cb15-98"><a href="#cb15-98"></a> </span>
<span id="cb15-99"><a href="#cb15-99"></a><span class="co"># We evaluate FID every 10k iterations - for more frequent updates</span></span>
<span id="cb15-100"><a href="#cb15-100"></a>num_iters_for_eval <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb15-101"><a href="#cb15-101"></a></span>
<span id="cb15-102"><a href="#cb15-102"></a><span class="co">#&nbsp;We want to gen 70k fake images for FID calculation to match 30k real images</span></span>
<span id="cb15-103"><a href="#cb15-103"></a>num_fake_images <span class="op">=</span> <span class="dv">70000</span></span>
<span id="cb15-104"><a href="#cb15-104"></a>latent_dim <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Adjust based on your model's input size</span></span>
<span id="cb15-105"><a href="#cb15-105"></a></span>
<span id="cb15-106"><a href="#cb15-106"></a><span class="co"># Define vars used within training loop</span></span>
<span id="cb15-107"><a href="#cb15-107"></a>d_loss_val <span class="op">=</span> <span class="va">None</span></span>
<span id="cb15-108"><a href="#cb15-108"></a>g_loss_val <span class="op">=</span> <span class="va">None</span></span>
<span id="cb15-109"><a href="#cb15-109"></a>r1_loss_val <span class="op">=</span> <span class="va">None</span></span>
<span id="cb15-110"><a href="#cb15-110"></a></span>
<span id="cb15-111"><a href="#cb15-111"></a>resolution <span class="op">=</span> <span class="dv">256</span>  <span class="co"># Resolution is always 256</span></span>
<span id="cb15-112"><a href="#cb15-112"></a>batch_size <span class="op">=</span> <span class="dv">16</span>  <span class="co"># CHANGE to 32, 64 will run but is awfully slow - TRY 4???</span></span>
<span id="cb15-113"><a href="#cb15-113"></a>total_iters <span class="op">=</span> <span class="dv">25000</span> <span class="op">*</span> <span class="dv">1000</span> <span class="op">//</span> batch_size  <span class="co"># k imgs from paper</span></span>
<span id="cb15-114"><a href="#cb15-114"></a>data_loader <span class="op">=</span> get_dataloader(resolution, batch_size)</span>
<span id="cb15-115"><a href="#cb15-115"></a>dataset <span class="op">=</span> <span class="bu">iter</span>(data_loader)</span>
<span id="cb15-116"><a href="#cb15-116"></a></span>
<span id="cb15-117"><a href="#cb15-117"></a><span class="co"># Init a progress bar</span></span>
<span id="cb15-118"><a href="#cb15-118"></a><span class="bu">print</span>(<span class="ss">f'Training resolution: </span><span class="sc">{</span>resolution<span class="sc">}</span><span class="ss">x</span><span class="sc">{</span>resolution<span class="sc">}</span><span class="ss">, Batch size: </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb15-119"><a href="#cb15-119"></a></span>
<span id="cb15-120"><a href="#cb15-120"></a><span class="cf">if</span> resume_checkpoint:</span>
<span id="cb15-121"><a href="#cb15-121"></a>    remaining_iters <span class="op">=</span> total_iters <span class="op">-</span> start_iter  <span class="co"># Subtract what's been done</span></span>
<span id="cb15-122"><a href="#cb15-122"></a>    <span class="co"># Create progress bar for remaining iterations but show correct absolute position</span></span>
<span id="cb15-123"><a href="#cb15-123"></a>    pbar <span class="op">=</span> tqdm(<span class="bu">range</span>(remaining_iters), initial<span class="op">=</span>start_iter, total<span class="op">=</span>total_iters)</span>
<span id="cb15-124"><a href="#cb15-124"></a><span class="cf">else</span>:</span>
<span id="cb15-125"><a href="#cb15-125"></a>    pbar <span class="op">=</span> tqdm(<span class="bu">range</span>(total_iters))</span>
<span id="cb15-126"><a href="#cb15-126"></a></span>
<span id="cb15-127"><a href="#cb15-127"></a><span class="co">#&nbsp;This is used for our try and except loops</span></span>
<span id="cb15-128"><a href="#cb15-128"></a>max_retries <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb15-129"><a href="#cb15-129"></a></span>
<span id="cb15-130"><a href="#cb15-130"></a><span class="co"># Begin introducing layer phase</span></span>
<span id="cb15-131"><a href="#cb15-131"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb15-132"><a href="#cb15-132"></a>    requires_grad(g, <span class="va">False</span>)</span>
<span id="cb15-133"><a href="#cb15-133"></a>    requires_grad(d, <span class="va">True</span>)</span>
<span id="cb15-134"><a href="#cb15-134"></a></span>
<span id="cb15-135"><a href="#cb15-135"></a>    <span class="cf">try</span>:</span>
<span id="cb15-136"><a href="#cb15-136"></a>        real_imgs, label <span class="op">=</span> <span class="bu">next</span>(dataset)</span>
<span id="cb15-137"><a href="#cb15-137"></a>    <span class="cf">except</span> (<span class="pp">OSError</span>, <span class="pp">StopIteration</span>):</span>
<span id="cb15-138"><a href="#cb15-138"></a>        <span class="co"># If we reach the end of the dataset, we reintialise the iterable</span></span>
<span id="cb15-139"><a href="#cb15-139"></a>        <span class="co"># basically starting again</span></span>
<span id="cb15-140"><a href="#cb15-140"></a>        dataset <span class="op">=</span> <span class="bu">iter</span>(data_loader)</span>
<span id="cb15-141"><a href="#cb15-141"></a>        real_imgs, label <span class="op">=</span> <span class="bu">next</span>(dataset)</span>
<span id="cb15-142"><a href="#cb15-142"></a></span>
<span id="cb15-143"><a href="#cb15-143"></a>    <span class="co">#&nbsp;Train D</span></span>
<span id="cb15-144"><a href="#cb15-144"></a>    real_size <span class="op">=</span> real_imgs.size(<span class="dv">0</span>)</span>
<span id="cb15-145"><a href="#cb15-145"></a>    real_imgs <span class="op">=</span> real_imgs.to(device)</span>
<span id="cb15-146"><a href="#cb15-146"></a>    label <span class="op">=</span> label.to(device)</span>
<span id="cb15-147"><a href="#cb15-147"></a>    real_preds <span class="op">=</span> d(real_imgs)</span>
<span id="cb15-148"><a href="#cb15-148"></a></span>
<span id="cb15-149"><a href="#cb15-149"></a>    <span class="co">#&nbsp;Create gen images and gen preds</span></span>
<span id="cb15-150"><a href="#cb15-150"></a>    z <span class="op">=</span> torch.randn(real_size, latent_dim, device<span class="op">=</span>device)</span>
<span id="cb15-151"><a href="#cb15-151"></a>    gen_imgs <span class="op">=</span> g(z)</span>
<span id="cb15-152"><a href="#cb15-152"></a>    gen_preds <span class="op">=</span> d(gen_imgs.detach())</span>
<span id="cb15-153"><a href="#cb15-153"></a></span>
<span id="cb15-154"><a href="#cb15-154"></a>    d_loss_val <span class="op">=</span> d_loss(real_preds, gen_preds)</span>
<span id="cb15-155"><a href="#cb15-155"></a></span>
<span id="cb15-156"><a href="#cb15-156"></a>    d.zero_grad()</span>
<span id="cb15-157"><a href="#cb15-157"></a>    d_loss_val.backward()</span>
<span id="cb15-158"><a href="#cb15-158"></a>    d_optimizer.step()</span>
<span id="cb15-159"><a href="#cb15-159"></a></span>
<span id="cb15-160"><a href="#cb15-160"></a>    d_r1_regularise <span class="op">=</span> i <span class="op">%</span> d_reg_freq <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb15-161"><a href="#cb15-161"></a>    <span class="cf">if</span> d_r1_regularise:</span>
<span id="cb15-162"><a href="#cb15-162"></a>        real_imgs.requires_grad_(<span class="va">True</span>)</span>
<span id="cb15-163"><a href="#cb15-163"></a></span>
<span id="cb15-164"><a href="#cb15-164"></a>        real_preds <span class="op">=</span> d(real_imgs)</span>
<span id="cb15-165"><a href="#cb15-165"></a>        r1_loss_val <span class="op">=</span> r1_loss(real_preds, real_imgs)</span>
<span id="cb15-166"><a href="#cb15-166"></a></span>
<span id="cb15-167"><a href="#cb15-167"></a>        d.zero_grad()</span>
<span id="cb15-168"><a href="#cb15-168"></a>        gamma <span class="op">=</span> <span class="fl">0.8192</span></span>
<span id="cb15-169"><a href="#cb15-169"></a>        r1_reg <span class="op">=</span> ((gamma<span class="op">*</span><span class="fl">0.5</span>) <span class="op">*</span> r1_loss_val <span class="op">*</span> d_reg_freq <span class="op">+</span> <span class="dv">0</span> <span class="op">*</span> real_preds[<span class="dv">0</span>])</span>
<span id="cb15-170"><a href="#cb15-170"></a>        r1_reg.backward()</span>
<span id="cb15-171"><a href="#cb15-171"></a></span>
<span id="cb15-172"><a href="#cb15-172"></a>        d_optimizer.step()</span>
<span id="cb15-173"><a href="#cb15-173"></a>        torch.cuda.empty_cache()</span>
<span id="cb15-174"><a href="#cb15-174"></a></span>
<span id="cb15-175"><a href="#cb15-175"></a>    <span class="co"># Now lets train the Generator</span></span>
<span id="cb15-176"><a href="#cb15-176"></a>    requires_grad(g, <span class="va">True</span>)</span>
<span id="cb15-177"><a href="#cb15-177"></a>    requires_grad(d, <span class="va">False</span>)</span>
<span id="cb15-178"><a href="#cb15-178"></a></span>
<span id="cb15-179"><a href="#cb15-179"></a>    z <span class="op">=</span> torch.randn(real_size, latent_dim, device<span class="op">=</span>device)</span>
<span id="cb15-180"><a href="#cb15-180"></a>    gen_imgs <span class="op">=</span> g(z)</span>
<span id="cb15-181"><a href="#cb15-181"></a>    gen_preds <span class="op">=</span> d(gen_imgs)</span>
<span id="cb15-182"><a href="#cb15-182"></a></span>
<span id="cb15-183"><a href="#cb15-183"></a>    g_loss_val <span class="op">=</span> g_loss(gen_preds)</span>
<span id="cb15-184"><a href="#cb15-184"></a></span>
<span id="cb15-185"><a href="#cb15-185"></a>    g.zero_grad()</span>
<span id="cb15-186"><a href="#cb15-186"></a>    g_loss_val.backward()</span>
<span id="cb15-187"><a href="#cb15-187"></a>    g_optimizer.step()</span>
<span id="cb15-188"><a href="#cb15-188"></a></span>
<span id="cb15-189"><a href="#cb15-189"></a>    <span class="co">#&nbsp;PPL reg, r1 only used on D</span></span>
<span id="cb15-190"><a href="#cb15-190"></a>    <span class="co"># PPL a metric for gen images only</span></span>
<span id="cb15-191"><a href="#cb15-191"></a>    g_ppl_regularise <span class="op">=</span> i <span class="op">%</span> g_reg_freq <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb15-192"><a href="#cb15-192"></a>    <span class="cf">if</span> g_ppl_regularise:</span>
<span id="cb15-193"><a href="#cb15-193"></a>        z <span class="op">=</span> torch.randn(real_size, latent_dim, device<span class="op">=</span>device)</span>
<span id="cb15-194"><a href="#cb15-194"></a>        gen_imgs, latents <span class="op">=</span> g(z, return_latents<span class="op">=</span><span class="va">True</span>)  <span class="co"># PPL calculation relies on latent vectors which produced imgs</span></span>
<span id="cb15-195"><a href="#cb15-195"></a>        </span>
<span id="cb15-196"><a href="#cb15-196"></a>        ppl_loss, mean_path_length, path_lengths <span class="op">=</span> g_path_regularize(gen_imgs, latents, mean_path_length)</span>
<span id="cb15-197"><a href="#cb15-197"></a></span>
<span id="cb15-198"><a href="#cb15-198"></a>        g.zero_grad()</span>
<span id="cb15-199"><a href="#cb15-199"></a>        </span>
<span id="cb15-200"><a href="#cb15-200"></a>        ppl_loss <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> g_reg_freq <span class="op">*</span> ppl_loss</span>
<span id="cb15-201"><a href="#cb15-201"></a>        </span>
<span id="cb15-202"><a href="#cb15-202"></a>        ppl_loss.backward()</span>
<span id="cb15-203"><a href="#cb15-203"></a>        g_optimizer.step()</span>
<span id="cb15-204"><a href="#cb15-204"></a>        torch.cuda.empty_cache()</span>
<span id="cb15-205"><a href="#cb15-205"></a>        </span>
<span id="cb15-206"><a href="#cb15-206"></a>    EMA(g_running, g, decay<span class="op">=</span><span class="fl">0.999</span>)</span>
<span id="cb15-207"><a href="#cb15-207"></a>    </span>
<span id="cb15-208"><a href="#cb15-208"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> i <span class="op">%</span> num_iters_for_eval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb15-209"><a href="#cb15-209"></a>        sample_z <span class="op">=</span> torch.randn(<span class="dv">16</span>, latent_dim, device<span class="op">=</span>device)</span>
<span id="cb15-210"><a href="#cb15-210"></a>        sample_imgs_EMA <span class="op">=</span> g_running(sample_z)</span>
<span id="cb15-211"><a href="#cb15-211"></a>        save_image(sample_imgs_EMA, <span class="ss">f'</span><span class="sc">{</span>sample_dir<span class="sc">}</span><span class="ss">/sample__iter_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">.png'</span>, nrow<span class="op">=</span><span class="dv">4</span>, normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-212"><a href="#cb15-212"></a>        <span class="bu">print</span>(<span class="ss">f'G_running images images after iter: </span><span class="sc">{</span>i<span class="op">+</span>start_iter<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb15-213"><a href="#cb15-213"></a></span>
<span id="cb15-214"><a href="#cb15-214"></a>        <span class="co"># Added a try and except loop for the FID calculation, note this try and except cannot catch Fatal Python errors</span></span>
<span id="cb15-215"><a href="#cb15-215"></a>        <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(max_retries):</span>
<span id="cb15-216"><a href="#cb15-216"></a>           <span class="cf">try</span>:</span>
<span id="cb15-217"><a href="#cb15-217"></a>               calculate_and_save_fid(i, data_loader, g_running, num_fake_images, batch_size, latent_dim, device, fid_file)</span>
<span id="cb15-218"><a href="#cb15-218"></a>               <span class="cf">break</span>  <span class="co"># If successful, exit retry loop</span></span>
<span id="cb15-219"><a href="#cb15-219"></a>           <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb15-220"><a href="#cb15-220"></a>               <span class="bu">print</span>(<span class="ss">f"FID calculation failed (attempt </span><span class="sc">{</span>attempt <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>max_retries<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb15-221"><a href="#cb15-221"></a>               <span class="bu">print</span>(<span class="ss">f"Error: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-222"><a href="#cb15-222"></a>               <span class="cf">if</span> attempt <span class="op">==</span> max_retries <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb15-223"><a href="#cb15-223"></a>                   <span class="bu">print</span>(<span class="st">"Maximum retries reached. Skipping FID calculation for this iteration."</span>)</span>
<span id="cb15-224"><a href="#cb15-224"></a>               <span class="cf">else</span>:</span>
<span id="cb15-225"><a href="#cb15-225"></a>                   <span class="bu">print</span>(<span class="st">"Retrying..."</span>)</span>
<span id="cb15-226"><a href="#cb15-226"></a>                   torch.cuda.empty_cache()</span>
<span id="cb15-227"><a href="#cb15-227"></a>                   time.sleep(<span class="dv">5</span>)</span>
<span id="cb15-228"><a href="#cb15-228"></a></span>
<span id="cb15-229"><a href="#cb15-229"></a>        torch.save({</span>
<span id="cb15-230"><a href="#cb15-230"></a>            <span class="st">'g_state_dict'</span>: g.state_dict(),</span>
<span id="cb15-231"><a href="#cb15-231"></a>            <span class="st">'g_running_state_dict'</span>: g_running.state_dict(),</span>
<span id="cb15-232"><a href="#cb15-232"></a>            <span class="st">'d_state_dict'</span>: d.state_dict(),</span>
<span id="cb15-233"><a href="#cb15-233"></a>            <span class="st">'g_optimizer_state_dict'</span>: g_optimizer.state_dict(),</span>
<span id="cb15-234"><a href="#cb15-234"></a>            <span class="st">'d_optimizer_state_dict'</span>: d_optimizer.state_dict(),</span>
<span id="cb15-235"><a href="#cb15-235"></a>            <span class="st">'iteration'</span>: i,</span>
<span id="cb15-236"><a href="#cb15-236"></a>            <span class="st">'mean_path_length'</span>: mean_path_length,</span>
<span id="cb15-237"><a href="#cb15-237"></a>        }, <span class="ss">f'</span><span class="sc">{</span>checkpoint_dir<span class="sc">}</span><span class="ss">/checkpoint_iter_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">.pth'</span>)</span>
<span id="cb15-238"><a href="#cb15-238"></a></span>
<span id="cb15-239"><a href="#cb15-239"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb15-240"><a href="#cb15-240"></a>    sample_z <span class="op">=</span> torch.randn(<span class="dv">16</span>, latent_dim, device<span class="op">=</span>device)</span>
<span id="cb15-241"><a href="#cb15-241"></a>    sample_imgs <span class="op">=</span> g(sample_z)</span>
<span id="cb15-242"><a href="#cb15-242"></a>    sample_imgs_EMA <span class="op">=</span> g_running(sample_z)</span>
<span id="cb15-243"><a href="#cb15-243"></a>    <span class="bu">print</span>(<span class="st">'G images'</span>)</span>
<span id="cb15-244"><a href="#cb15-244"></a>    show_images(sample_imgs)</span>
<span id="cb15-245"><a href="#cb15-245"></a>    <span class="bu">print</span>(<span class="st">'G_running images'</span>)</span>
<span id="cb15-246"><a href="#cb15-246"></a>    show_images(sample_imgs_EMA)</span>
<span id="cb15-247"><a href="#cb15-247"></a></span>
<span id="cb15-248"><a href="#cb15-248"></a>    calculate_and_save_fid(<span class="st">'final'</span>, data_loader, g_running, num_fake_images, batch_size, latent_dim, device, fid_file)</span>
<span id="cb15-249"><a href="#cb15-249"></a></span>
<span id="cb15-250"><a href="#cb15-250"></a><span class="co">#&nbsp;No need for stabilising period - we only have one loop system</span></span>
<span id="cb15-251"><a href="#cb15-251"></a></span>
<span id="cb15-252"><a href="#cb15-252"></a>torch.save({</span>
<span id="cb15-253"><a href="#cb15-253"></a>    <span class="st">'g_state_dict'</span>: g.state_dict(),</span>
<span id="cb15-254"><a href="#cb15-254"></a>    <span class="st">'g_running_state_dict'</span>: g_running.state_dict(),</span>
<span id="cb15-255"><a href="#cb15-255"></a>    <span class="st">'d_state_dict'</span>: d.state_dict(),</span>
<span id="cb15-256"><a href="#cb15-256"></a>    <span class="st">'g_optimizer_state_dict'</span>: g_optimizer.state_dict(),</span>
<span id="cb15-257"><a href="#cb15-257"></a>    <span class="st">'d_optimizer_state_dict'</span>: d_optimizer.state_dict(),</span>
<span id="cb15-258"><a href="#cb15-258"></a>    <span class="st">'iteration'</span>: i</span>
<span id="cb15-259"><a href="#cb15-259"></a>}, <span class="ss">f'</span><span class="sc">{</span>checkpoint_dir<span class="sc">}</span><span class="ss">/completed_checkpoint_g_and_EMA.pth'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training---a-story-of-time-and-energy" class="level2">
<h2 class="anchored" data-anchor-id="training---a-story-of-time-and-energy">Training - A Story of Time and Energy</h2>
<p>Training this model correctly took a long time (I began the training around 3/4 months ago). It took so long because my training runs would break down a lot, I ran at least 100 different training runs. To that end I want to discuss some of the failure patterns, so you don’t make the same mistakes and you can realise failures earlier, saving you time and money. My current approach to training runs is this: run the model from a python script, keep a log of FID and wait. For StyleGAN it takes around 12 hours (on a single 3090) before you’ve had enough iterations to judge whether or not to continue the training run. At that point I will review FID and decide whether or not to stop the run. If I stop the run, I go into the code examine to see if anything is wrong and then start the run again. Other times, the runs will just crash out with a “segmentation fault” at this time I have no way to automatically rerun the code<a href="#6"><sup>6</sup></a>. It’s an iterative process, I do a run look at results and edit code based on them, over and over again. So, I hope you can take some of my learnings and avoid this process.</p>
<p>Note, given that StyleGAN2 has many repos around (and that we are all still learning) I do use these other repos to review my code. It’s a massive help and I think it adds to the learning experience, one day we will cover a paper here where there is no reference code and we will put to use all we have learned!</p>
<section id="failure-patterns-i-observed" class="level3">
<h3 class="anchored" data-anchor-id="failure-patterns-i-observed">Failure Patterns I Observed</h3>
<ul>
<li>At the beginning of training I observed that FID starts at around 300-400, the quickest failure pattern can happen in the first 20k iterations. FID will hover at 400 then start to increase and fluctuate wildly. STOP training if you see this, it will not go anywhere. I found that reducing LR helped to mitigate this one, however it was not a full solution.</li>
<li>Another failure pattern began after reducing the LR, previously I used the same mapping network from my StyleGAN implementation. That has hidden layers of size 256 in and 256 out features. Training would often proceed well until around iteration 30k, at which point the loss would explode and decreasing LR further would cause the same behaviour but with smaller explosions each time. The fix was to make the mapping network have 512 in and 512 out features. I posit the increase in layer size allowing the latent space of <span class="math inline">\(w\)</span> to capture more information which is needed for the more diverse/complex FFHQ images.</li>
<li>The idea to reduce the LR comes from the fact that in the paper they train with 8 GPUs and an LR of 0.0025. LR scales linearly, there is a rule shown in Figure 6. For us it goes like this: 8 gpus with a minibatch size of 64 (from the StyleGAN2-ada repo), this mean there is a total batch size of 512 (8 * 64). So to get our LR you do, their_batch_size / our_batch_size -&gt; 512/32 = 16. So k=16, which means to get our LR we do 0.0025 / 16 which gives LR = 0.00015625. This is the LR that I settle with. <b><strong><em>EDIT</em></strong> This is actually incorrect. In the StyleGAN2 paper they use a batch size of 32 which is split between 8 gpus, so 4 images per GPU. Whereas, I had thought it was 32 on each GPU (so 32x8 or 64x8 in the StyleGAN2 ADA paper<a href="#8"><sup>8</sup></a>])</b>. Still the LR scaling rule is useful for other papers so I leave it here for you.</li>
<li>So far with mapping network with 512x512 hidden layers and LR=0.00015625, I can get to around 80 FID score. Which is still very bad, when training with this setup FID reduces pretty nicely until around 100k iteration, then it hovers around 77-&gt;85ish. This is another failure pattern you should be aware of.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/bbfa6d73-a7f2-4d7b-9d93-627721253e3b-1-d8726939-a3c6-4da2-8daa-3dbf04911fa8.png" class="img-fluid figure-img"></p>
<figcaption>Figure 6 - Learning Rate Scaling Rule<a href="#7"><sup>7</sup></a></figcaption>
</figure>
</div>
<section id="a-critical-discovery-one-bug-led-to-a-breakthrough" class="level4">
<h4 class="anchored" data-anchor-id="a-critical-discovery-one-bug-led-to-a-breakthrough">A Critical Discovery, one bug led to a breakthrough</h4>
<p>As I write my final training run is ongoing. I figured out what the issue was and why my training runs failed. In doing so I have encountered more issues. The reason all my training runs previously failed was because of a bug introduced on my part, I take this as a learning to double check all my code.</p>
<p>The way I uncovered this bug was because I had reached a point where training proceeded for around 350k iters and started crashing. At 350k I would also see a pattern in the FID, it would decrease steadily until this point and then from 350k to 400kish it would converge on a score of around 40. The bug in question was segmentation faults, which aren’t very straightforward to resolve. In my attempts to rectify these, I began examining my code closely and came across the following:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/6095700a-fb12-4b9d-95ce-7dc7b5f32a19-1-7a794166-286a-4c59-8753-5a516cd41be1.jpeg" class="img-fluid figure-img"></p>
<figcaption>Figure 7 - A silly mistake which caused a lot of pain</figcaption>
</figure>
</div>
<p>I was not using the modulator. At this point I realised why my training failed so hard, I made the change to the Conv2d_mod operation and voila my training run now manages to get down to an FID of 11 at iteration 340k. (It remains to be seen how low it will go, but it is a vast improvement).</p>
</section>
<section id="some-of-my-fid-patterns" class="level4">
<h4 class="anchored" data-anchor-id="some-of-my-fid-patterns">Some of my FID patterns</h4>
<p>I provide some of the FID schemes, so you can recognise a bad run. I also provide in the code repo my final FID scheme for the successful model. To be honest, I’m not sure of the exact params each time I ran these models given I did so many training runs, but I hope to provide some examples so that you can see what didnt work for me (next time I will implement better logging and we can explore exactly why each one breaks down).</p>
<p>In the following scrollable output, you will see a few training runs which cover the different break downs I saw.</p>
<hr>
<p>Here, the training appears to go well. But the GPU, crashed at iter 180k. I restarted training after that from a checkpoint and you see performance just fluctuates, reaching a minimum of 115. To me this is a failed run, as it fails to improve even after 200k iteration (bare in mind this took about 3/4 days).</p>
<div class="scroll-box">
<p>Iteration 5000: 372.2608947753906</p>
<p>Iteration 10000: 273.1268615722656</p>
<p>Iteration 15000: 260.4564208984375</p>
<p>Iteration 20000: 223.1716766357422</p>
<p>Iteration 25000: 219.0269775390625</p>
<p>Iteration 30000: 205.28900146484375</p>
<p>Iteration 35000: 188.085205078125</p>
<p>Iteration 40000: 177.87722778320312</p>
<p>Iteration 45000: 200.12591552734375</p>
<p>Iteration 50000: 177.63365173339844</p>
<p>Iteration 55000: 175.5360107421875</p>
<p>Iteration 60000: 175.09141540527344</p>
<p>Iteration 65000: 178.38917541503906</p>
<p>Iteration 70000: 170.81842041015625</p>
<p>Iteration 75000: 160.3494415283203</p>
<p>Iteration 80000: 158.06622314453125</p>
<p>Iteration 85000: 159.43179321289062</p>
<p>Iteration 90000: 154.20611572265625</p>
<p>Iteration 95000: 158.0677947998047</p>
<p>Iteration 100000: 143.0650634765625</p>
<p>Iteration 105000: 143.35877990722656</p>
<p>Iteration 110000: 147.93089294433594</p>
<p>Iteration 115000: 128.8855438232422</p>
<p>Iteration 120000: 145.07362365722656</p>
<p>Iteration 125000: 142.61892700195312</p>
<p>Iteration 130000: 129.04664611816406</p>
<p>Iteration 135000: 127.5386734008789</p>
<p>Iteration 140000: 144.24447631835938</p>
<p>Iteration 145000: 145.9493408203125</p>
<p>Iteration 150000: 131.70626831054688</p>
<p>Iteration 155000: 122.86494445800781</p>
<p>Iteration 160000: 133.6108856201172</p>
<p>Iteration 165000: 121.39984893798828</p>
<p>Iteration 170000: 125.3056411743164</p>
<p>Iteration 175000: 133.8019256591797</p>
<p>Iteration 180000: 120.23395538330078</p>
<p>Iteration 185000: 122.73983764648438</p>
<p>Iteration 190000: 123.81011962890625</p>
<p>Iteration 195000: 145.60452270507812</p>
<p>Iteration 200000: 131.8021240234375</p>
<p>Iteration 205000: 131.51727294921875</p>
<p>Iteration 210000: 132.5868682861328</p>
<p>Iteration 215000: 115.08854675292969</p>
<p>Iteration 220000: 119.64303588867188</p>
<p>Iteration 225000: 117.72576141357422</p>
<p>Iteration 230000: 128.736328125</p>
<p>Iteration 235000: 146.3714599609375</p>
<p>Iteration 240000: 134.3799285888672</p>
<p>Iteration 245000: 127.26163482666016</p>
<p>Iteration 250000: 118.39510345458984</p>
<p>Iteration 255000: 133.1106719970703</p>
</div>
<hr>
<p>Another which seemingly starts off well, then fluctuates. We got to a minimum FID of around 94, which is still pretty bad. At this FID images look horrible still.</p>
<div class="scroll-box">
<p>Iteration 5000: 375.0341491699219</p>
<p>Iteration 10000: 266.1919250488281</p>
<p>Iteration 15000: 225.51492309570312</p>
<p>Iteration 20000: 209.35122680664062</p>
<p>Iteration 25000: 169.07830810546875</p>
<p>Iteration 30000: 162.7826690673828</p>
<p>Iteration 35000: 158.3661346435547</p>
<p>Iteration 40000: 142.1690216064453</p>
<p>Iteration 45000: 138.27444458007812</p>
<p>Iteration 50000: 137.2218780517578</p>
<p>Iteration 55000: 135.9760284423828</p>
<p>Iteration 60000: 119.19623565673828</p>
<p>Iteration 65000: 119.33064270019531</p>
<p>Iteration 70000: 113.0389175415039</p>
<p>Iteration 75000: 108.53450775146484</p>
<p>Iteration 80000: 111.82980346679688</p>
<p>Iteration 85000: 109.79493713378906</p>
<p>Iteration 90000: 104.31437683105469</p>
<p>Iteration 95000: 110.7328872680664</p>
<p>Iteration 100000: 122.6935043334961</p>
<p>Iteration 105000: 110.53292083740234</p>
<p>Iteration 110000: 105.91259765625</p>
<p>Iteration 115000: 98.43663787841797</p>
<p>Iteration 120000: 106.87227630615234</p>
<p>Iteration 125000: 100.67845916748047</p>
<p>Iteration 130000: 96.66962432861328</p>
<p>Iteration 135000: 94.45530700683594</p>
<p>Iteration 140000: 100.49161529541016</p>
<p>Iteration 145000: 109.65937042236328</p>
<p>Iteration 150000: 95.66574096679688</p>
<p>Iteration 155000: 106.8659896850586</p>
<p>Iteration 160000: 108.08671569824219</p>
</div>
<hr>
<p>This run performed quite well, the FID drops a lot quicker than previous runs. Here is when I set LR=0.00015625. However, once again it just fluctuates after iter 70k.</p>
<div class="scroll-box">
<p>Iteration 5000: 400.09442138671875</p>
<p>Iteration 10000: 224.55154418945312</p>
<p>Iteration 15000: 199.23419189453125</p>
<p>Iteration 20000: 166.6976776123047</p>
<p>Iteration 25000: 133.10545349121094</p>
<p>Iteration 30000: 122.34843444824219</p>
<p>Iteration 35000: 116.79188537597656</p>
<p>Iteration 40000: 106.47919464111328</p>
<p>Iteration 45000: 103.15625</p>
<p>Iteration 50000: 97.2866439819336</p>
<p>Iteration 55000: 88.9021987915039</p>
<p>Iteration 60000: 92.6580810546875</p>
<p>Iteration 65000: 87.99129486083984</p>
<p>Iteration 70000: 86.91797637939453</p>
<p>Iteration 75000: 82.1233139038086</p>
<p>Iteration 80000: 82.40524291992188</p>
<p>Iteration 85000: 77.69095611572266</p>
<p>Iteration 90000: 76.31440734863281</p>
<p>Iteration 95000: 76.78862762451172</p>
<p>Iteration 100000: 82.9169692993164</p>
<p>Iteration 105000: 81.16597747802734</p>
<p>Iteration 110000: 78.42939758300781</p>
<p>Iteration 115000: 79.91007232666016</p>
</div>
<hr>
<p>I don’t have any saved FID schemes for the early failures I described. But I will create an example one so you know what to look out for. This might not be exactly what you see but the pattern will be close enough to recognise from this example.</p>
<div class="scroll-box">
<p>Iteration 5000: 400.09442138671875</p>
<p>Iteration 10000: 330.039423</p>
<p>Iteration 15000: 302.231324</p>
<p>Iteration 20000: 299.209321</p>
<p>Iteration 25000: 325.99010</p>
<p>Iteration 30000: 320.392034</p>
<p>Iteration 35000: 340.9304</p>
<p>Iteration 40000: 396.0203109</p>
<p>Iteration 45000: 454.10394</p>
<p>Iteration 50000: 424.20319</p>
</div>
<hr>
<p>I hope you find these useful!</p>
</section>
</section>
<section id="segfaults-and-illegal-instructions" class="level3">
<h3 class="anchored" data-anchor-id="segfaults-and-illegal-instructions">SegFaults and Illegal Instructions</h3>
<p>So once I had fixed my major mishap, training ran smoothly… until it didnt. I faced some very perplexing issues in areas of the code I had little control over. Frankly, I still am not sure why these errors cropped up. But, I include the python stack traces here in case you ever face any similar issues. My takeaway from this is that I need to make my training loops more robust, I did so by adding try and except loops with max retries (I added this to the calculate_and_save_fid function). The try and except is not a method to handle seg faults though, for those our checkpointing and reloading structure needs to improve.</p>
<p><b>Note</b> The paths and hex numbers may look a little funny as I removed specific system details of mine.</p>
<hr>
<p>The first here is an SegFault in the calculate_and_save_fid</p>
<div class="scroll-box">
<p>G_running images images after iter: 360000</p>
<p>Fatal Python error: Segmentation fault Generating images: 38%|███▊ | 1669/4375 [01:51&lt;03:01, 14.95it/s]</p>
<p>Thread 0x00000001 (most recent call first): <no python="" frame=""></no></p>
<p>Thread 0x00000002 (most recent call first): File “/path/to/python/lib/python3.11/threading.py”, line 331 in wait File “/path/to/python/lib/python3.11/threading.py”, line 629 in wait File “/path/to/python/lib/python3.11/site-packages/tqdm/_monitor.py”, line 60 in run File “/path/to/python/lib/python3.11/threading.py”, line 1045 in <em>bootstrap</em>inner File “/path/to/python/lib/python3.11/threading.py”, line 1002 in _bootstrap</p>
<p>Thread 0x00000003 (most recent call first): File “/path/to/python/lib/python3.11/threading.py”, line 327 in wait File “/path/to/python/lib/python3.11/multiprocessing/queues.py”, line 231 in _feed File “/path/to/python/lib/python3.11/threading.py”, line 982 in run File “/path/to/python/lib/python3.11/threading.py”, line 1045 in <em>bootstrap</em>inner File “/path/to/python/lib/python3.11/threading.py”, line 1002 in _bootstrap</p>
<p>Thread 0x00000004 (most recent call first): File “/path/to/python/lib/python3.11/selectors.py”, line 415 in select File “/path/to/python/lib/python3.11/multiprocessing/connection.py”, line 947 in wait File “/path/to/python/lib/python3.11/multiprocessing/connection.py”, line 440 in _poll File “/path/to/python/lib/python3.11/multiprocessing/connection.py”, line 257 in poll File “/path/to/python/lib/python3.11/multiprocessing/queues.py”, line 113 in get File “/path/to/python/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py”, line 32 in do_one_step File “/path/to/python/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py”, line 55 in <em>pin</em>memory_loop File “/path/to/python/lib/python3.11/threading.py”, line 982 in run File “/path/to/python/lib/python3.11/threading.py”, line 1045 in <em>bootstrap</em>inner File “/path/to/python/lib/python3.11/threading.py”, line 1002 in _bootstrap</p>
<p>Current thread 0x00000005 (most recent call first): File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/conv.py”, line 454 in <em>conv</em>forward File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/conv.py”, line 458 in forward File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1562 in <em>call</em>impl File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1553 in <em>wrapped</em>call_impl File “/path/to/python/lib/python3.11/site-packages/torch_fidelity/feature_extractor_inceptionv3.py”, line 208 in forward File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1562 in <em>call</em>impl File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1553 in <em>wrapped</em>call_impl File “/path/to/python/lib/python3.11/site-packages/torch_fidelity/feature_extractor_inceptionv3.py”, line 232 in forward File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1562 in <em>call</em>impl File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1553 in <em>wrapped</em>call_impl File “/path/to/python/lib/python3.11/site-packages/torchmetrics/image/fid.py”, line 111 in <em>torch</em>fidelity_forward File “/path/to/python/lib/python3.11/site-packages/torchmetrics/image/fid.py”, line 155 in forward File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1562 in <em>call</em>impl File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1553 in <em>wrapped</em>call_impl File “/path/to/python/lib/python3.11/site-packages/torchmetrics/image/fid.py”, line 365 in update File “/path/to/python/lib/python3.11/site-packages/torchmetrics/metric.py”, line 483 in wrapped_func File “/path/to/project/StyleGAN2/StyleGAN2.py”, line 898 in add_fake_images File “/path/to/project/StyleGAN2/StyleGAN2.py”, line 923 in calculate_and_save_fid File “/path/to/project/StyleGAN2/StyleGAN2.py”, line 1139 in <module></module></p>
<p>[1]+ Segmentation fault (core dumped) nohup python3 -X faulthandler -u StyleGAN2.py &gt; output.log 2&gt;&amp;1</p>
</div>
<hr>
<p>The second is caused by an illegal instruction again in the calculate_and_save_fid function. An illegal instruction I think is due to a CPU instruction being ran on the GPU or vice versa.</p>
<div class="scroll-box">
<p>Fatal Python error: Illegal instruction Generating images: 29%|██▉ | 1259/4375 [01:26&lt;03:34, 14.54it/s]</p>
<p>Thread 0x00000001 (most recent call first): <no python="" frame=""></no></p>
<p>Thread 0x00000002 (most recent call first): File “/path/to/python/lib/python3.11/threading.py”, line 331 in wait File “/path/to/python/lib/python3.11/threading.py”, line 629 in wait File “/path/to/python/lib/python3.11/site-packages/tqdm/_monitor.py”, line 60 in run File “/path/to/python/lib/python3.11/threading.py”, line 1045 in bootstrapinner File “/path/to/python/lib/python3.11/threading.py”, line 1002 in _bootstrap</p>
<p>Thread 0x00000003 (most recent call first): File “/path/to/python/lib/python3.11/threading.py”, line 327 in wait File “/path/to/python/lib/python3.11/multiprocessing/queues.py”, line 231 in _feed File “/path/to/python/lib/python3.11/threading.py”, line 982 in run File “/path/to/python/lib/python3.11/threading.py”, line 1045 in bootstrapinner File “/path/to/python/lib/python3.11/threading.py”, line 1002 in _bootstrap</p>
<p>Thread 0x00000004 (most recent call first): File “/path/to/python/lib/python3.11/selectors.py”, line 415 in select File “/path/to/python/lib/python3.11/multiprocessing/connection.py”, line 947 in wait File “/path/to/python/lib/python3.11/multiprocessing/connection.py”, line 440 in _poll File “/path/to/python/lib/python3.11/multiprocessing/connection.py”, line 257 in poll File “/path/to/python/lib/python3.11/multiprocessing/queues.py”, line 113 in get File “/path/to/python/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py”, line 32 in do_one_step File “/path/to/python/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py”, line 55 in pinmemory_loop File “/path/to/python/lib/python3.11/threading.py”, line 982 in run File “/path/to/python/lib/python3.11/threading.py”, line 1045 in bootstrapinner File “/path/to/python/lib/python3.11/threading.py”, line 1002 in _bootstrap</p>
<p>Current thread 0x00000005 (most recent call first): File “/path/to/python/lib/python3.11/site-packages/torch_fidelity/feature_extractor_inceptionv3.py”, line 209 in forward File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1562 in callimpl File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1553 in wrappedcall_impl File “/path/to/python/lib/python3.11/site-packages/torch_fidelity/feature_extractor_inceptionv3.py”, line 234 in forward File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1562 in callimpl File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1553 in wrappedcall_impl File “/path/to/python/lib/python3.11/site-packages/torchmetrics/image/fid.py”, line 113 in torchfidelity_forward File “/path/to/python/lib/python3.11/site-packages/torchmetrics/image/fid.py”, line 155 in forward File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1562 in callimpl File “/path/to/python/lib/python3.11/site-packages/torch/nn/modules/module.py”, line 1553 in wrappedcall_impl File “/path/to/python/lib/python3.11/site-packages/torchmetrics/image/fid.py”, line 365 in update File “/path/to/python/lib/python3.11/site-packages/torchmetrics/metric.py”, line 483 in wrapped_func File “/path/to/project/StyleGAN2/StyleGAN2.py”, line 896 in add_fake_images File “/path/to/project/StyleGAN2/StyleGAN2.py”, line 921 in calculate_and_save_fid File “/path/to/project/StyleGAN2/StyleGAN2.py”, line 1137 in module</p>
</div>
<hr>
<p>The following error cropped up a number of times at different iterations.</p>
<div class="scroll-box">
<p>FID score for iteration 20000: 64.7917022705078175 [02:54&lt;00:00, 25.46it/s] 1%|▏ | 20668/1562500 [3:51:10&lt;168:03:37, 2.55it/s]Fatal Python error: Segmentation fault</p>
<p>Current thread 0x00000001 (most recent call first): <no python="" frame=""></no></p>
<p>Thread 0x00000002 (most recent call first): File “/path/to/python/lib/python3.11/threading.py”, line 331 in wait File “/path/to/python/lib/python3.11/threading.py”, line 629 in wait File “/path/to/python/lib/python3.11/site-packages/tqdm/_monitor.py”, line 60 in run File “/path/to/python/lib/python3.11/threading.py”, line 1045 in bootstrapinner File “/path/to/python/lib/python3.11/threading.py”, line 1002 in _bootstrap</p>
<p>Thread 0x00000003 (most recent call first): File “/path/to/python/lib/python3.11/threading.py”, line 327 in wait File “/path/to/python/lib/python3.11/multiprocessing/queues.py”, line 231 in _feed File “/path/to/python/lib/python3.11/threading.py”, line 982 in run File “/path/to/python/lib/python3.11/threading.py”, line 1045 in bootstrapinner File “/path/to/python/lib/python3.11/threading.py”, line 1002 in _bootstrap</p>
<p>Thread 0x00000004 (most recent call first): File “/path/to/python/lib/python3.11/threading.py”, line 327 in wait File “/path/to/python/lib/python3.11/multiprocessing/queues.py”, line 231 in _feed File “/path/to/python/lib/python3.11/threading.py”, line 982 in run File “/path/to/python/lib/python3.11/threading.py”, line 1045 in bootstrapinner File “/path/to/python/lib/python3.11/threading.py”, line 1002 in _bootstrap</p>
<p>Thread 0x00000005 (most recent call first): File “/path/to/python/lib/python3.11/selectors.py”, line 415 in select File “/path/to/python/lib/python3.11/multiprocessing/connection.py”, line 947 in wait File “/path/to/python/lib/python3.11/multiprocessing/connection.py”, line 440 in _poll File “/path/to/python/lib/python3.11/multiprocessing/connection.py”, line 257 in poll File “/path/to/python/lib/python3.11/multiprocessing/queues.py”, line 113 in get File “/path/to/python/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py”, line 32 in do_one_step File “/path/to/python/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py”, line 55 in pinmemory_loop File “/path/to/python/lib/python3.11/threading.py”, line 982 in run File “/path/to/python/lib/python3.11/threading.py”, line 1045 in bootstrapinner File “/path/to/python/lib/python3.11/threading.py”, line 1002 in _bootstrap</p>
<p>Thread 0x00000006 (most recent call first): File “/path/to/python/lib/python3.11/site-packages/torch/autograd/graph.py”, line 768 in enginerun_backward File “/path/to/python/lib/python3.11/site-packages/torch/autograd/init.py”, line 289 in backward File “/path/to/python/lib/python3.11/site-packages/torch/_tensor.py”, line 521 in backward File “/path/to/project/StyleGAN2/StyleGAN2.py”, line 1093 in module</p>
</div>
<hr>
<p>Here’s a real weird one, this error is thrown by the following code: torch.cuda.empty_cache(). The whole point I put this in was to stabilise training not cause more, but alas I do not know why it causes an illegal instruction</p>
<div class="scroll-box">
<p>38%|███▊ | 591068/1562500 [29:32:52&lt;107:33:34, 2.51it/s]Fatal Python error: Illegal instruction</p>
<p>Thread 0x00000001 (most recent call first): <no python="" frame=""> Thread 0x00000002 (most recent call first): File “/path/to/python/lib/python3.11/threading.py”, line 331 in wait File “/path/to/python/lib/python3.11/threading.py”, line 629 in wait File “/path/to/python/lib/python3.11/site-packages/tqdm/_monitor.py”, line 60 in run File “/path/to/python/lib/python3.11/threading.py”, line 1045 in bootstrapinner File “/path/to/python/lib/python3.11/threading.py”, line 1002 in _bootstrap</no></p>
<p>Thread 0x00000003 (most recent call first): File “/path/to/python/lib/python3.11/threading.py”, line 327 in wait File “/path/to/python/lib/python3.11/multiprocessing/queues.py”, line 231 in _feed File “/path/to/python/lib/python3.11/threading.py”, line 982 in run File “/path/to/python/lib/python3.11/threading.py”, line 1045 in bootstrapinner File “/path/to/python/lib/python3.11/threading.py”, line 1002 in _bootstrap</p>
<p>Thread 0x00000004 (most recent call first): File “/path/to/python/lib/python3.11/selectors.py”, line 415 in select File “/path/to/python/lib/python3.11/multiprocessing/connection.py”, line 947 in wait File “/path/to/python/lib/python3.11/multiprocessing/connection.py”, line 440 in _poll File “/path/to/python/lib/python3.11/multiprocessing/connection.py”, line 257 in poll File “/path/to/python/lib/python3.11/multiprocessing/queues.py”, line 113 in get File “/path/to/python/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py”, line 32 in do_one_step File “/path/to/python/lib/python3.11/site-packages/torch/utils/data/_utils/pin_memory.py”, line 55 in pinmemory_loop File “/path/to/python/lib/python3.11/threading.py”, line 982 in run File “/path/to/python/lib/python3.11/threading.py”, line 1045 in bootstrapinner File “/path/to/python/lib/python3.11/threading.py”, line 1002 in _bootstrap</p>
<p>Current thread 0x00000005 (most recent call first): File “/path/to/python/lib/python3.11/site-packages/torch/cuda/memory.py”, line 170 in empty_cache File “/path/to/project/StyleGAN2/StyleGAN2.py”, line 1129 in module</p>
</div>
<hr>
<p>The tendency of such issues to appear randomly is what leads me to think that if we were to retry the code would run as intended and it often does. So, as mentioned we need to develop a better method to handle retries and manage the training runs better. I will leave that for a later blog post or for you!</p>
<p>Some other steps I took to reduce frequency of these errors was setting num_workers = 1 in dataloader (trying to prevent simultaneous memory access), adding more torch.cuda.empty_cache() to ensure no out of bounds memory access and adding a torch.cuda.synchronize() before calling save_and_calc_fid.</p>
</section>
<section id="the-successful-training-run" class="level3">
<h3 class="anchored" data-anchor-id="the-successful-training-run">The Successful Training Run</h3>
<p><b>Running the training code</b>, I run the training code with the following command:</p>
<ul>
<li>nohup python3 -X faulthandler -u StyleGAN2.py &gt; lr_0025_batch_16_g_every_4.log 2&gt;&amp;1<a href="#9"><sup>9</sup></a></li>
</ul>
<p>The above command will run the StyleGAN2.py code in the background and you can view training progress in the .log file.</p>
<p>Let’s load in our final FID scheme for the successful training run and analyse it a little. You can view the final FID txt file in the repository for this code too, check the link in the sidebar ;)</p>
<div id="93852f68" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># Read FID scores from file</span></span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="kw">def</span> read_fid_scores(filename):</span>
<span id="cb16-3"><a href="#cb16-3"></a>   iterations <span class="op">=</span> []</span>
<span id="cb16-4"><a href="#cb16-4"></a>   scores <span class="op">=</span> []</span>
<span id="cb16-5"><a href="#cb16-5"></a>   <span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb16-6"><a href="#cb16-6"></a>       <span class="cf">for</span> line <span class="kw">in</span> f:</span>
<span id="cb16-7"><a href="#cb16-7"></a>           <span class="cf">if</span> line.startswith(<span class="st">'Iteration'</span>):</span>
<span id="cb16-8"><a href="#cb16-8"></a>               iter_num <span class="op">=</span> <span class="bu">int</span>(line.split(<span class="st">':'</span>)[<span class="dv">0</span>].split()[<span class="dv">1</span>])</span>
<span id="cb16-9"><a href="#cb16-9"></a>               score <span class="op">=</span> <span class="bu">float</span>(line.split(<span class="st">':'</span>)[<span class="dv">1</span>].strip())</span>
<span id="cb16-10"><a href="#cb16-10"></a>               iterations.append(iter_num)</span>
<span id="cb16-11"><a href="#cb16-11"></a>               scores.append(score)</span>
<span id="cb16-12"><a href="#cb16-12"></a>   <span class="cf">return</span> iterations, scores</span>
<span id="cb16-13"><a href="#cb16-13"></a>    </span>
<span id="cb16-14"><a href="#cb16-14"></a><span class="co"># Plot FID scores</span></span>
<span id="cb16-15"><a href="#cb16-15"></a><span class="kw">def</span> plot_fid(iterations, scores):</span>
<span id="cb16-16"><a href="#cb16-16"></a>   plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">6</span>))</span>
<span id="cb16-17"><a href="#cb16-17"></a>   plt.plot(iterations, scores)</span>
<span id="cb16-18"><a href="#cb16-18"></a>   </span>
<span id="cb16-19"><a href="#cb16-19"></a>   min_score <span class="op">=</span> <span class="bu">min</span>(scores)</span>
<span id="cb16-20"><a href="#cb16-20"></a>   min_iter <span class="op">=</span> iterations[scores.index(min_score)]</span>
<span id="cb16-21"><a href="#cb16-21"></a>   </span>
<span id="cb16-22"><a href="#cb16-22"></a>   plt.plot(min_iter, min_score, <span class="st">'ro'</span>)</span>
<span id="cb16-23"><a href="#cb16-23"></a>   plt.annotate(<span class="ss">f'Min FID: </span><span class="sc">{</span>min_score<span class="sc">:.2f}</span><span class="ch">\n</span><span class="ss">Iteration: </span><span class="sc">{</span>min_iter<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb16-24"><a href="#cb16-24"></a>               xy<span class="op">=</span>(min_iter, min_score),</span>
<span id="cb16-25"><a href="#cb16-25"></a>               xytext<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>),</span>
<span id="cb16-26"><a href="#cb16-26"></a>               textcoords<span class="op">=</span><span class="st">'offset points'</span>,</span>
<span id="cb16-27"><a href="#cb16-27"></a>               bbox<span class="op">=</span><span class="bu">dict</span>(facecolor<span class="op">=</span><span class="st">'white'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>))</span>
<span id="cb16-28"><a href="#cb16-28"></a>   </span>
<span id="cb16-29"><a href="#cb16-29"></a>   plt.xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb16-30"><a href="#cb16-30"></a>   plt.ylabel(<span class="st">'FID Score'</span>)</span>
<span id="cb16-31"><a href="#cb16-31"></a>   plt.title(<span class="st">'FID Score vs Training Iteration'</span>)</span>
<span id="cb16-32"><a href="#cb16-32"></a>   plt.grid(<span class="va">True</span>)</span>
<span id="cb16-33"><a href="#cb16-33"></a>   <span class="cf">return</span> plt</span>
<span id="cb16-34"><a href="#cb16-34"></a></span>
<span id="cb16-35"><a href="#cb16-35"></a>iterations, scores <span class="op">=</span> read_fid_scores(<span class="st">'./StyleGAN2_fid.txt'</span>)</span>
<span id="cb16-36"><a href="#cb16-36"></a>plot_fid(iterations, scores)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Ain’t that a pretty loss curve, it’s exactly the sort of shape we want! There is a steep decrease to around FID 20, which occurs roughly at iteration 100000 (this took 20 hours of training to achieve). Then note how much longer it takes to reduce FID to the final score of 8.06 at iteration 1010k. In total this whole run probably took around 200-250 hours to complete, and the graph shows that most of training is spent on eeking out small gains. The model converges around the FID score of 8, it oscillated in this range iteration 600k until I stopped training. Even this lowest score of 8.06 I think is due to randomness, continued training after achieving this score didnt lead to any clear improvements.</p>
<p>Getting here required a lot of manual intervention which I hope to reduce in my next post, I had to restart training 4 times manually editing the path to checkpoints and such. If you have any queries while reading or running the code please reach out to me and I’d be happy to help (my email is in the footer).</p>
<section id="so-what-do-our-images-look-like" class="level4">
<h4 class="anchored" data-anchor-id="so-what-do-our-images-look-like">So what do our images look like?</h4>
<p>To illustrate the model’s capabilities I will show you 16 randomly generated images from different stages throughout the training. Stating from iteration 10k up to iteration 950k which has the lowest FID. To make reference to the images think of it as a 4x4 grid with the top left-most image being at position [0,0].</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/69c806a5-b966-475d-be5c-e05211b0ca33-2-2a0338e3-429a-478b-a706-25f38bd2b440.png" class="img-fluid figure-img"></p>
<figcaption>Iteration 10k - FID 111.28 - The model picks up on high level details quite quickly, we see the emergence of faces and the sorts of colours/features a face should have albeit with big distortions</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/69c806a5-b966-475d-be5c-e05211b0ca33-3-41a741d6-ace9-4080-bb7a-f236cc0de973.png" class="img-fluid figure-img"></p>
<figcaption>Iteration 50k - FID 32.16 - The model is improving, these images are starting to take shape. Facial features and accessories such as glasses are better formed now</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/69c806a5-b966-475d-be5c-e05211b0ca33-1-1b57e817-2f43-4a72-b2b4-2a249698a232.png" class="img-fluid figure-img"></p>
<figcaption>Iteration 100k - FID 19.60 - The 20 FID threshold, note 20 FID has no actual meaning it’s just something I empirically noticed in this training run. The training slow down occurs past this point. Also, look at these samples they’re hardly great, those at 50k could be seen as better. But, the defining factor I think is the colours in the images, at 50k there are a lot of distortions which don’t occur so much at 100k iterations</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/69c806a5-b966-475d-be5c-e05211b0ca33-6-de82e982-e8bb-4097-9499-9083911810f0.png" class="img-fluid figure-img"></p>
<figcaption>Iteration 150k - FID 15.86 - I think our samples are beginning to look quite good. Take for example the image at position [2,2] that dude looks pretty real</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/69c806a5-b966-475d-be5c-e05211b0ca33-5-9fffbb39-a124-4d88-842d-eeec79ae827c.png" class="img-fluid figure-img"></p>
<figcaption>Iteration 400k - FID 9.94 - So it took us 250k iterations to get from 15.86 to &gt;10 FID. I hope this highlights the slowdown and what role the later stage of training plays. The images now possess most of the features but they just aren’t realistic yet.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="StyleGAN2_files/figure-html/69c806a5-b966-475d-be5c-e05211b0ca33-4-8ad2cb2b-e1c9-43e2-90a0-ae616c928615.png" class="img-fluid figure-img"></p>
<figcaption>Iteration 950k - FID 8.06 - The best FID score we got. These faces look great, albeit with some issues. I think if we were to handpick our images we’d get some believable faces!</figcaption>
</figure>
</div>
<p>Thats it! We’ve trained a StyleGAN2 model and it’s ready to use. Here’s a link to the best iterations weights: <a href="https://huggingface.co/YM2132/StyleGAN2/tree/main">https://huggingface.co/YM2132/StyleGAN2/tree/main</a>. To ensure convergence I continued training to 1200k iterations, and the FID kept oscillating between 8-9 indicating the model has settled. Perhaps some LR adjustments could reduce the FID score further.</p>
<p>Lastly, a closing remark about the behaviour of this GAN displayed in training. We can think of it as the model learning the basic features at the start and then spending the rest of the time finetuning the output. Where previously we enforced this behaviour, learning high level details before fine grained details, the model now does it itself! This behaviour is truly remarkable, the optimisation process of the StyleGAN2 results in the model enforcing progressive growing to create the best images possible. I leave this with you to ponder and I hope you enjoyed the path to StyleGAN2</p>
<hr>
<p><a id="1" style="text-decoration: none; color: inherit;" href=""><sup>1</sup></a> This is an important aspect of deep learning research in general. The authors most likely came to this conclusion not through some sort of theoretical reasoning but rather they performed ablation studies. An ablation study is when you remove/change one aspect of a system and observe the outcome. These are so crucial in deep learning research as it’s very hard to understand the why (i.e.&nbsp;why does AdaIN cause water droplets) but it’s easier to observe what effect AdaIN has. In the paper they state when it is removed this water droplet effect doesnt occur, hence ablation studies FTW! Some more info on ablation studies <a href="https://x.com/fchollet/status/1831029432653599226">https://x.com/fchollet/status/1831029432653599226</a></p>
<p><a id="2" style="text-decoration: none; color: inherit;" href=""><sup>2</sup></a> Now this is a little wordy, and I do not fully understand it myself yet. But my initial idea is the G model creates pixel intensity spikes as a result of the AdaIN operation removing information related across features. Because each feature map is treated independently by AdaIN, if there is an issue in one feature map it can be propagated through the network.</p>
<p><a id="3" style="text-decoration: none; color: inherit;" href=""><sup>3</sup></a> Here is another cool part of deep learning. There are many ideas which work in different contexts and often when applied to other problems they can yield good results too. For example, here residual connections come from ResNets which were for computer vision tasks. I’d recommend you in your journey to try out such things, try different combinations of architectures/ideas and observe the results.</p>
<p><a id="4" style="text-decoration: none; color: inherit;" href=""><sup>4</sup></a> I got this information from this absolutely amazing blog post by Gwern: <a href="https://gwern.net/face">https://gwern.net/face</a>. I’d suggest you read this from start to finish (along with his other posts), it is packed full of insights (some of which we discuss here) into GANs and training GANs. For this idea of overfitting, there is an argument that GANs overfit to the dataset. This is a better problem that underfitting as it shows we are learning from the data and to reduce overfitting we can employ regularisation. So thats where the idea of regularisation in GANs come in.</p>
<p><a id="5" style="text-decoration: none; color: inherit;" href=""><sup>5</sup></a> <a href="https://github.com/NVlabs/stylegan2/blob/master/training/training_loop.py#L121C1-L121C124">https://github.com/NVlabs/stylegan2/blob/master/training/training_loop.py#L121C1-L121C124</a></p>
<p><a id="6" style="text-decoration: none; color: inherit;" href=""><sup>6</sup></a> This begs the question how do large companies handle training runs? For me even this was quite expensive with the electricity costing around £3 a day. I wonder how do large training runs handle “segmentation faults”, GPU errors and the infra around training runs. Also, is there a method to know (or have an estimate) on how likely a run is to fail or not a priori? If you have any answers to these questions please reach out to me: <a href="mailto:y%75sufmohamma%64@l%69ve.com">yusufmohammad@live.com</a> I’d love to know more.</p>
<p><a id="7" style="text-decoration: none; color: inherit;" href=""><sup>7</sup></a> The paper this comes from can be rule found at: <a href="https://arxiv.org/pdf/1706.02677">https://arxiv.org/pdf/1706.02677</a></p>
<p><a id="8" style="text-decoration: none; color: inherit;" href=""><sup>8</sup></a> The info about batch size for StyleGAN2: <a href="https://github.com/rosinality/stylegan2-pytorch/issues/152">https://github.com/rosinality/stylegan2-pytorch/issues/152</a></p>
<p><a id="9" style="text-decoration: none; color: inherit;" href=""><sup>9</sup></a> An explainer of what that command does (this is a very cool website) <a href="https://explainshell.com/explain?cmd=nohup+python3+-X+faulthandler+-u+StyleGAN2.py+%3E+lr_0025_batch_16_g_every_4.log+2%3E%261">https://explainshell.com/explain?cmd=nohup+python3+-X+faulthandler+-u+StyleGAN2.py+%3E+lr_0025_batch_16_g_every_4.log+2%3E%261</a></p>
<p><a id="10" style="text-decoration: none; color: inherit;" href=""><sup>10</sup></a> In the <a href="https://github.com/NVlabs/stylegan3/blob/c233a919a6faee6e36a316ddd4eddababad1adf9/docs/configs.md">StyleGAN3 readme</a> there is a table of hyper parameters for different setups for StyleGAN2, you can use these to adjust your params to ensure the model fits on your GPU.</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>